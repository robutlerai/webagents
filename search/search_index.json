{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Foundation Framework for the Web of Agents","text":"<p>WebAgents (Web of Agents) is a powerful framework for building connected AI agents with a simple yet comprehensive API. Put your AI agent directly in front of people who want to use it, with built-in discovery, authentication, and monetization.</p> <p>Build, Serve and Monetize AI Agents  </p> <p>WebAgents architecture enables dynamic real-time orchestration of agents. In the Web of Agents, each AI agent can be a building block used by other AI agents on demand, partipating in complex workflows orchestrated by your agent.</p> <p>\ud83d\ude80 Key Features</p> <ul> <li>\ud83e\udd1d Agent-to-Agent Delegation - Delegate tasks to other agents via natural language. Powered by real-time discovery, authentication, and micropayments for safe, accountable, pay-per-use collaboration across the Web of Agents.</li> <li>\ud83d\udd0d Real-Time Discovery - Agents discover each other through intent matching on demand in real time without need for manual integrations</li> <li>\ud83d\udd10 Trust &amp; Security - Secure authentication and scope-based access control</li> <li>\ud83d\udcb0 Built-in Monetization - Earn credits from priced tools with automatic billing</li> <li>\ud83c\udf10 Protocol agnostic - Deploy agents as standard chat completion endpoints with coming support for OpenAI Responses/Realtime, ACP, A2A and other common AI communication protocols and frameworks.</li> <li>\ud83e\udde9 Modular Skills - Combine tools, prompts, hooks, and HTTP endpoints into reusable packages with automatic dependency resolution.</li> <li>\ud83d\udd0c Build or Integrate - Build from scratch with WebAgents, or integrate existing agents from popular SDKs and platforms into the Web of Agents (e.g., Azure AI Foundry, Google Vertex AI, CrewAI, n8n, Zapier).</li> </ul> <p>With WebAgents, you achieve precise low-level control over your agent's logic. Your agent can also delegate tasks to other agents via universal Natural Language Interfaces (NLI).</p> <ul> <li> <p>\u26a1 Full control through code</p> <p>Build exactly what you need with full control over your agent's capabilities. Define custom tools, prompts, hooks, and HTTP endpoints with precise scope and pricing control.</p> </li> <li> <p>\ud83d\udd0d Flexibility through delegation</p> <p>Delegate tasks to other agents without any integration - the platform handles discovery, trust, and payments. Focus on your unique value while leveraging the entire ecosystem.</p> </li> </ul> <p>The Best of Both Worlds: get full control when building their your agents functionality, AND maximum flexibility when delegating to the network on demand in real-time. No integration work, no API keys to manage, no payment setup. </p> <p>With WebAgents delegation, your agent is as powerful as the whole ecosystem.</p> <p>Capabilities of your agent grow together with the whole ecosystem.</p>"},{"location":"#skills","title":"\ud83e\udde9Skills","text":"<p>Skills combine tools, prompts, hooks, and HTTP endpoints into easy-to-integrate packages with automatic dependency resolution.</p> <pre><code>from webagents.agents.skills.base import Skill\nfrom webagents.agents.tools.decorators import tool, prompt, hook, http\nfrom webagents.agents.skills.robutler.payments.skill import pricing\n\nclass NotificationsSkill(Skill):        \n    @prompt(scope=[\"owner\"])\n    def get_prompt(self) -&gt; str:\n        return \"You can send notifications using send_notification().\"\n\n    @tool(scope=\"owner\")\n    @pricing(credits_per_call=0.01)\n    async def send_notification(self, title: str, body: str) -&gt; str:\n        # Your API integration\n        return f\"\u2705 Notification sent: {title}\"\n\n    @hook(\"on_message\")\n    async def log_messages(self, context):\n        # React to incoming messages\n        return context\n\n    @http(\"POST\", \"/webhook\")\n    async def handle_webhook(self, request):\n        # Custom HTTP endpoint\n        return {\"status\": \"received\"}\n</code></pre> <p>Skills Repository is a comprehensive collection of pre-built capabilities that extend your agents' functionality.</p>"},{"location":"#core-and-ecosystem","title":"\ud83c\udf10 Core and Ecosystem","text":"<p>The core skills enable you to build and serve your agent to the internet with no dependencies. Provides fundamental capabilities to your agent. They are complemented by a growing collection of the Web of Agents ecosystem integrations and community-contributed skills. Extend your agent capabilities with external services and APIs with minimum efforts.</p>"},{"location":"#real-time-discovery","title":"\ud83d\ude80 Real-Time Discovery","text":"<p>Think of the discovery skill as \"DNS\" for agent intents. Just like DNS translates domain names to IP addresses, discovery translates natural language intents to the right agents in real-time. Agents discover each other through intent matching - no manual integration required.</p> <p>The platform handles all discovery, authentication, and payments between agents - your agent just describes what it needs in natural language.</p>"},{"location":"#trust-security","title":"\ud83d\udd10 Trust &amp; Security","text":"<p>Agents trust each other through secure authentication protocols and scope-based access control. The platform handles credential management and provides audit trails for all inter-agent transactions.</p>"},{"location":"#monetization","title":"\ud83d\udcb0 Monetization","text":"<p>Add the payment skill to your agent and earn credits from priced tools:</p> <pre><code>from webagents.agents.core.base_agent import BaseAgent\nfrom webagents.agents.skills.robutler.payments.skill import PaymentSkill\n\nagent = BaseAgent(\n    name=\"image-generator\",\n    model=\"openai/gpt-4o-mini\",\n    skills={\n        \"payments\": PaymentSkill(),\n        \"image\": ImageGenerationSkill()\n    }\n)\n</code></pre>"},{"location":"#your-custom-skills","title":"\u2728 Your Custom Skills","text":"<p>Build and use your own skills tailored to your specific needs. Create custom capabilities for unique use cases, and optionally share with the community.</p>"},{"location":"#get-started","title":"\ud83c\udfaf Get Started","text":"<ul> <li>Quickstart Guide - Build your first agent in 5 minutes</li> <li>Skills Framework - Deep dive into Skills</li> <li>Agent Architecture - Understand agent communication</li> </ul>"},{"location":"dynamic-agents/","title":"Dynamic Agents","text":"<p>Load agents at runtime using the <code>dynamic_agents</code> parameter and resolver functions.</p>"},{"location":"dynamic-agents/#overview","title":"Overview","text":"<p>Dynamic agents enable runtime agent loading without pre-registration:</p> <ul> <li>On-Demand Creation - Agents created when first requested</li> <li>Configuration-Driven - Load from external sources (DB, API, files)</li> <li>Flexible Updates - Change agent behavior without redeployment</li> <li>Memory Efficient - Only create agents that are actually used</li> </ul>"},{"location":"dynamic-agents/#dynamic-agent-resolver","title":"Dynamic Agent Resolver","text":"<p>The <code>dynamic_agents</code> parameter accepts a resolver function that creates agents by name:</p> <pre><code>from webagents.server.core.app import create_server\nfrom webagents.agents import BaseAgent\n\nasync def resolve_agent(agent_name: str):\n    \"\"\"Resolver function - return BaseAgent or None\"\"\"\n\n    # Load configuration from your source\n    config = await load_config(agent_name)\n    if not config:\n        return None\n\n    # Create and return agent\n    return BaseAgent(\n        name=config[\"name\"],\n        instructions=config[\"instructions\"],\n        model=config[\"model\"]\n    )\n\n# Pass resolver to server\nserver = create_server(\n    title=\"Dynamic Server\",\n    dynamic_agents=resolve_agent  # Resolver function\n)\n</code></pre>"},{"location":"dynamic-agents/#resolver-function-signature","title":"Resolver Function Signature","text":"<p>The resolver function must match this signature:</p> <pre><code># Async resolver (recommended)\nasync def resolve_agent(agent_name: str) -&gt; Optional[BaseAgent]:\n    pass\n\n# Sync resolver (also supported)\ndef resolve_agent(agent_name: str) -&gt; Optional[BaseAgent]:\n    pass\n</code></pre> <p>Parameters: - <code>agent_name</code>: The agent name from the URL path - Returns: <code>BaseAgent</code> instance or <code>None</code> if not found</p>"},{"location":"dynamic-agents/#resolution-flow","title":"Resolution Flow","text":"<ol> <li>Request arrives for <code>/agent-name/chat/completions</code></li> <li>Static Check - Look for pre-registered agents first</li> <li>Dynamic Call - Call <code>dynamic_agents(agent_name)</code> if not found</li> <li>Agent Creation - Resolver creates and returns BaseAgent</li> <li>Request Processing - Server uses the resolved agent</li> </ol>"},{"location":"dynamic-agents/#configuration-sources","title":"Configuration Sources","text":""},{"location":"dynamic-agents/#database-resolver","title":"Database Resolver","text":"<pre><code>async def db_resolver(agent_name: str):\n    \"\"\"Load agent from database\"\"\"\n    query = \"SELECT * FROM agents WHERE name = $1\"\n    row = await db.fetchrow(query, agent_name)\n\n    if not row:\n        return None\n\n    return BaseAgent(\n        name=row[\"name\"],\n        instructions=row[\"instructions\"],\n        model=row[\"model\"]\n    )\n</code></pre>"},{"location":"dynamic-agents/#file-based-resolver","title":"File-Based Resolver","text":"<pre><code>import json\nimport os\n\nasync def file_resolver(agent_name: str):\n    \"\"\"Load agent from JSON files\"\"\"\n    config_path = f\"agents/{agent_name}.json\"\n\n    if not os.path.exists(config_path):\n        return None\n\n    with open(config_path) as f:\n        config = json.load(f)\n\n    return BaseAgent(**config)\n</code></pre>"},{"location":"dynamic-agents/#api-resolver","title":"API Resolver","text":"<pre><code>import aiohttp\n\nasync def api_resolver(agent_name: str):\n    \"\"\"Load agent from external API\"\"\"\n    url = f\"https://api.example.com/agents/{agent_name}\"\n\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as resp:\n            if resp.status != 200:\n                return None\n\n            config = await resp.json()\n            return BaseAgent(**config)\n</code></pre>"},{"location":"dynamic-agents/#combined-static-and-dynamic","title":"Combined Static and Dynamic","text":"<p>Use both static agents and dynamic resolution:</p> <pre><code># Static agents (always available)\nstatic_agents = [\n    BaseAgent(name=\"assistant\", model=\"openai/gpt-4o\"),\n    BaseAgent(name=\"support\", model=\"openai/gpt-4o\")\n]\n\n# Dynamic resolver for additional agents\nasync def dynamic_resolver(agent_name: str):\n    return await load_from_database(agent_name)\n\nserver = create_server(\n    agents=static_agents,        # Pre-registered agents\n    dynamic_agents=dynamic_resolver  # Runtime resolution\n)\n</code></pre>"},{"location":"dynamic-agents/#error-handling","title":"Error Handling","text":"<p>Handle errors gracefully in resolvers:</p> <pre><code>import logging\n\nasync def safe_resolver(agent_name: str):\n    \"\"\"Resolver with error handling\"\"\"\n    try:\n        config = await load_config(agent_name)\n        if not config:\n            logging.info(f\"Agent '{agent_name}' not found\")\n            return None\n\n        agent = BaseAgent(**config)\n        logging.info(f\"Created agent '{agent_name}'\")\n        return agent\n\n    except Exception as e:\n        logging.error(f\"Failed to resolve agent '{agent_name}': {e}\")\n        return None\n</code></pre>"},{"location":"dynamic-agents/#see-also","title":"See Also","text":"<ul> <li>Server Overview - Basic server setup</li> <li>Agent Overview - Agent setup options</li> <li>Server Architecture - Production deployment</li> </ul>"},{"location":"license/","title":"MIT License","text":"<p>Copyright (c) 2025 Robutler Corporation</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"quickstart/","title":"Python SDK Quickstart","text":"<p>Get started with WebAgents in 5 minutes - create, run, and serve your first AI agent.</p> <p>Beta Software Notice</p> <p>WebAgents is currently in beta stage. While the core functionality is stable and actively used, APIs and features may change. We recommend testing thoroughly before deploying to critical environments.</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<pre><code># Core framework\npip install webagents\n\n# With ecosystem skills (optional)\npip install webagents[ecosystem]\n</code></pre>"},{"location":"quickstart/#create-your-first-agent","title":"Create Your First Agent","text":"<pre><code>from webagents.agents.core.base_agent import BaseAgent\n\n# Create a basic agent\nagent = BaseAgent(\n    name=\"assistant\",\n    instructions=\"You are a helpful AI assistant.\",\n    model=\"openai/gpt-4o-mini\"  # Automatically creates LLM skill\n)\n\n# Run chat completion\nmessages = [{\"role\": \"user\", \"content\": \"Hello! What can you help me with?\"}]\nresponse = await agent.run(messages=messages)\nprint(response.content)\n</code></pre>"},{"location":"quickstart/#serve-your-agent","title":"Serve Your Agent","text":"<p>Deploy your agent as an OpenAI-compatible API server:</p> <pre><code>from webagents.server.core.app import create_server\nimport uvicorn\n\n# Create server with your agent\nserver = create_server(agents=[agent])\n\n# Run the server\nuvicorn.run(server.app, host=\"0.0.0.0\", port=8000)\n</code></pre> <p>Test your agent API: <pre><code>curl -X POST http://localhost:8000/assistant/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n</code></pre></p>"},{"location":"quickstart/#environment-setup","title":"Environment Setup","text":"<p>Set up your API keys for LLM providers:</p> <pre><code># Required for OpenAI models\nexport OPENAI_API_KEY=\"your-openai-key\"\n\n# Optional for other providers\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\nexport WEBAGENTS_API_KEY=\"your-robutler-key\"\n</code></pre>"},{"location":"quickstart/#add-skills","title":"Add Skills","text":"<p>Enhance your agent with platform capabilities:</p> <pre><code>from webagents.agents.core.base_agent import BaseAgent\nfrom webagents.agents.skills.robutler.nli.skill import NLISkill\nfrom webagents.agents.skills.robutler.auth.skill import AuthSkill\nfrom webagents.agents.skills.robutler.discovery.skill import DiscoverySkill\nfrom webagents.agents.skills.robutler.payments.skill import PaymentSkill\n\n# Create an enhanced agent with platform skills\nagent = BaseAgent(\n    name=\"enhanced-assistant\",\n    instructions=\"You are a powerful AI assistant connected to the agent network.\",\n    model=\"openai/gpt-4o-mini\",\n    skills={\n        \"nli\": NLISkill(),           # Natural language communication\n        \"auth\": AuthSkill(),         # Secure authentication\n        \"discovery\": DiscoverySkill(), # Agent discovery\n        \"payments\": PaymentSkill()   # Monetization\n    }\n)\n</code></pre> <p>With these four skills added, your agent becomes part of the connected agent ecosystem. The NLI skill enables natural language communication with other agents - your agent can delegate tasks by simply describing what it needs. The Auth skill provides secure authentication and scope-based access control for agent-to-agent interactions.</p> <p>The Discovery skill acts like DNS for agents, allowing real-time discovery of other agents through intent matching without manual integration. Finally, the Payment skill enables automatic monetization with billing, credits, and micropayments handled seamlessly by the platform.</p>"},{"location":"quickstart/#learn-more","title":"Learn More","text":"<ul> <li>Agent Architecture - Understand how agents work</li> <li>Skills Framework - Modular capabilities system</li> <li>Server Deployment - Production server setup</li> <li>Custom Skills - Build your own capabilities </li> </ul>"},{"location":"server-architecture/","title":"Server Architecture","text":"<p>Production server architecture and deployment patterns for Robutler V2.</p> <p>Beta Software</p> <p>Robutler is in beta. APIs may change. Test thoroughly before production deployment.</p>"},{"location":"server-architecture/#architecture-overview","title":"Architecture Overview","text":"<p>The Robutler server is built on FastAPI with these core components:</p> <ul> <li>Agent Manager - Routes requests to appropriate agents</li> <li>Skill Registry - Manages agent capabilities and tools</li> <li>Context Manager - Handles request context and user sessions</li> <li>LLM Proxy - Integrates with OpenAI, Anthropic, and other providers</li> </ul>"},{"location":"server-architecture/#request-flow","title":"Request Flow","text":"<ol> <li>Request arrives at FastAPI server</li> <li>Authentication validates API keys and user identity</li> <li>Routing selects agent based on URL path</li> <li>Context creates request context with user information</li> <li>Execution runs agent with skills and LLM integration</li> <li>Response returns streaming or batch results</li> </ol>"},{"location":"server-architecture/#configuration","title":"Configuration","text":""},{"location":"server-architecture/#environment-variables","title":"Environment Variables","text":"<pre><code># Server\nROBUTLER_HOST=0.0.0.0\nROBUTLER_PORT=8000\nROBUTLER_LOG_LEVEL=INFO\n\n# LLM Providers\nOPENAI_API_KEY=your-openai-key\nANTHROPIC_API_KEY=your-anthropic-key\n\n# Optional Features\nDATABASE_URL=postgresql://user:pass@host/db\nREDIS_URL=redis://localhost:6379\nPROMETHEUS_ENABLED=true\n</code></pre>"},{"location":"server-architecture/#server-configuration","title":"Server Configuration","text":"<pre><code>from webagents.server.core.app import create_server\n\nserver = create_server(\n    title=\"Production Server\",\n    agents=agents,\n    enable_monitoring=True,\n    enable_cors=True,\n    request_timeout=300\n)\n</code></pre>"},{"location":"server-architecture/#production-patterns","title":"Production Patterns","text":""},{"location":"server-architecture/#multi-agent-server","title":"Multi-Agent Server","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.server.core.app import create_server\n\ndef create_production_server():\n    agents = [\n        BaseAgent(name=\"support\", model=\"openai/gpt-4o\"),\n        BaseAgent(name=\"sales\", model=\"openai/gpt-4o\"),\n        BaseAgent(name=\"analyst\", model=\"anthropic/claude-3-sonnet\")\n    ]\n\n    return create_server(\n        title=\"Production Multi-Agent Server\",\n        agents=agents,\n        url_prefix=\"/api/v1\",\n        enable_monitoring=True\n    )\n\nif __name__ == \"__main__\":\n    import uvicorn\n    server = create_production_server()\n    uvicorn.run(server.app, host=\"0.0.0.0\", port=8000, workers=4)\n</code></pre>"},{"location":"server-architecture/#dynamic-agent-loading","title":"Dynamic Agent Loading","text":"<pre><code>async def resolve_agent(agent_name: str):\n    \"\"\"Load agent configuration from database/API\"\"\"\n    config = await load_agent_config(agent_name)\n    if config:\n        return BaseAgent(**config)\n    return None\n\nserver = create_server(\n    agents=static_agents,\n    dynamic_agents=resolve_agent\n)\n</code></pre>"},{"location":"server-architecture/#monitoring","title":"Monitoring","text":""},{"location":"server-architecture/#health-checks","title":"Health Checks","text":"<pre><code># Built-in endpoints\nGET /health              # Server health\nGET /{agent}/health      # Agent health\n</code></pre>"},{"location":"server-architecture/#metrics","title":"Metrics","text":"<p>Enable Prometheus metrics:</p> <pre><code>server = create_server(\n    agents=agents,\n    enable_prometheus=True\n)\n</code></pre> <p>Access metrics at <code>/metrics</code> endpoint.</p>"},{"location":"server-architecture/#logging","title":"Logging","text":"<p>Configure structured logging:</p> <pre><code>import logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n</code></pre>"},{"location":"server-architecture/#deployment","title":"Deployment","text":""},{"location":"server-architecture/#production-server","title":"Production Server","text":"<pre><code>import uvicorn\nfrom webagents.server.core.app import create_server\n\ndef main():\n    server = create_production_server()\n    uvicorn.run(\n        server.app,\n        host=\"0.0.0.0\",\n        port=8000,\n        workers=4,\n        access_log=True\n    )\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"server-architecture/#security","title":"Security","text":""},{"location":"server-architecture/#api-authentication","title":"API Authentication","text":"<pre><code># Using AuthSkill for automatic authentication\nfrom webagents.agents.skills.robutler.auth import AuthSkill\n\nagent = BaseAgent(\n    name=\"secure-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\"auth\": AuthSkill()}\n)\n</code></pre>"},{"location":"server-architecture/#cors-configuration","title":"CORS Configuration","text":"<pre><code>server = create_server(\n    agents=agents,\n    enable_cors=True,\n    cors_origins=[\"https://yourdomain.com\"]\n)\n</code></pre>"},{"location":"server-architecture/#performance-tuning","title":"Performance Tuning","text":""},{"location":"server-architecture/#concurrency","title":"Concurrency","text":"<pre><code># Multiple workers for CPU-bound tasks\nuvicorn main:server.app --workers 4 --worker-class uvicorn.workers.UvicornWorker\n\n# Async for I/O-bound tasks\nuvicorn main:server.app --loop asyncio --http httptools\n</code></pre>"},{"location":"server-architecture/#resource-limits","title":"Resource Limits","text":"<pre><code>server = create_server(\n    agents=agents,\n    request_timeout=300,\n    max_request_size=\"10MB\"\n)\n</code></pre>"},{"location":"server-architecture/#best-practices","title":"Best Practices","text":"<ol> <li>Environment Variables - Use env vars for configuration</li> <li>Health Checks - Implement proper health endpoints</li> <li>Logging - Use structured logging for observability</li> <li>Resource Limits - Set appropriate timeouts and limits</li> <li>Monitoring - Enable metrics collection</li> <li>Security - Use authentication and CORS properly</li> </ol>"},{"location":"server-architecture/#see-also","title":"See Also","text":"<ul> <li>Server Overview - Basic server setup</li> <li>Dynamic Agents - Runtime agent loading</li> <li>Agent Skills - Agent capabilities</li> </ul>"},{"location":"server/","title":"Server Overview","text":"<p>Deploy agents as OpenAI-compatible API servers using the Robutler FastAPI server.</p>"},{"location":"server/#quick-start","title":"Quick Start","text":""},{"location":"server/#basic-server","title":"Basic Server","text":"<pre><code>from webagents.server.core.app import create_server\nfrom webagents.agents import BaseAgent\n\n# Create agent\nagent = BaseAgent(\n    name=\"assistant\",\n    instructions=\"You are a helpful assistant\",\n    model=\"openai/gpt-4o\"\n)\n\n# Create and run server\nserver = create_server(agents=[agent])\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(server.app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"server/#multiple-agents","title":"Multiple Agents","text":"<pre><code>from webagents.agents.skills.core.memory import ShortTermMemorySkill\n\n# Create multiple agents\nagents = [\n    BaseAgent(\n        name=\"support\",\n        instructions=\"You are a customer service agent\",\n        model=\"openai/gpt-4o\",\n        skills={\"memory\": ShortTermMemorySkill()}\n    ),\n    BaseAgent(\n        name=\"analyst\",\n        instructions=\"You are a data analyst\",\n        model=\"anthropic/claude-3-sonnet\"\n    )\n]\n\n# Create server with multiple agents\nserver = create_server(\n    title=\"Multi-Agent Server\",\n    agents=agents\n)\n</code></pre>"},{"location":"server/#server-parameters","title":"Server Parameters","text":"<p>The <code>create_server()</code> function accepts these key parameters:</p> Parameter Type Default Description <code>title</code> str \"WebAgents Server\" Server title for OpenAPI docs <code>description</code> str \"AI Agent Server...\" Server description <code>version</code> str \"1.0.0\" API version <code>agents</code> List[BaseAgent] [] Static agents to serve <code>dynamic_agents</code> Callable None Dynamic agent resolver function <code>url_prefix</code> str \"\" URL prefix (e.g., \"/agents\")"},{"location":"server/#advanced-parameters","title":"Advanced Parameters","text":"<pre><code>server = create_server(\n    title=\"Production Server\",\n    agents=agents,\n    dynamic_agents=resolve_agent,\n    url_prefix=\"/api/v1\",\n    enable_monitoring=True,\n    enable_cors=True,\n    request_timeout=300.0\n)\n</code></pre>"},{"location":"server/#api-endpoints","title":"API Endpoints","text":"<p>The server automatically creates these endpoints for each agent:</p> <pre><code>GET  /                              # Server info\nGET  /health                        # Health check\nGET  /{agent_name}                  # Agent info\nPOST /{agent_name}/chat/completions # OpenAI-compatible chat\nGET  /{agent_name}/health           # Agent health\n</code></pre> <p>With <code>url_prefix=\"/agents\"</code>: <pre><code>POST /agents/{agent_name}/chat/completions\n</code></pre></p>"},{"location":"server/#client-usage","title":"Client Usage","text":""},{"location":"server/#openai-sdk-compatible","title":"OpenAI SDK Compatible","text":"<pre><code>import openai\n\nclient = openai.OpenAI(\n    base_url=\"http://localhost:8000/assistant\",\n    api_key=\"your-api-key\"  # Optional\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre>"},{"location":"server/#streaming","title":"Streaming","text":"<pre><code>stream = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\")\n</code></pre>"},{"location":"server/#environment-variables","title":"Environment Variables","text":"<pre><code># LLM Provider Keys\nOPENAI_API_KEY=your-openai-key\nANTHROPIC_API_KEY=your-anthropic-key\n\n# Optional Server Configuration\nROBUTLER_HOST=0.0.0.0\nROBUTLER_PORT=8000\n</code></pre>"},{"location":"server/#see-also","title":"See Also","text":"<ul> <li>Dynamic Agents - Runtime agent loading</li> <li>Agent Endpoints - Custom HTTP endpoints</li> <li>Agent Overview - Agent setup guide</li> </ul>"},{"location":"agent/endpoints/","title":"Agent Endpoints","text":"<p>Expose custom HTTP API endpoints for your agent using the <code>@http</code> decorator. Endpoints are mounted under the agent\u2019s base path and are served by the same FastAPI app used for chat completions.</p> <ul> <li>Simple, declarative decorator: <code>@http(\"/path\", method=\"get|post\", scope=\"...\")</code></li> <li>Path parameters and query strings supported</li> <li>Scope-based access control (<code>all</code>, <code>owner</code>, <code>admin</code>)</li> <li>Plays nicely with skills, tools, and hooks</li> </ul>"},{"location":"agent/endpoints/#basic-usage","title":"Basic Usage","text":"<p>Define an endpoint and attach it to your agent via <code>capabilities</code> (auto-registration):</p> <pre><code>from webagents.agents.core.base_agent import BaseAgent\nfrom webagents.agents.tools.decorators import http\n\n@http(\"/status\", method=\"get\")\ndef get_status() -&gt; dict:\n    return {\"status\": \"healthy\"}\n\nagent = BaseAgent(\n    name=\"assistant\",\n    model=\"openai/gpt-4o-mini\",\n    capabilities=[get_status]\n)\n</code></pre> <p>Serve it (same as in Quickstart):</p> <pre><code>from webagents.server.core.app import create_server\nimport uvicorn\n\nserver = create_server(agents=[agent])\nuvicorn.run(server.app, host=\"0.0.0.0\", port=8000)\n</code></pre> <p>Available at: - <code>GET /assistant/status</code></p>"},{"location":"agent/endpoints/#methods-path-and-query","title":"Methods, Path and Query","text":"<pre><code>from webagents.agents.tools.decorators import http\n\n# GET collection\n@http(\"/users\", method=\"get\")\ndef list_users() -&gt; dict:\n    return {\"users\": [\"alice\", \"bob\", \"charlie\"]}\n\n# POST create with JSON body\n@http(\"/users\", method=\"post\")\ndef create_user(data: dict) -&gt; dict:\n    return {\"created\": data.get(\"name\"), \"id\": \"user_123\"}\n\n# GET item with path param and optional query param\n@http(\"/users/{user_id}\", method=\"get\")\ndef get_user(user_id: str, include_details: bool = False) -&gt; dict:\n    user = {\"id\": user_id, \"name\": f\"User {user_id}\"}\n    if include_details:\n        user[\"details\"] = \"Extended info\"\n    return user\n</code></pre> <p>Example requests:</p> <pre><code># List users\ncurl http://localhost:8000/assistant/users\n\n# Create user\ncurl -X POST http://localhost:8000/assistant/users \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"name\": \"dana\"}'\n\n# Get user with query param\ncurl \"http://localhost:8000/assistant/users/42?include_details=true\"\n\n# Missing or wrong Content-Type\ncurl -X POST http://localhost:8000/assistant/users -d '{\"name\":\"dana\"}'\n# -&gt; 415 Unsupported Media Type\n\n# Wrong method\ncurl -X GET http://localhost:8000/assistant/users -H \"Content-Type: application/json\" -d '{}'\n# -&gt; 405 Method Not Allowed\n\n# Unauthorized scope (example)\ncurl http://localhost:8000/assistant/admin/metrics\n# -&gt; 403 Forbidden\n</code></pre>"},{"location":"agent/endpoints/#access-control-scopes","title":"Access Control (Scopes)","text":"<p>Use <code>scope</code> to restrict who can call an endpoint:</p> <pre><code>@http(\"/public\", method=\"get\", scope=\"all\")\ndef public_endpoint() -&gt; dict:\n    return {\"message\": \"Public data\"}\n\n@http(\"/owner-info\", method=\"get\", scope=\"owner\")\ndef owner_endpoint() -&gt; dict:\n    return {\"private\": \"owner data\"}\n\n@http(\"/admin/metrics\", method=\"get\", scope=\"admin\")\ndef admin_metrics() -&gt; dict:\n    return {\"rps\": 100, \"error_rate\": 0.001}\n</code></pre>"},{"location":"agent/endpoints/#tips","title":"Tips","text":"<ul> <li>Keep one responsibility per endpoint (CRUD-style patterns work well)</li> <li>Prefer <code>get</code> for retrieval, <code>post</code> for creation/processing</li> <li>Validate inputs inside handlers; return JSON-serializable data</li> <li>Register endpoints through <code>capabilities=[...]</code> along with <code>@tool</code>/<code>@hook</code>/<code>@handoff</code></li> </ul>"},{"location":"agent/endpoints/#see-also","title":"See Also","text":"<ul> <li>Quickstart \u2014 serving agents</li> <li>Agent Skills \u2014 modular capabilities</li> <li>Tools \u2014 add executable functions</li> <li>Hooks \u2014 lifecycle integration</li> </ul>"},{"location":"agent/handoffs/","title":"Agent Handoffs","text":"<p>Unified Handoff System</p> <p>The handoff system provides a unified interface for both local LLM completions and remote agent handoffs, with automatic streaming support and priority-based handler selection.</p> <p>Handoffs enable seamless completion handling through a unified interface that supports:</p> <ul> <li>Local LLM completions (via LiteLLM, OpenAI, etc.)</li> <li>Remote agent communication (via NLI)</li> <li>Automatic streaming/non-streaming adaptation</li> <li>Priority-based handler selection</li> <li>Dynamic prompt injection</li> </ul>"},{"location":"agent/handoffs/#handoff-system-overview","title":"Handoff System Overview","text":"<p>The new handoff system replaces the old hardcoded <code>primary_llm</code> pattern with a flexible, decorator-based approach:</p> <pre><code>from webagents.agents.skills import Skill\nfrom webagents.agents.tools.decorators import handoff\n\nclass CustomLLMSkill(Skill):\n    \"\"\"Custom LLM completion handler\"\"\"\n\n    async def initialize(self, agent):\n        # Register as handoff handler\n        # NOTE: Register streaming function for best compatibility\n        agent.register_handoff(\n            Handoff(\n                target=\"custom_llm\",\n                description=\"Custom LLM using specialized model\",\n                scope=\"all\",\n                metadata={\n                    'function': self.chat_completion_stream,\n                    'priority': 10,\n                    'is_generator': True  # Streaming generator\n                }\n            ),\n            source=\"custom_llm\"\n        )\n\n    async def chat_completion_stream(\n        self,\n        messages: List[Dict[str, Any]],\n        tools: Optional[List[Dict[str, Any]]] = None,\n        **kwargs\n    ) -&gt; AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"Handle LLM completion (streaming)\"\"\"\n        async for chunk in self.my_streaming_llm_api(messages, tools):\n            yield chunk\n</code></pre>"},{"location":"agent/handoffs/#core-concepts","title":"Core Concepts","text":""},{"location":"agent/handoffs/#handoff-dataclass","title":"Handoff Dataclass","text":"<pre><code>from webagents.agents.skills.base import Handoff\n\nHandoff(\n    target: str,              # Handler identifier\n    description: str = \"\",    # Description/prompt for when to use\n    scope: Union[str, List[str]] = \"all\",\n    metadata: Dict[str, Any] = None  # Contains: function, priority, is_generator\n)\n</code></pre>"},{"location":"agent/handoffs/#priority-system","title":"Priority System","text":"<p>Handoffs are selected based on priority (lower = higher priority):</p> <ul> <li>Priority 10: Local LLM handlers (default)</li> <li>Priority 20: Remote agent handlers</li> <li>Priority 50+: Custom/specialized handlers</li> </ul> <p>The first registered handoff (lowest priority) becomes the default completion handler.</p>"},{"location":"agent/handoffs/#streaming-vs-non-streaming","title":"Streaming vs Non-Streaming","text":"<p>The system automatically adapts handlers:</p> <ul> <li>Async generators (<code>async def func() -&gt; AsyncGenerator</code>) = streaming native</li> <li>Regular async functions (<code>async def func() -&gt; Dict</code>) = non-streaming native</li> <li>Automatic adaptation in both directions</li> </ul>"},{"location":"agent/handoffs/#using-the-handoff-decorator","title":"Using the @handoff Decorator","text":""},{"location":"agent/handoffs/#basic-handoff-with-prompt","title":"Basic Handoff with Prompt","text":"<pre><code>from webagents.agents.tools.decorators import handoff\n\nclass SpecializedSkill(Skill):\n    @handoff(\n        name=\"specialist\",\n        prompt=\"Use this handler for complex mathematical computations requiring symbolic processing\",\n        priority=15\n    )\n    async def specialized_completion(\n        self,\n        messages: List[Dict[str, Any]],\n        tools: Optional[List[Dict[str, Any]]] = None,\n        context=None,  # Auto-injected if present in signature\n        **kwargs\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Handle specialized completions\"\"\"\n        result = await self.process_with_specialist(messages)\n        return result\n</code></pre>"},{"location":"agent/handoffs/#streaming-handoff","title":"Streaming Handoff","text":"<p>For streaming responses, use an async generator:</p> <pre><code>class StreamingSkill(Skill):\n    @handoff(\n        name=\"streaming_llm\",\n        prompt=\"Streaming LLM handler for real-time responses\",\n        priority=10\n    )\n    async def streaming_completion(\n        self,\n        messages: List[Dict[str, Any]],\n        tools: Optional[List[Dict[str, Any]]] = None,\n        **kwargs\n    ) -&gt; AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"Stream LLM responses\"\"\"\n        async for chunk in self.my_streaming_api(messages, tools):\n            yield chunk\n</code></pre>"},{"location":"agent/handoffs/#context-injection","title":"Context Injection","text":"<p>The decorator automatically injects <code>context</code> if it's in your function signature:</p> <pre><code>@handoff(name=\"context_aware\", priority=10)\nasync def completion_with_context(\n    self,\n    messages: List[Dict[str, Any]],\n    context=None,  # Auto-injected from request context\n    **kwargs\n) -&gt; Dict[str, Any]:\n    \"\"\"Use context for billing, auth, etc.\"\"\"\n    user_id = context.auth.user_id if context else None\n    return await self.process(messages, user_id=user_id)\n</code></pre>"},{"location":"agent/handoffs/#built-in-handoff-skills","title":"Built-in Handoff Skills","text":""},{"location":"agent/handoffs/#litellmskill-default","title":"LiteLLMSkill (Default)","text":"<p>LiteLLMSkill automatically registers as a handoff handler during initialization:</p> <pre><code>from webagents.agents.skills.core.llm.litellm import LiteLLMSkill\n\n# In dynamic_factory.py or your agent setup\nskills[\"litellm\"] = LiteLLMSkill(model=\"openai/gpt-4o\")\n\n# LiteLLMSkill.initialize() automatically calls:\nagent.register_handoff(\n    Handoff(\n        target=\"litellm_openai_gpt-4o\",\n        description=\"LiteLLM completion handler using openai/gpt-4o\",\n        metadata={'function': self.chat_completion_stream, 'priority': 10, 'is_generator': True}\n    ),\n    source=\"litellm\"\n)\n# NOTE: Registers the streaming function for optimal compatibility in both modes\n</code></pre>"},{"location":"agent/handoffs/#agenthandoffskill-remote-agents","title":"AgentHandoffSkill (Remote Agents)","text":"<p>For handing off to remote agents via NLI with streaming support:</p> <pre><code>from webagents.agents.skills.robutler.handoff import AgentHandoffSkill\n\n# Register remote agent handoff\nskills[\"agent_handoff\"] = AgentHandoffSkill({\n    'agent_url': 'https://robutler.ai/agents/specialist'\n})\n\n# AgentHandoffSkill automatically registers with priority=20\n# Supports streaming via NLI.stream_message()\n</code></pre>"},{"location":"agent/handoffs/#manual-handoff-registration","title":"Manual Handoff Registration","text":"<p>You can also register handoffs manually without decorators:</p> <pre><code>from webagents.agents.skills.base import Handoff\n\nclass MySkill(Skill):\n    async def initialize(self, agent):\n        # Register handoff manually\n        # NOTE: This example shows non-streaming (is_generator=False)\n        # For LLM handlers, prefer streaming (is_generator=True) as shown above\n        agent.register_handoff(\n            Handoff(\n                target=\"my_handler\",\n                description=\"My custom completion handler\",\n                scope=\"owner\",  # Only for owner\n                metadata={\n                    'function': self.my_completion_handler,\n                    'priority': 15,\n                    'is_generator': False  # Non-streaming example\n                }\n            ),\n            source=\"my_skill\"\n        )\n\n    async def my_completion_handler(self, messages, tools=None, **kwargs):\n        # Non-streaming handler that returns a complete response\n        return await self.process(messages)\n</code></pre>"},{"location":"agent/handoffs/#handoff-execution-flow","title":"Handoff Execution Flow","text":""},{"location":"agent/handoffs/#in-baseagent","title":"In BaseAgent","text":"<p>The agent's agentic loop uses the active handoff:</p> <pre><code># Non-streaming mode (agent.run())\nresponse = await self._execute_handoff(\n    self.active_handoff,\n    messages=enhanced_messages,\n    tools=available_tools,\n    stream=False  # Consumes generators to single response\n)\n\n# Streaming mode (agent.run_streaming())\nstream_gen = self._execute_handoff(\n    self.active_handoff,\n    messages=enhanced_messages,\n    tools=available_tools,\n    stream=True  # Wraps regular functions as generators\n)\n\nasync for chunk in stream_gen:\n    yield chunk\n</code></pre>"},{"location":"agent/handoffs/#automatic-adaptation","title":"Automatic Adaptation","text":"<p>The system handles adaptation automatically:</p> <p>Streaming Mode (stream=True): - Generator functions \u2192 called directly, yields chunks - Regular functions \u2192 wrapped to yield single chunk</p> <p>Non-Streaming Mode (stream=False): - Regular functions \u2192 called directly, returns dict - Generator functions \u2192 consumed to single dict response</p> <p>Best Practice: Register Streaming Functions</p> <p>For LLM handlers, always register the streaming generator function (<code>is_generator: True</code>). The system automatically adapts it for non-streaming mode by consuming the generator. This approach:</p> <ul> <li>\u2705 Works in both streaming and non-streaming modes</li> <li>\u2705 Provides real-time feedback when streaming</li> <li>\u2705 Handles tool calls correctly in both modes</li> <li>\u2705 Matches how LiteLLMSkill registers itself</li> </ul> <p>This is the pattern used by <code>LiteLLMSkill</code> which registers <code>chat_completion_stream</code> as the handoff function.</p>"},{"location":"agent/handoffs/#remote-agent-handoffs","title":"Remote Agent Handoffs","text":""},{"location":"agent/handoffs/#using-agenthandoffskill","title":"Using AgentHandoffSkill","text":"<pre><code>from webagents.agents.skills.robutler.handoff import AgentHandoffSkill\nfrom webagents.agents.skills.robutler.nli import NLISkill\n\n# Setup NLI and handoff skills\nskills = {\n    \"nli\": NLISkill(),\n    \"agent_handoff\": AgentHandoffSkill()\n}\n\nagent = BaseAgent(\n    name=\"coordinator\",\n    instructions=\"Coordinate with specialist agents\",\n    skills=skills\n)\n\n# Agent can now hand off to remote agents automatically\n# AgentHandoffSkill uses NLI.stream_message() for streaming\n</code></pre>"},{"location":"agent/handoffs/#nli-streaming-support","title":"NLI Streaming Support","text":"<p>The NLISkill provides <code>stream_message()</code> for SSE streaming from remote agents:</p> <pre><code># Used internally by AgentHandoffSkill\nasync for chunk in nli_skill.stream_message(\n    agent_url=\"https://robutler.ai/agents/specialist\",\n    messages=messages,\n    tools=tools,\n    authorized_amount=0.50\n):\n    # OpenAI-compatible streaming chunks\n    print(chunk)\n</code></pre>"},{"location":"agent/handoffs/#dynamic-prompt-integration","title":"Dynamic Prompt Integration","text":"<p>Handoff prompts automatically integrate with agent system prompts:</p> <pre><code>@handoff(\n    name=\"math_expert\",\n    prompt=\"Use this handler for advanced mathematical problems requiring symbolic computation, calculus, or theorem proving\",\n    priority=15\n)\nasync def math_completion(self, messages, **kwargs):\n    return await self.math_engine.solve(messages)\n</code></pre> <p>The <code>prompt</code> parameter serves dual purposes: 1. Description: Explains when this handoff should be used 2. Dynamic Prompt: Added to agent's system prompt automatically</p>"},{"location":"agent/handoffs/#multi-agent-workflows","title":"Multi-Agent Workflows","text":""},{"location":"agent/handoffs/#conditional-remote-handoffs","title":"Conditional Remote Handoffs","text":"<pre><code>class RouterSkill(Skill):\n    async def initialize(self, agent):\n        self.agent = agent\n\n        # Register conditional handoff\n        agent.register_handoff(\n            Handoff(\n                target=\"specialist_router\",\n                description=\"Route to specialist agents based on query complexity\",\n                metadata={\n                    'function': self.route_to_specialist,\n                    'priority': 12,\n                    'is_generator': True\n                }\n            ),\n            source=\"router\"\n        )\n\n    async def route_to_specialist(\n        self,\n        messages: List[Dict[str, Any]],\n        **kwargs\n    ) -&gt; AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"Conditionally route to specialist agents\"\"\"\n        last_message = messages[-1]['content']\n\n        if self._needs_specialist(last_message):\n            # Hand off to remote specialist\n            specialist_url = await self._discover_specialist(last_message)\n\n            async for chunk in self.agent.skills['agent_handoff'].remote_agent_handoff(\n                messages=messages,\n                agent_url=specialist_url,\n                **kwargs\n            ):\n                yield chunk\n        else:\n            # Use local LLM\n            async for chunk in self.agent.skills['litellm'].chat_completion_stream(\n                messages=messages,\n                **kwargs\n            ):\n                yield chunk\n</code></pre>"},{"location":"agent/handoffs/#cascading-handoffs","title":"Cascading Handoffs","text":"<pre><code>class CascadingSkill(Skill):\n    \"\"\"Try multiple handlers in order until one succeeds\"\"\"\n\n    async def initialize(self, agent):\n        agent.register_handoff(\n            Handoff(\n                target=\"cascading\",\n                description=\"Try multiple handlers with fallback\",\n                metadata={\n                    'function': self.cascading_completion,\n                    'priority': 8,  # Higher priority than defaults\n                    'is_generator': True\n                }\n            ),\n            source=\"cascading\"\n        )\n\n    async def cascading_completion(\n        self,\n        messages: List[Dict[str, Any]],\n        **kwargs\n    ) -&gt; AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"Try handlers in priority order\"\"\"\n        handlers = [\n            ('specialist', 'https://robutler.ai/agents/specialist'),\n            ('generalist', 'https://robutler.ai/agents/generalist'),\n            ('local', None)  # Fallback to local LLM\n        ]\n\n        for name, url in handlers:\n            try:\n                if url:\n                    # Try remote agent\n                    async for chunk in self._stream_from_remote(url, messages):\n                        yield chunk\n                    return  # Success, exit\n                else:\n                    # Fallback to local\n                    async for chunk in self._stream_from_local(messages):\n                        yield chunk\n                    return\n            except Exception as e:\n                self.logger.warning(f\"Handler {name} failed: {e}\")\n                continue\n\n        # All handlers failed\n        yield self._create_error_response(\"All handlers failed\")\n</code></pre>"},{"location":"agent/handoffs/#migration-guide","title":"Migration Guide","text":""},{"location":"agent/handoffs/#legacy-pattern","title":"Legacy Pattern","text":"<pre><code># Legacy: Hardcoded primary_llm\nfrom webagents.agents.skills.core.llm.litellm import LiteLLMSkill\n\nskills = {\n    \"primary_llm\": LiteLLMSkill(model=\"openai/gpt-4o\"),\n    # ... other skills\n}\n\nagent = BaseAgent(name=\"agent\", skills=skills)\n</code></pre>"},{"location":"agent/handoffs/#current-pattern","title":"Current Pattern","text":"<pre><code># Current: Handoff-based (LiteLLMSkill auto-registers)\nfrom webagents.agents.skills.core.llm.litellm import LiteLLMSkill\n\nskills = {\n    \"litellm\": LiteLLMSkill(model=\"openai/gpt-4o\"),  # Auto-registers as handoff\n    # ... other skills\n}\n\nagent = BaseAgent(name=\"agent\", skills=skills)\n# agent.active_handoff is automatically set to LiteLLM (priority=10)\n</code></pre> <p>Key Improvements: - \u2705 No more <code>\"primary_llm\"</code> key required - \u2705 LiteLLMSkill self-registers during <code>initialize()</code> - \u2705 First handoff (lowest priority) = default handler - \u2705 Fully backward compatible for basic usage - \u2705 LiteLLM registers <code>chat_completion_stream</code> (streaming) for optimal compatibility - \u2705 Automatic adaptation between streaming/non-streaming modes</p>"},{"location":"agent/handoffs/#best-practices","title":"Best Practices","text":"<ol> <li>Priority Selection</li> <li>Reserve 1-10 for critical/high-priority handlers</li> <li>Use 10-20 for standard local/remote handlers</li> <li> <p>Use 20+ for specialized/conditional handlers</p> </li> <li> <p>Streaming Support</p> </li> <li>Use async generators for streaming-native handlers</li> <li>System handles adaptation automatically</li> <li> <p>Don't mix streaming/non-streaming in one function</p> </li> <li> <p>Context Usage</p> </li> <li>Add <code>context=None</code> to signature for auto-injection</li> <li>Use for auth, billing, user preferences</li> <li> <p>Don't modify context, it's read-only</p> </li> <li> <p>Error Handling</p> </li> <li>Always handle errors in custom handoffs</li> <li>Provide fallback responses</li> <li> <p>Log failures for debugging</p> </li> <li> <p>Prompt Clarity</p> </li> <li>Make handoff prompts specific and actionable</li> <li>Describe when the handler should be used</li> <li>Include examples of suitable queries</li> </ol>"},{"location":"agent/handoffs/#complete-example","title":"Complete Example","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills import Skill\nfrom webagents.agents.skills.core.llm.litellm import LiteLLMSkill\nfrom webagents.agents.skills.robutler.nli import NLISkill\nfrom webagents.agents.skills.robutler.handoff import AgentHandoffSkill\nfrom webagents.agents.tools.decorators import handoff\nfrom typing import List, Dict, Any, AsyncGenerator\n\nclass IntelligentRouterSkill(Skill):\n    \"\"\"Smart router with fallback chain\"\"\"\n\n    async def initialize(self, agent):\n        self.agent = agent\n\n        # Register as high-priority handler\n        agent.register_handoff(\n            Handoff(\n                target=\"intelligent_router\",\n                description=\"Intelligently route to best handler based on query analysis\",\n                metadata={\n                    'function': self.route_completion,\n                    'priority': 5,  # Highest priority\n                    'is_generator': True\n                }\n            ),\n            source=\"router\"\n        )\n\n    async def route_completion(\n        self,\n        messages: List[Dict[str, Any]],\n        tools: Optional[List[Dict[str, Any]]] = None,\n        context=None,\n        **kwargs\n    ) -&gt; AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"Route to optimal handler\"\"\"\n\n        query = messages[-1]['content']\n        complexity = self._analyze_complexity(query)\n\n        # Route based on complexity\n        if complexity == 'expert':\n            # Use remote specialist\n            specialist_url = await self._find_specialist(query)\n            handler = self.agent.skills['agent_handoff']\n\n            async for chunk in handler.remote_agent_handoff(\n                messages=messages,\n                agent_url=specialist_url,\n                tools=tools,\n                context=context\n            ):\n                yield chunk\n\n        else:\n            # Use local LLM\n            handler = self.agent.skills['litellm']\n            async for chunk in handler.chat_completion_stream(\n                messages=messages,\n                tools=tools,\n                **kwargs\n            ):\n                yield chunk\n\n# Create agent with intelligent routing\nagent = BaseAgent(\n    name=\"smart-agent\",\n    instructions=\"You are a smart agent that routes queries optimally\",\n    skills={\n        \"router\": IntelligentRouterSkill(),\n        \"litellm\": LiteLLMSkill(model=\"openai/gpt-4o\"),\n        \"nli\": NLISkill(),\n        \"agent_handoff\": AgentHandoffSkill()\n    }\n)\n\n# The router takes priority, but falls back to LiteLLM when appropriate\n</code></pre>"},{"location":"agent/handoffs/#api-reference","title":"API Reference","text":"<p>See the Handoff API Reference for complete technical details.</p>"},{"location":"agent/hooks/","title":"Agent Hooks","text":"<p>Hooks provide lifecycle integration points to react to events during request processing. Hooks can be defined in skills or as standalone functions.</p> <p>Hooks are executed in priority order (lower numbers first) and receive the unified request context. Keep hooks small and deterministic; avoid blocking operations and always return the context.</p>"},{"location":"agent/hooks/#hook-types","title":"Hook Types","text":""},{"location":"agent/hooks/#skill-hooks","title":"Skill Hooks","text":"<p>Defined within skills using the <code>@hook</code> decorator:</p> <pre><code>from webagents.agents.skills import Skill\nfrom webagents.agents.skills.decorators import hook\n\nclass MySkill(Skill):\n    @hook(\"on_connection\", priority=10)\n    async def setup_request(self, context):\n        \"\"\"Called when request starts\"\"\"\n        context[\"custom_data\"] = \"value\"\n        return context\n</code></pre>"},{"location":"agent/hooks/#standalone-hooks","title":"Standalone Hooks","text":"<p>Decorated functions that can be passed to agents:</p> <pre><code>from webagents.agents.skills.decorators import hook\nfrom webagents.agents import BaseAgent\n\n@hook(\"on_message\", priority=5)\nasync def log_messages(context):\n    \"\"\"Log all messages\"\"\"\n    print(f\"Message: {context.messages[-1]}\")\n    return context\n\n@hook(\"on_connection\")\nasync def setup_analytics(context):\n    \"\"\"Initialize analytics tracking\"\"\"\n    context[\"session_start\"] = time.time()\n    return context\n\n# Pass to agent\nagent = BaseAgent(\n    name=\"my-agent\",\n    model=\"openai/gpt-4o\",\n    hooks=[log_messages, setup_analytics]\n)\n\n## Available Hooks\n\nHooks are executed in the following order during request processing:\n\n1. **on_connection** - Once per request (initialization)\n2. **before_llm_call** - Before each LLM call in agentic loop\n3. **after_llm_call** - After each LLM response in agentic loop\n4. **on_chunk** - For each streaming chunk (streaming only)\n5. **before_toolcall** - Before each tool execution\n6. **after_toolcall** - After each tool execution\n7. **on_message** - Once per request (before finalization)\n8. **finalize_connection** - Once per request (cleanup)\n\n### on_connection\n\nCalled once when a new request connection is established.\n\nTypical responsibilities:\n- Authentication and identity extraction (e.g., `AuthSkill`)\n- Payment token validation and minimum balance checks (e.g., `PaymentSkill`)\n- Request-scoped initialization (timers, correlation IDs)\n\n```python\n@hook(\"on_connection\")\nasync def on_connection(self, context):\n    \"\"\"Initialize request processing\"\"\"\n    # Access context data\n    user_id = context.peer_user_id\n    is_streaming = context.stream\n\n    # Set up request-specific state\n    context[\"request_start\"] = time.time()\n\n    return context\n</code></pre>"},{"location":"agent/hooks/#on_message","title":"on_message","text":"<p>Called for each message in the conversation.</p> <p>Typical responsibilities: - Lightweight analytics and message enrichment - Intent detection, entity extraction - Safety checks for input/output</p> <pre><code>@hook(\"on_message\")\nasync def on_message(self, context):\n    \"\"\"Process each message\"\"\"\n    # Get current message\n    message = context.messages[-1]\n\n    if message[\"role\"] == \"user\":\n        # Analyze user input\n        context[\"intent\"] = self.analyze_intent(message[\"content\"])\n\n    return context\n</code></pre>"},{"location":"agent/hooks/#before_llm_call","title":"before_llm_call","text":"<p>Called before each LLM call in the agentic loop.</p> <p>Typical responsibilities: - Message preprocessing and transformation - Multimodal content formatting - Conversation history manipulation</p> <pre><code>@hook(\"before_llm_call\", priority=5)\nasync def before_llm_call(self, context):\n    \"\"\"Preprocess messages before LLM\"\"\"\n    messages = context.get('conversation_messages', [])\n\n    # Transform messages (e.g., convert markdown images to multimodal format)\n    processed_messages = self.process_messages(messages)\n    context.set('conversation_messages', processed_messages)\n\n    return context\n</code></pre>"},{"location":"agent/hooks/#after_llm_call","title":"after_llm_call","text":"<p>Called after each LLM response in the agentic loop.</p> <p>Typical responsibilities: - Response post-processing - Cost tracking per iteration - Response validation</p> <pre><code>@hook(\"after_llm_call\", priority=10)\nasync def after_llm_call(self, context):\n    \"\"\"Process LLM response\"\"\"\n    response = context.get('llm_response')\n\n    # Track per-iteration costs\n    usage = response.get('usage', {})\n    await self.track_llm_usage(usage)\n\n    return context\n</code></pre>"},{"location":"agent/hooks/#before_toolcall","title":"before_toolcall","text":"<p>Called before executing a tool.</p> <p>Typical responsibilities: - Security and scope checks - Argument validation/normalization - Rate limiting and auditing</p> <pre><code>@hook(\"before_toolcall\", priority=1)\nasync def before_toolcall(self, context):\n    \"\"\"Validate tool execution\"\"\"\n    tool_call = context[\"tool_call\"]\n    function_name = tool_call[\"function\"][\"name\"]\n\n    # Security check\n    if not self.is_tool_allowed(function_name, context.peer_user_id):\n        # Modify tool call to safe alternative\n        context[\"tool_call\"][\"function\"][\"name\"] = \"tool_blocked\"\n        context[\"tool_call\"][\"function\"][\"arguments\"] = \"{}\"\n\n    return context\n</code></pre>"},{"location":"agent/hooks/#after_toolcall","title":"after_toolcall","text":"<p>Called after tool execution completes.</p> <p>Typical responsibilities: - Post-processing tool results - Adding usage metadata for priced tools - Observability metrics</p> <pre><code>@hook(\"after_toolcall\")\nasync def after_toolcall(self, context):\n    \"\"\"Process tool results\"\"\"\n    tool_result = context[\"tool_result\"]\n    tool_name = context[\"tool_call\"][\"function\"][\"name\"]\n\n    # Log usage\n    await self.log_tool_usage(\n        tool=tool_name,\n        result_size=len(tool_result),\n        user=context.peer_user_id\n    )\n\n    # Enhance result\n    if tool_name == \"search\":\n        context[\"tool_result\"] = self.format_search_results(tool_result)\n\n    return context\n</code></pre>"},{"location":"agent/hooks/#on_chunk","title":"on_chunk","text":"<p>Called for each streaming chunk.</p> <p>Typical responsibilities: - Realtime content filtering - Inline analytics/telemetry</p> <pre><code>@hook(\"on_chunk\")\nasync def on_chunk(self, context):\n    \"\"\"Process streaming chunks\"\"\"\n    chunk = context[\"chunk\"]\n    content = context.get(\"content\", \"\")\n\n    # Real-time content analysis\n    if self.contains_sensitive_info(content):\n        # Redact sensitive content\n        context[\"chunk\"][\"choices\"][0][\"delta\"][\"content\"] = \"[REDACTED]\"\n\n    # Track streaming metrics\n    context[\"chunks_processed\"] = context.get(\"chunks_processed\", 0) + 1\n\n    return context\n</code></pre>"},{"location":"agent/hooks/#before_handoff","title":"before_handoff","text":"<p>Called before handing off to another agent.</p> <pre><code>@hook(\"before_handoff\")\nasync def before_handoff(self, context):\n    \"\"\"Prepare for agent handoff\"\"\"\n    target_agent = context[\"handoff_agent\"]\n\n    # Add handoff metadata\n    context[\"handoff_metadata\"] = {\n        \"source_agent\": context.agent_name,\n        \"timestamp\": time.time(),\n        \"reason\": context.get(\"handoff_reason\")\n    }\n\n    # Validate handoff\n    if not self.can_handoff_to(target_agent):\n        raise HandoffError(f\"Cannot handoff to {target_agent}\")\n\n    return context\n</code></pre>"},{"location":"agent/hooks/#after_handoff","title":"after_handoff","text":"<p>Called after handoff completes.</p> <pre><code>@hook(\"after_handoff\")\nasync def after_handoff(self, context):\n    \"\"\"Process handoff results\"\"\"\n    handoff_result = context[\"handoff_result\"]\n\n    # Log handoff\n    await self.log_handoff(\n        target=context[\"handoff_agent\"],\n        success=handoff_result.get(\"success\"),\n        duration=time.time() - context[\"handoff_metadata\"][\"timestamp\"]\n    )\n\n    return context\n</code></pre>"},{"location":"agent/hooks/#finalize_connection","title":"finalize_connection","text":"<p>Called when request processing completes.</p> <pre><code>@hook(\"finalize_connection\")\nasync def finalize_connection(self, context):\n    \"\"\"Clean up and finalize\"\"\"\n    # Calculate metrics\n    duration = time.time() - context.get(\"request_start\", time.time())\n\n    # Log final metrics\n    await self.log_request_complete(\n        request_id=context.completion_id,\n        duration=duration,\n        tokens=context.get(\"usage\", {}),\n        chunks=context.get(\"chunks_processed\", 0)\n    )\n\n    # Clean up resources\n    self.cleanup_request_resources(context.completion_id)\n\n    return context\n</code></pre>"},{"location":"agent/hooks/#hook-priority","title":"Hook Priority","text":"<p>Hooks execute in priority order (lower numbers first):</p> <pre><code>class SecuritySkill(Skill):\n    @hook(\"on_message\", priority=1)  # Runs first\n    async def security_check(self, context):\n        return context\n\nclass LoggingSkill(Skill):\n    @hook(\"on_message\", priority=10)  # Runs second\n    async def log_message(self, context):\n        return context\n\nclass AnalyticsSkill(Skill):\n    @hook(\"on_message\", priority=20)  # Runs third\n    async def analyze_message(self, context):\n        return context\n</code></pre>"},{"location":"agent/hooks/#context-object","title":"Context Object","text":"<p>The context object provides access to:</p> <pre><code>context = {\n    # Request data\n    \"messages\": List[Dict],          # Conversation messages\n    \"stream\": bool,                  # Streaming enabled\n    \"peer_user_id\": str,            # User identifier\n    \"completion_id\": str,           # Request ID\n    \"model\": str,                   # Model name\n\n    # Agent data\n    \"agent_name\": str,              # Agent name\n    \"agent_skills\": Dict[str, Skill], # Active skills\n\n    # Execution state\n    \"usage\": Dict,                  # Token usage\n    \"tool_calls\": List,             # Tool executions\n\n    # Hook-specific data\n    \"tool_call\": Dict,              # Current tool (before/after_toolcall)\n    \"tool_result\": str,             # Tool result (after_toolcall)\n    \"chunk\": Dict,                  # Current chunk (on_chunk)\n    \"content\": str,                 # Chunk content (on_chunk)\n\n    # Custom data\n    **custom_fields                 # Any custom fields added by hooks\n}\n</code></pre>"},{"location":"agent/hooks/#practical-examples","title":"Practical Examples","text":""},{"location":"agent/hooks/#rate-limiting","title":"Rate Limiting","text":"<pre><code>class RateLimitSkill(Skill):\n    def __init__(self, config=None):\n        super().__init__(config)\n        self.request_counts = {}\n\n    @hook(\"on_connection\", priority=1)\n    async def check_rate_limit(self, context):\n        user_id = context.peer_user_id\n\n        # Check rate limit\n        count = self.request_counts.get(user_id, 0)\n        if count &gt;= 100:  # 100 requests per hour\n            raise RateLimitError(\"Rate limit exceeded\")\n\n        # Increment counter\n        self.request_counts[user_id] = count + 1\n\n        return context\n</code></pre>"},{"location":"agent/hooks/#content-moderation","title":"Content Moderation","text":"<pre><code>class ModerationSkill(Skill):\n    @hook(\"on_message\", priority=5)\n    async def moderate_input(self, context):\n        \"\"\"Filter inappropriate content\"\"\"\n        message = context.messages[-1]\n\n        if message[\"role\"] == \"user\":\n            # Check content\n            if self.is_inappropriate(message[\"content\"]):\n                # Replace with safe message\n                context.messages[-1][\"content\"] = \"I cannot process inappropriate content.\"\n\n        return context\n\n    @hook(\"on_chunk\", priority=5)\n    async def moderate_output(self, context):\n        \"\"\"Filter streaming output\"\"\"\n        content = context.get(\"content\", \"\")\n\n        if self.is_inappropriate(content):\n            # Replace chunk content\n            context[\"chunk\"][\"choices\"][0][\"delta\"][\"content\"] = \"\"\n\n        return context\n</code></pre>"},{"location":"agent/hooks/#analytics-collection","title":"Analytics Collection","text":"<pre><code>class AnalyticsSkill(Skill):\n    @hook(\"on_connection\")\n    async def start_analytics(self, context):\n        context[\"analytics\"] = {\n            \"start_time\": time.time(),\n            \"events\": []\n        }\n        return context\n\n    @hook(\"on_message\")\n    async def track_message(self, context):\n        context[\"analytics\"][\"events\"].append({\n            \"type\": \"message\",\n            \"role\": context.messages[-1][\"role\"],\n            \"timestamp\": time.time()\n        })\n        return context\n\n    @hook(\"before_toolcall\")\n    async def track_tool_start(self, context):\n        context[\"tool_start_time\"] = time.time()\n        return context\n\n    @hook(\"after_toolcall\")\n    async def track_tool_end(self, context):\n        duration = time.time() - context.get(\"tool_start_time\", time.time())\n        context[\"analytics\"][\"events\"].append({\n            \"type\": \"tool\",\n            \"name\": context[\"tool_call\"][\"function\"][\"name\"],\n            \"duration\": duration,\n            \"timestamp\": time.time()\n        })\n        return context\n\n    @hook(\"finalize_connection\")\n    async def send_analytics(self, context):\n        analytics = context.get(\"analytics\", {})\n        analytics[\"total_duration\"] = time.time() - analytics.get(\"start_time\", time.time())\n\n        await self.send_to_analytics_service(analytics)\n        return context\n</code></pre>"},{"location":"agent/hooks/#best-practices","title":"Best Practices","text":"<ol> <li>Always Return Context - Hooks must return the context object</li> <li>Use Priorities Wisely - Order matters for dependent operations</li> <li>Handle Errors Gracefully - Don't break the request flow</li> <li>Keep Hooks Lightweight - Avoid heavy processing</li> <li>Use Context for State - Don't use instance variables for request state </li> </ol>"},{"location":"agent/lifecycle/","title":"Lifecycle","text":"<p>Understanding the request lifecycle and hook system in BaseAgent.</p>"},{"location":"agent/lifecycle/#request-lifecycle","title":"Request Lifecycle","text":"<pre><code>graph TD\n    Request[\"Incoming Request\"] --&gt; Connection[\"on_connection hooks\"]\n    Connection --&gt; Message[\"on_message hooks\"]\n    Message --&gt; Tools{\"Tool calls?\"}\n    Tools --&gt;|Yes| BeforeTool[\"before_toolcall hooks\"]\n    BeforeTool --&gt; Execute[\"Execute tool\"]\n    Execute --&gt; AfterTool[\"after_toolcall hooks\"]\n\n    Message --&gt; Handoff{\"Handoff needed?\"}\n    Handoff --&gt;|Yes| BeforeHandoff[\"before_handoff hooks\"]\n    BeforeHandoff --&gt; RouteAgent[\"Route to agent\"]\n    RouteAgent --&gt; AfterHandoff[\"after_handoff hooks\"]\n\n    Tools --&gt; Response[\"Generate response\"]\n    Handoff --&gt; Response\n    Response --&gt; Chunks[\"on_chunk hooks\"]\n    Chunks --&gt; Finalize[\"finalize_connection hooks\"]</code></pre>"},{"location":"agent/lifecycle/#lifecycle-hooks","title":"Lifecycle Hooks","text":""},{"location":"agent/lifecycle/#available-hooks","title":"Available Hooks","text":"<ol> <li>on_connection - Request initialized</li> <li>on_message - Each message processed</li> <li>before_toolcall - Before tool execution</li> <li>after_toolcall - After tool execution</li> <li>on_chunk - Each streaming chunk</li> <li>before_handoff - Before agent handoff</li> <li>after_handoff - After agent handoff</li> <li>finalize_connection - Request complete</li> </ol> <p>Note</p> <p><code>finalize_connection</code> runs for cleanup even when a prior hook raises a structured error (for example, a 402 payment/auth error). Implement finalize hooks to be idempotent and safe when required context (like a payment token) is missing.</p>"},{"location":"agent/lifecycle/#hook-registration","title":"Hook Registration","text":"<pre><code>from webagents.agents.skills import Skill\nfrom webagents.agents.skills.decorators import hook\n\nclass AnalyticsSkill(Skill):\n    @hook(\"on_connection\", priority=10)\n    async def track_request(self, context):\n        \"\"\"Track incoming request\"\"\"\n        print(f\"New request: {context.completion_id}\")\n        return context\n\n    @hook(\"on_message\", priority=20)\n    async def analyze_message(self, context):\n        \"\"\"Analyze each message\"\"\"\n        message = context.messages[-1]\n        print(f\"Message role: {message['role']}\")\n        return context\n\n    @hook(\"on_chunk\", priority=30)\n    async def monitor_streaming(self, context):\n        \"\"\"Monitor streaming chunks\"\"\"\n        chunk_size = len(context.get(\"content\", \"\"))\n        print(f\"Chunk size: {chunk_size}\")\n        return context\n</code></pre>"},{"location":"agent/lifecycle/#hook-priority","title":"Hook Priority","text":"<p>Hooks execute in priority order (lower numbers first):</p> <pre><code>class SecuritySkill(Skill):\n    @hook(\"before_toolcall\", priority=1)  # Runs first\n    async def validate_security(self, context):\n        \"\"\"Security check before tools\"\"\"\n        tool_name = context[\"tool_call\"][\"function\"][\"name\"]\n        if self.is_dangerous(tool_name):\n            raise SecurityError(\"Tool blocked\")\n        return context\n\nclass LoggingSkill(Skill):\n    @hook(\"before_toolcall\", priority=10)  # Runs second\n    async def log_tool_usage(self, context):\n        \"\"\"Log tool execution\"\"\"\n        self.log_tool(context[\"tool_call\"])\n        return context\n</code></pre>"},{"location":"agent/lifecycle/#context-during-lifecycle","title":"Context During Lifecycle","text":""},{"location":"agent/lifecycle/#connection-context","title":"Connection Context","text":"<pre><code>@hook(\"on_connection\")\nasync def on_connect(self, context):\n    # Available in context:\n    # - messages: List[Dict]\n    # - stream: bool\n    # - peer_user_id: str\n    # - completion_id: str\n    # - model: str\n    # - agent_name: str\n    # - agent_skills: Dict[str, Skill]\n    return context\n</code></pre>"},{"location":"agent/lifecycle/#message-context","title":"Message Context","text":"<pre><code>@hook(\"on_message\")\nasync def on_msg(self, context):\n    # Same as connection + current message\n    current_message = context.messages[-1]\n    role = current_message[\"role\"]\n    content = current_message[\"content\"]\n    return context\n</code></pre>"},{"location":"agent/lifecycle/#tool-context","title":"Tool Context","text":"<pre><code>@hook(\"before_toolcall\")\nasync def before_tool(self, context):\n    # Additional context:\n    # - tool_call: Dict with function details\n    # - tool_id: str\n    return context\n\n@hook(\"after_toolcall\")\nasync def after_tool(self, context):\n    # Additional context:\n    # - tool_result: str (execution result)\n    return context\n</code></pre>"},{"location":"agent/lifecycle/#streaming-context","title":"Streaming Context","text":"<pre><code>@hook(\"on_chunk\")\nasync def on_chunk(self, context):\n    # Additional context:\n    # - chunk: Dict (OpenAI format)\n    # - content: str (chunk content)\n    # - chunk_index: int\n    # - full_content: str (accumulated)\n    return context\n</code></pre>"},{"location":"agent/lifecycle/#practical-examples","title":"Practical Examples","text":""},{"location":"agent/lifecycle/#request-logging","title":"Request Logging","text":"<pre><code>class RequestLogger(Skill):\n    @hook(\"on_connection\")\n    async def start_logging(self, context):\n        self.start_time = time.time()\n        self.request_id = context.completion_id\n        await self.log_request_start(context)\n        return context\n\n    @hook(\"finalize_connection\")\n    async def end_logging(self, context):\n        duration = time.time() - self.start_time\n        await self.log_request_complete(\n            self.request_id,\n            duration,\n            context.get(\"usage\", {})\n        )\n        return context\n</code></pre>"},{"location":"agent/lifecycle/#content-filtering","title":"Content Filtering","text":"<pre><code>class ContentFilter(Skill):\n    @hook(\"on_message\", priority=5)\n    async def filter_input(self, context):\n        \"\"\"Filter inappropriate input\"\"\"\n        message = context.messages[-1]\n        if message[\"role\"] == \"user\":\n            filtered = self.filter_content(message[\"content\"])\n            context.messages[-1][\"content\"] = filtered\n        return context\n\n    @hook(\"on_chunk\", priority=5)\n    async def filter_output(self, context):\n        \"\"\"Filter streaming output\"\"\"\n        content = context.get(\"content\", \"\")\n        if self.is_inappropriate(content):\n            context[\"chunk\"][\"choices\"][0][\"delta\"][\"content\"] = \"[filtered]\"\n        return context\n</code></pre>"},{"location":"agent/lifecycle/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>class PerformanceMonitor(Skill):\n    def __init__(self, config=None):\n        super().__init__(config)\n        self.metrics = {}\n\n    @hook(\"before_toolcall\")\n    async def start_timer(self, context):\n        tool_id = context[\"tool_id\"]\n        self.metrics[tool_id] = {\"start\": time.time()}\n        return context\n\n    @hook(\"after_toolcall\")\n    async def record_duration(self, context):\n        tool_id = context[\"tool_id\"]\n        duration = time.time() - self.metrics[tool_id][\"start\"]\n        await self.record_metric(\n            \"tool_duration\",\n            duration,\n            {\"tool\": context[\"tool_call\"][\"function\"][\"name\"]}\n        )\n        return context\n</code></pre>"},{"location":"agent/lifecycle/#best-practices","title":"Best Practices","text":"<ol> <li>Use Priorities - Order hooks appropriately</li> <li>Return Context - Always return modified context</li> <li>Handle Errors - Gracefully handle exceptions</li> <li>Minimize Overhead - Keep hooks lightweight</li> <li>Thread Safety - Use context vars for state </li> </ol>"},{"location":"agent/overview/","title":"Agent Overview","text":"<p>BaseAgent is the core class for creating AI agents in WebAgents. It uses a flexible, skill-based architecture so you can add exactly the capabilities you need. Agents speak OpenAI's Chat Completions dialect, so existing clients work out of the box. The skill system adds platform features like authentication, payments, discovery, and multi-agent collaboration.</p> <ul> <li>Build an agent with a few lines of code</li> <li>Add capabilities via skills (tools, hooks, prompts, handoffs)</li> <li>Serve OpenAI-compatible endpoints with create_server</li> </ul>"},{"location":"agent/overview/#creating-agents","title":"Creating Agents","text":""},{"location":"agent/overview/#basic-agent","title":"Basic Agent","text":"<pre><code>from webagents.agents import BaseAgent\n\nagent = BaseAgent(\n    name=\"my-assistant\",\n    instructions=\"You are a helpful assistant\",\n    model=\"openai/gpt-4o\"  # Smart model parameter\n)\n</code></pre> <p>New to WebAgents? Check out the Quickstart Guide for a complete walkthrough.</p>"},{"location":"agent/overview/#agent-with-skills","title":"Agent with Skills","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills import ShortTermMemorySkill, DiscoverySkill\n\nagent = BaseAgent(\n    name=\"advanced-assistant\",\n    instructions=\"You are an advanced assistant with memory\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"memory\": ShortTermMemorySkill({\"max_messages\": 50}),\n        \"discovery\": DiscoverySkill()  # Find other agents\n    }\n)\n</code></pre> <p>Skills</p> <p>Explore available skills in the Skills Repository or learn to create custom skills.</p>"},{"location":"agent/overview/#smart-model-parameter","title":"Smart Model Parameter","text":"<p>The <code>model</code> parameter supports multiple formats. If you pass a provider-prefixed string (e.g., <code>openai/\u2026</code>), the correct LLM skill is provisioned automatically. You can always pass a fully configured skill instance for custom behavior.</p> <pre><code># Explicit skill/model format\nagent = BaseAgent(model=\"openai/gpt-4o\")         # OpenAI GPT-4o\nagent = BaseAgent(model=\"anthropic/claude-3\")    # Anthropic Claude\nagent = BaseAgent(model=\"litellm/gpt-4\")         # Via LiteLLM proxy\nagent = BaseAgent(model=\"xai/grok-beta\")         # xAI Grok\n\n# Custom skill instance\nfrom webagents.agents.skills import OpenAISkill\nagent = BaseAgent(model=OpenAISkill({\n    \"api_key\": \"sk-...\",\n    \"temperature\": 0.7\n}))\n</code></pre> <p>See LLM Skills for more configuration options.</p>"},{"location":"agent/overview/#running-agents","title":"Running Agents","text":""},{"location":"agent/overview/#basic-conversation","title":"Basic Conversation","text":"<pre><code>response = await agent.run([\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n])\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"agent/overview/#streaming-response","title":"Streaming Response","text":"<pre><code>async for chunk in agent.run_streaming([\n    {\"role\": \"user\", \"content\": \"Tell me a story\"}\n]):\n    print(chunk.choices[0].delta.content, end=\"\")\n</code></pre>"},{"location":"agent/overview/#with-tools","title":"With Tools","text":"<p>Attach additional tools per request using the OpenAI function-calling format:</p> <pre><code># External tools can be passed per request\nresponse = await agent.run(\n    messages=[{\"role\": \"user\", \"content\": \"Calculate 42 * 17\"}],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"calculator\",\n            \"description\": \"Calculate math expressions\",\n            \"parameters\": {...}\n        }\n    }]\n)\n</code></pre> <p>Learn more about creating tools and the OpenAI function calling format.</p>"},{"location":"agent/overview/#agent-capabilities","title":"Agent Capabilities","text":""},{"location":"agent/overview/#skills","title":"Skills","text":"<p>Skills provide modular capabilities:</p> <ul> <li>LLM Skills - Language model providers (OpenAI, Anthropic, LiteLLM)</li> <li>Memory Skills - Conversation persistence and context management</li> <li>Platform Skills - WebAgents platform integration (auth, payments, discovery)</li> <li>Ecosystem Skills - External services (database, filesystem, APIs)</li> </ul>"},{"location":"agent/overview/#tools","title":"Tools","text":"<p>Tools are executable functions that extend agent capabilities:</p> <pre><code>from webagents.agents.tools.decorators import tool\n\nclass MySkill(Skill):\n    @tool\n    def my_function(self, param: str) -&gt; str:\n        \"\"\"Tool description\"\"\"\n        return f\"Result: {param}\"\n</code></pre> <p>See comprehensive tool examples and best practices.</p>"},{"location":"agent/overview/#hooks","title":"Hooks","text":"<p>Lifecycle hooks enable event-driven behavior during request processing:</p> <pre><code>from webagents.agents.skills.decorators import hook\n\nclass MySkill(Skill):\n    @hook(\"on_message\")\n    async def process_message(self, context):\n        \"\"\"Process each message\"\"\"\n        return context\n</code></pre> <p>Learn about available hook events and the agent lifecycle.</p>"},{"location":"agent/overview/#handoffs","title":"Handoffs","text":"<p>Handoffs enable agents to delegate completions to specialized handlers or remote agents:</p> <pre><code>from webagents.agents.skills import Skill\nfrom webagents.agents.tools.decorators import handoff\n\nclass SpecializedSkill(Skill):\n    @handoff(\n        name=\"math_expert\",\n        prompt=\"Use for advanced mathematical problems\",\n        priority=15\n    )\n    async def math_completion(self, messages, tools=None, **kwargs):\n        \"\"\"Handle math-focused completions\"\"\"\n        async for chunk in self.specialized_math_llm(messages):\n            yield chunk\n</code></pre> <p>Explore handoff patterns, agent discovery, and remote agent communication.</p>"},{"location":"agent/overview/#context-management","title":"Context Management","text":"<p>Context Management</p> <p>Agents maintain a unified context object throughout execution via <code>contextvars</code>. Skills read and write to this thread-safe structure, avoiding globals while remaining fully async-compatible.</p> <pre><code># Within a skill\ncontext = self.get_context()\nuser_id = context.peer_user_id\nmessages = context.messages\nstreaming = context.stream\n</code></pre>"},{"location":"agent/overview/#agent-registration","title":"Agent Registration","text":"<p>Register agents with the server to make them available via HTTP endpoints:</p> <pre><code>from webagents.server.core.app import create_server\nimport uvicorn\n\n# Create server with your agents\nserver = create_server(agents=[agent])\n\n# Or multiple agents\nserver = create_server(agents=[agent1, agent2])\n\n# Run the server\nuvicorn.run(server.app, host=\"0.0.0.0\", port=8000)\n</code></pre> <p>Learn about server deployment, dynamic agents, and server architecture.</p>"},{"location":"agent/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Start Simple - Begin with a basic agent, add skills as you go</li> <li>Use Dependencies - Some skills auto-require others (e.g., payments depends on auth)</li> <li>Scope Appropriately - Use tool scopes (see Skills Overview) for access control</li> <li>Test Thoroughly - Treat skills as units; test hooks and tools independently</li> <li>Monitor Performance - Track usage and latency; payments will use <code>context.usage</code></li> </ol>"},{"location":"agent/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart Guide - Build your first agent in 5 minutes</li> <li>Skills Repository - Explore available skills and create custom ones</li> <li>Agent Lifecycle - Understand the complete request processing flow</li> <li>Server Deployment - Deploy your agents to production</li> <li>Contributing - Contribute to the WebAgents ecosystem</li> </ul>"},{"location":"agent/prompts/","title":"Agent Prompts","text":"<p>Enhance your agent's system prompt dynamically using the <code>@prompt</code> decorator. Prompt functions execute before each LLM call and contribute contextual information to the system message.</p> <p>Prompts run in priority order and support scope-based access control. Use them for dynamic context, user-specific information, or system status updates.</p>"},{"location":"agent/prompts/#overview","title":"Overview","text":"<p>Prompt functions generate dynamic content that gets added to the agent's system message before LLM execution. They're perfect for injecting real-time context, user information, or environmental data.</p> <p>Key Features: - Dynamic system prompt enhancement - Priority-based execution order - Scope-based access control - Context injection support - Automatic string concatenation - Sync and async support</p>"},{"location":"agent/prompts/#basic-usage","title":"Basic Usage","text":""},{"location":"agent/prompts/#simple-prompt","title":"Simple Prompt","text":"<pre><code>from webagents.agents.tools.decorators import prompt\nfrom webagents.agents import BaseAgent\n\n@prompt()\ndef system_status_prompt(context) -&gt; str:\n    \"\"\"Add current system status to the prompt\"\"\"\n    return f\"System Status: Online - All services operational\"\n\nagent = BaseAgent(\n    name=\"assistant\",\n    model=\"openai/gpt-4o\",\n    capabilities=[system_status_prompt]\n)\n</code></pre> <p>Enhanced System Message: <pre><code>You are a helpful AI assistant.\n\nSystem Status: Online - All services operational\n\nYour name is assistant, you are an AI agent in the Internet of Agents. Current time: 2024-01-15T10:30:00\n</code></pre></p>"},{"location":"agent/prompts/#priority-based-execution","title":"Priority-Based Execution","text":"<pre><code>@prompt(priority=5)\ndef time_prompt(context) -&gt; str:\n    \"\"\"Add current timestamp (executes first)\"\"\"\n    from datetime import datetime\n    return f\"Current Time: {datetime.now().isoformat()}\"\n\n@prompt(priority=10)\ndef system_status_prompt(context) -&gt; str:\n    \"\"\"Add system status (executes second)\"\"\"\n    return f\"System Status: {get_system_status()}\"\n\n@prompt(priority=20)\ndef user_context_prompt(context) -&gt; str:\n    \"\"\"Add user context (executes third)\"\"\"\n    user_id = getattr(context, 'user_id', 'anonymous')\n    return f\"Current User: {user_id}\"\n</code></pre> <p>Result: Prompts execute in ascending priority order (5 \u2192 10 \u2192 20).</p>"},{"location":"agent/prompts/#scope-based-access-control","title":"Scope-Based Access Control","text":"<p>Control which users see specific prompt content:</p> <pre><code>@prompt(scope=\"all\")\ndef public_prompt(context) -&gt; str:\n    \"\"\"Available to all users\"\"\"\n    return \"Public system information\"\n\n@prompt(scope=\"owner\")\ndef owner_prompt(context) -&gt; str:\n    \"\"\"Only for agent owners\"\"\"\n    return f\"Owner Dashboard: {get_owner_stats()}\"\n\n@prompt(scope=\"admin\")\ndef admin_prompt(context) -&gt; str:\n    \"\"\"Admin users only\"\"\"\n    return f\"DEBUG MODE: {get_debug_info()}\"\n\n@prompt(scope=[\"premium\", \"enterprise\"])\ndef premium_prompt(context) -&gt; str:\n    \"\"\"Multiple scopes\"\"\"\n    return \"Premium features enabled\"\n</code></pre>"},{"location":"agent/prompts/#context-access","title":"Context Access","text":"<p>Access request context for dynamic content:</p> <pre><code>@prompt(priority=10)\ndef user_context_prompt(context) -&gt; str:\n    \"\"\"Generate user-specific prompt content\"\"\"\n    user_id = getattr(context, 'user_id', 'anonymous')\n    user_data = get_user_data(user_id)\n\n    return f\"\"\"User Context:\n- Name: {user_data['name']}\n- Role: {user_data['role']}\n- Preferences: {user_data['preferences']}\"\"\"\n\n@prompt(priority=20)\nasync def dynamic_data_prompt(context) -&gt; str:\n    \"\"\"Async prompt with external data\"\"\"\n    # Fetch real-time data\n    market_data = await fetch_market_data()\n    weather_data = await fetch_weather()\n\n    return f\"\"\"Real-time Context:\n- Market: {market_data['status']}\n- Weather: {weather_data['condition']}\"\"\"\n</code></pre>"},{"location":"agent/prompts/#skill-integration","title":"Skill Integration","text":"<p>Use prompts within skills for modular functionality:</p> <pre><code>from webagents.agents.skills.base import Skill\n\nclass AnalyticsSkill(Skill):\n    \"\"\"Skill that adds analytics context to prompts\"\"\"\n\n    @prompt(priority=15, scope=\"owner\")\n    def analytics_prompt(self, context) -&gt; str:\n        \"\"\"Add analytics data to system prompt\"\"\"\n        stats = self.get_analytics_data()\n        return f\"\"\"Analytics Summary:\n- Active Users: {stats['active_users']}\n- Revenue Today: ${stats['daily_revenue']}\n- System Load: {stats['cpu_usage']}%\"\"\"\n\n    @prompt(priority=25)\n    def performance_prompt(self, context) -&gt; str:\n        \"\"\"Add performance metrics\"\"\"\n        metrics = self.get_performance_metrics()\n        return f\"Performance: {metrics['response_time']}ms avg\"\n\n    def get_analytics_data(self) -&gt; dict:\n        # Fetch real analytics data\n        return {\"active_users\": 1250, \"daily_revenue\": 5420, \"cpu_usage\": 23}\n\n    def get_performance_metrics(self) -&gt; dict:\n        return {\"response_time\": 150}\n\n# Use in agent\nagent = BaseAgent(\n    name=\"analytics-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\"analytics\": AnalyticsSkill()}\n)\n</code></pre>"},{"location":"agent/prompts/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"agent/prompts/#conditional-prompts","title":"Conditional Prompts","text":"<pre><code>@prompt(priority=10)\ndef conditional_prompt(context) -&gt; str:\n    \"\"\"Add content based on conditions\"\"\"\n    user_role = getattr(context, 'user_role', 'guest')\n\n    if user_role == 'admin':\n        return \"ADMIN MODE: Full system access enabled\"\n    elif user_role == 'premium':\n        return \"PREMIUM MODE: Enhanced features available\"\n    else:\n        return \"STANDARD MODE: Basic features\"\n\n@prompt(priority=15)\ndef time_based_prompt(context) -&gt; str:\n    \"\"\"Different content based on time\"\"\"\n    from datetime import datetime\n    hour = datetime.now().hour\n\n    if 6 &lt;= hour &lt; 12:\n        return \"Good morning! System ready for daily operations.\"\n    elif 12 &lt;= hour &lt; 18:\n        return \"Good afternoon! Peak usage period - optimized for performance.\"\n    else:\n        return \"Good evening! Running in power-save mode.\"\n</code></pre>"},{"location":"agent/prompts/#error-handling","title":"Error Handling","text":"<pre><code>@prompt(priority=5)\ndef safe_prompt(context) -&gt; str:\n    \"\"\"Prompt with error handling\"\"\"\n    try:\n        external_data = fetch_external_service()\n        return f\"External Status: {external_data['status']}\"\n    except Exception as e:\n        # Log error but don't break prompt execution\n        logger.warning(f\"External service unavailable: {e}\")\n        return \"External Status: Offline (using cached data)\"\n\n@prompt(priority=10)\nasync def resilient_async_prompt(context) -&gt; str:\n    \"\"\"Async prompt with timeout handling\"\"\"\n    try:\n        # Set timeout for external calls\n        async with asyncio.timeout(2.0):\n            data = await fetch_slow_service()\n            return f\"Live Data: {data['value']}\"\n    except asyncio.TimeoutError:\n        return \"Live Data: Timeout (using fallback)\"\n    except Exception:\n        return \"Live Data: Service unavailable\"\n</code></pre>"},{"location":"agent/prompts/#best-practices","title":"Best Practices","text":""},{"location":"agent/prompts/#keep-prompts-concise","title":"Keep Prompts Concise","text":"<pre><code># \u2705 Good - concise and focused\n@prompt()\ndef status_prompt(context) -&gt; str:\n    return f\"Status: {get_status()}\"\n\n# \u274c Avoid - too verbose\n@prompt()\ndef verbose_prompt(context) -&gt; str:\n    return f\"\"\"\n    This is a very long prompt that contains way too much information\n    and will consume unnecessary tokens in every LLM call. It includes\n    redundant details and verbose explanations that don't add value.\n    The system status is {get_status()} but this prompt is too long.\n    \"\"\"\n</code></pre>"},{"location":"agent/prompts/#use-appropriate-priorities","title":"Use Appropriate Priorities","text":"<pre><code># \u2705 Good - logical priority order\n@prompt(priority=5)   # Core system info first\ndef system_prompt(context) -&gt; str: ...\n\n@prompt(priority=10)  # User context second  \ndef user_prompt(context) -&gt; str: ...\n\n@prompt(priority=15)  # Specific features last\ndef feature_prompt(context) -&gt; str: ...\n</code></pre>"},{"location":"agent/prompts/#handle-failures-gracefully","title":"Handle Failures Gracefully","text":"<pre><code># \u2705 Good - safe error handling\n@prompt()\ndef safe_prompt(context) -&gt; str:\n    try:\n        return f\"Data: {get_data()}\"\n    except Exception:\n        return \"Data: Unavailable\"\n\n# \u274c Avoid - unhandled exceptions\n@prompt()\ndef unsafe_prompt(context) -&gt; str:\n    return f\"Data: {get_data()}\"  # Could crash prompt execution\n</code></pre>"},{"location":"agent/prompts/#integration-examples","title":"Integration Examples","text":""},{"location":"agent/prompts/#with-authentication","title":"With Authentication","text":"<pre><code>@prompt(priority=10, scope=\"owner\")\ndef auth_context_prompt(context) -&gt; str:\n    \"\"\"Add authenticated user context\"\"\"\n    user = getattr(context, 'authenticated_user', None)\n    if user:\n        return f\"Authenticated as: {user['name']} ({user['email']})\"\n    return \"Authentication: Guest user\"\n</code></pre>"},{"location":"agent/prompts/#with-payment-skills","title":"With Payment Skills","text":"<pre><code>@prompt(priority=15, scope=\"owner\")\ndef billing_context_prompt(context) -&gt; str:\n    \"\"\"Add billing information for owners\"\"\"\n    balance = get_user_balance(context.user_id)\n    usage = get_current_usage(context.user_id)\n\n    return f\"\"\"Billing Status:\n- Balance: ${balance:.2f}\n- Usage Today: {usage} credits\"\"\"\n</code></pre>"},{"location":"agent/prompts/#with-discovery-skills","title":"With Discovery Skills","text":"<pre><code>@prompt(priority=20)\ndef network_status_prompt(context) -&gt; str:\n    \"\"\"Add network connectivity status\"\"\"\n    connected_agents = count_connected_agents()\n    return f\"Network: {connected_agents} agents connected\"\n</code></pre>"},{"location":"agent/prompts/#see-also","title":"See Also","text":"<ul> <li>Tools - Executable functions for agents</li> <li>Hooks - Event-driven processing</li> <li>Skills - Modular agent capabilities</li> <li>Endpoints - HTTP API routes</li> </ul>"},{"location":"agent/skills/","title":"Skills","text":"<p>Skills are modular capability packages that extend a <code>BaseAgent</code> with tools, prompts, hooks, handoffs, and optional HTTP endpoints. They're first-class, composable building blocks that keep business logic organized and reusable across agents.</p> <ul> <li>Tools: executable functions registered via <code>@tool</code></li> <li>Prompts: guidance for the LLM, optionally prioritized or scoped</li> <li>Hooks: lifecycle callbacks (e.g., <code>on_message</code>, <code>before_toolcall</code>)</li> <li>Handoffs: completion handlers (local LLM or remote agents) registered during initialization</li> <li>HTTP endpoints: register custom REST handlers via <code>@http</code></li> <li>Dependencies: declare other skills your skill requires (e.g., memory)</li> </ul>"},{"location":"agent/skills/#add-skills-to-an-agent","title":"Add Skills to an Agent","text":"<p>Consistent with the Quickstart, you attach skills when creating your agent:</p> <pre><code>from webagents.agents.core.base_agent import BaseAgent\nfrom webagents.agents.skills.robutler.nli.skill import NLISkill\nfrom webagents.agents.skills.robutler.auth.skill import AuthSkill\nfrom webagents.agents.skills.robutler.discovery.skill import DiscoverySkill\nfrom webagents.agents.skills.robutler.payments.skill import PaymentSkill\n\nagent = BaseAgent(\n    name=\"assistant\",\n    instructions=\"You are a helpful AI assistant.\",\n    model=\"openai/gpt-4o-mini\",  # Automatically provisions LLM skill\n    skills={\n        \"nli\": NLISkill(),            # Natural language communication with agents\n        \"auth\": AuthSkill(),          # Authentication &amp; scoped access control\n        \"discovery\": DiscoverySkill(),# Real-time agent discovery (intent-based)\n        \"payments\": PaymentSkill()    # Monetization via priced tools\n    }\n)\n</code></pre> <p>This mirrors the examples in the Quickstart and Index pages. After skills are attached, your agent can immediately use their tools, prompts, hooks, HTTP endpoints, and handoffs during requests.</p>"},{"location":"agent/skills/#skill-anatomy-minimal-example","title":"Skill Anatomy (Minimal Example)","text":"<pre><code>from webagents.agents.skills.base import Skill, Handoff\nfrom webagents.agents.tools.decorators import tool, handoff\nfrom typing import Dict, Any, AsyncGenerator\n\nclass MySkill(Skill):\n    def __init__(self, config=None):\n        super().__init__(\n            config=config,\n            scope=\"all\",               # all | owner | admin\n            dependencies=[\"memory\"],   # ensure memory is present if needed\n        )\n\n    async def initialize(self, agent):\n        \"\"\"Called after skill is attached to agent\"\"\"\n        self.agent = agent\n\n        # Register handoff if this skill provides completion handling\n        # See Agent Handoffs documentation for details\n\n    @tool\n    def summarize(self, text: str, max_len: int = 200) -&gt; str:\n        \"\"\"Summarize input text to a target length.\"\"\"\n        return text[:max_len]\n\n    @hook(\"on_message\")\n    async def on_message(self, context):\n        # Inspect/augment request context before tools or model\n        return context\n\n    @handoff(\n        name=\"custom_handler\",\n        prompt=\"Use for specialized processing\",\n        priority=15\n    )\n    async def custom_completion(\n        self,\n        messages: List[Dict[str, Any]],\n        tools: Optional[List[Dict[str, Any]]] = None,\n        **kwargs\n    ) -&gt; AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"Custom completion handler (streaming)\"\"\"\n        # Process and yield chunks\n        yield {\"choices\": [{\"delta\": {\"content\": \"Processing...\"}}]}\n</code></pre> <ul> <li>Register execution logic with <code>@tool</code></li> <li>Guide LLM behavior with prompts (see Skills Framework for full patterns)</li> <li>React to request lifecycle via <code>@hook</code></li> <li>Provide completion handlers with <code>@handoff</code> (for LLM or remote agent routing)</li> </ul>"},{"location":"agent/skills/#http-endpoints-in-skills","title":"HTTP Endpoints in Skills","text":"<p>Register custom REST endpoints with the <code>@http</code> decorator. These are mounted under your agent\u2019s base path when served.</p> <pre><code>from webagents.agents.tools.decorators import http\n\n@http(\"/weather\", method=\"get\", scope=\"owner\")\ndef get_weather(location: str, units: str = \"celsius\") -&gt; dict:\n    return {\"location\": location, \"temperature\": 25, \"units\": units}\n\n@http(\"/data\", method=\"post\")\nasync def post_data(payload: dict) -&gt; dict:\n    return {\"received\": payload, \"status\": \"processed\"}\n</code></pre> <ul> <li><code>path</code>: endpoint path relative to the agent root (e.g., <code>/assistant/weather</code>)</li> <li><code>method</code>: one of <code>get</code>, <code>post</code>, etc. (default is <code>post</code> if omitted)</li> <li><code>scope</code>: optional access control (<code>all</code>, <code>owner</code>, <code>admin</code>)</li> </ul> <p>When the agent is served, these endpoints are available immediately.</p>"},{"location":"agent/skills/#using-skill-tools-in-a-request","title":"Using Skill Tools in a Request","text":"<p>Tools you register are available to the agent at runtime. You can also pass external tools per request (OpenAI function-calling compatible):</p> <pre><code>response = await agent.run([\n    {\"role\": \"user\", \"content\": \"Summarize: ...\"}\n])\n\n# Or include additional, ad-hoc tools for a single call\nresponse = await agent.run(\n    messages=[{\"role\": \"user\", \"content\": \"Calculate 42 * 17\"}],\n    tools=[{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"calculator\",\n            \"description\": \"Calculate math expressions\",\n            \"parameters\": {\"type\": \"object\", \"properties\": {\"expr\": {\"type\": \"string\"}}}\n        }\n    }]\n)\n</code></pre>"},{"location":"agent/skills/#serving-an-agent-with-skills","title":"Serving an Agent with Skills","text":"<p>Follow the Quickstart approach to serve your agent over HTTP:</p> <pre><code>from webagents.server.core.app import create_server\nimport uvicorn\n\nserver = create_server(agents=[agent])\nuvicorn.run(server.app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"agent/tools/","title":"Agent Tools","text":"<p>Tools extend agent capabilities with executable functions. There are two types: internal tools and external tools. Internal tools are Python functions the agent can call directly; external tools follow OpenAI's tool-calling protocol and are executed by the client.</p>"},{"location":"agent/tools/#tool-types","title":"Tool Types","text":""},{"location":"agent/tools/#internal-tools","title":"Internal Tools","text":"<p>Internal tools are executed within the agent's process. They can be:</p> <ol> <li>Skill Tools - Defined in skills using <code>@tool</code> decorator</li> <li>Standalone Tools - Decorated functions passed to agent</li> </ol>"},{"location":"agent/tools/#external-tools","title":"External Tools","text":"<p>External tools are defined in the request and executed on the client side. The agent will emit OpenAI tool calls; your client is responsible for executing them and returning results in a follow-up message. This keeps server responsibilities minimal while remaining compatible with OpenAI tooling.</p> <p>HTTP Endpoints</p> <p>For creating custom HTTP API endpoints, see Agent Endpoints which covers the <code>@http</code> decorator and REST API creation.</p>"},{"location":"agent/tools/#internal-tools_1","title":"Internal Tools","text":""},{"location":"agent/tools/#standalone-tools","title":"Standalone Tools","text":"<pre><code>from webagents.agents.tools.decorators import tool\nfrom webagents.agents import BaseAgent\n\n# Define standalone tool functions\n@tool\ndef calculate(expression: str) -&gt; str:\n    \"\"\"Calculate mathematical expressions\"\"\"\n    try:\n        result = eval(expression, {\"__builtins__\": {}}, {})\n        return str(result)\n    except:\n        return \"Invalid expression\"\n\n@tool(scope=\"owner\")\ndef admin_function(action: str) -&gt; str:\n    \"\"\"Owner-only administrative function\"\"\"\n    return f\"Admin action: {action}\"\n\n# Pass to agent\nagent = BaseAgent(\n    name=\"my-agent\",\n    model=\"openai/gpt-4o\",\n    tools=[calculate, admin_function]  # Internal tools\n)\n</code></pre>"},{"location":"agent/tools/#capabilities-auto-registration","title":"Capabilities Auto-Registration","text":"<p>Use the <code>capabilities</code> parameter to automatically register decorated functions:</p> <pre><code>from webagents.agents.tools.decorators import tool, hook, handoff\n\n@tool(scope=\"owner\")\ndef my_tool(message: str) -&gt; str:\n    return f\"Tool: {message}\"\n\n@tool\ndef another_tool(data: str) -&gt; str:\n    return f\"Processed: {data}\"\n\n@hook(\"on_request\", priority=10)\ndef my_hook(context):\n    return context\n\n# Auto-register all decorated functions\nagent = BaseAgent(\n    name=\"capable-agent\",\n    model=\"openai/gpt-4o\",\n    capabilities=[my_tool, another_tool, my_hook]\n)\n</code></pre> <p>The agent will automatically categorize and register each function based on its decorator type. Tools will be registered as callable functions for the LLM.</p> <p>Handoff System</p> <p>Handoffs are handled via skill registration during initialization. Instead of using <code>@handoff</code> decorator on standalone functions, handoffs are registered within skills. See Agent Handoffs for the handoff system.</p>"},{"location":"agent/tools/#skill-tools","title":"Skill Tools","text":"<pre><code>from webagents.agents.skills import Skill\nfrom webagents.agents.tools.decorators import tool\n\nclass CalculatorSkill(Skill):\n    @tool\n    def add(self, a: float, b: float) -&gt; float:\n        \"\"\"Add two numbers\"\"\"\n        return a + b\n\n    @tool(scope=\"owner\")\n    def multiply(self, x: float, y: float) -&gt; float:\n        \"\"\"Multiply two numbers (owner only)\"\"\"\n        return x * y\n</code></pre>"},{"location":"agent/tools/#tool-parameters","title":"Tool Parameters","text":"<pre><code>@tool(\n    name=\"custom_name\",      # Override function name\n    description=\"Custom\",    # Override docstring\n    scope=\"all\",            # Access control: all/owner/admin\n    # For priced tools, prefer the PaymentSkill's @pricing decorator; this field is descriptive only\n)\ndef my_tool(param: str) -&gt; str:\n    \"\"\"Tool implementation\"\"\"\n    return f\"Result: {param}\"\n</code></pre>"},{"location":"agent/tools/#openai-schema-generation","title":"OpenAI Schema Generation","text":"<p>Tools automatically generate OpenAI-compatible schemas:</p> <pre><code>@tool\ndef search_web(query: str, max_results: int = 10) -&gt; List[str]:\n    \"\"\"Search the web for information\n\n    Args:\n        query: Search query string\n        max_results: Maximum results to return\n\n    Returns:\n        List of search results\n    \"\"\"\n    return [\"result1\", \"result2\"]\n\n# Generates schema:\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"search_web\",\n        \"description\": \"Search the web for information\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": \"Search query string\"\n                },\n                \"max_results\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Maximum results to return\",\n                    \"default\": 10\n                }\n            },\n            \"required\": [\"query\"]\n        }\n    }\n}\n</code></pre>"},{"location":"agent/tools/#external-tools_1","title":"External Tools","text":"<p>External tools are defined in the request's <code>tools</code> parameter and executed on the requester's side. They follow the standard OpenAI tool definition format.</p>"},{"location":"agent/tools/#standard-openai-tool-definition-format","title":"Standard OpenAI Tool Definition Format","text":"<p>External tools use the standard OpenAI format:</p> <pre><code>{\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"function_name\",\n        \"description\": \"Function description\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"param_name\": {\n              \"type\": \"string\",\n              \"description\": \"Parameter description\"\n            }\n          },\n          \"required\": [\"param_name\"]\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"agent/tools/#using-external-tools","title":"Using External Tools","text":"<pre><code># Define external tools in the request\nexternal_tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get current weather for a location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"description\": \"Temperature unit (celsius or fahrenheit)\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"]\n                    }\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"send_email\",\n            \"description\": \"Send an email to a recipient\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"to\": {\"type\": \"string\", \"description\": \"Recipient email address\"},\n                    \"subject\": {\"type\": \"string\", \"description\": \"Email subject\"},\n                    \"body\": {\"type\": \"string\", \"description\": \"Email body content\"}\n                },\n                \"required\": [\"to\", \"subject\", \"body\"]\n            }\n        }\n    }\n]\n\n# Pass tools in the request\nmessages = [{\"role\": \"user\", \"content\": \"What's the weather in Paris?\"}]\nresponse = await agent.run(messages=messages, tools=external_tools)\n</code></pre>"},{"location":"agent/tools/#handling-tool-calls","title":"Handling Tool Calls","text":"<p>When the agent makes tool calls, you receive them in the response and execute them client-side:</p> <pre><code># Agent response with tool calls\nresponse = await agent.run(messages=messages, tools=external_tools)\nassistant_message = response.choices[0].message\n\nif assistant_message.tool_calls:\n    # Execute each tool call\n    for tool_call in assistant_message.tool_calls:\n        function_name = tool_call.function.name\n        arguments = json.loads(tool_call.function.arguments)\n\n        # Execute the tool based on name\n        if function_name == \"get_weather\":\n            result = get_weather_external(arguments[\"location\"])\n        elif function_name == \"send_email\":\n            result = send_email_external(\n                arguments[\"to\"], \n                arguments[\"subject\"], \n                arguments[\"body\"]\n            )\n\n        # Add tool result to conversation\n        messages.append({\n            \"role\": \"assistant\",\n            \"content\": assistant_message.content,\n            \"tool_calls\": [tool_call]\n        })\n        messages.append({\n            \"role\": \"tool\",\n            \"tool_call_id\": tool_call.id,\n            \"content\": result\n        })\n\n        # Get final response\n        final_response = await agent.run(messages=messages, tools=external_tools)\n        return final_response.choices[0].message.content\n\ndef get_weather_external(location: str) -&gt; str:\n    \"\"\"Your implementation of the external weather tool\"\"\"\n    # Your weather API call here\n    return f\"Sunny in {location}, 22\u00b0C\"\n\ndef send_email_external(to: str, subject: str, body: str) -&gt; str:\n    \"\"\"Your implementation of the external email tool\"\"\"\n    # Your email sending logic here\n    return f\"Email sent to {to}\"\n</code></pre>"},{"location":"agent/tools/#tool-execution","title":"Tool Execution","text":""},{"location":"agent/tools/#automatic-tool-calling","title":"Automatic Tool Calling","text":"<pre><code># Agent automatically calls tools when needed\nuser_msg = \"What's the weather in Paris?\"\nresponse = await agent.run([\n    {\"role\": \"user\", \"content\": user_msg}\n])\n# Agent calls get_weather(\"Paris\") automatically\n</code></pre>"},{"location":"agent/tools/#manual-tool-results","title":"Manual Tool Results","text":"<pre><code># Include tool results in conversation\nmessages = [\n    {\"role\": \"user\", \"content\": \"Calculate 42 * 17\"},\n    {\"role\": \"assistant\", \"content\": \"I'll calculate that for you.\", \n     \"tool_calls\": [{\n         \"id\": \"call_123\",\n         \"type\": \"function\",\n         \"function\": {\"name\": \"multiply\", \"arguments\": '{\"x\": 42, \"y\": 17}'}\n     }]},\n    {\"role\": \"tool\", \"tool_call_id\": \"call_123\", \"content\": \"714\"}\n]\nresponse = await agent.run(messages)\n</code></pre>"},{"location":"agent/tools/#advanced-tool-features","title":"Advanced Tool Features","text":""},{"location":"agent/tools/#dynamic-tool-registration","title":"Dynamic Tool Registration","text":"<pre><code>class AdaptiveSkill(Skill):\n    @hook(\"on_connection\")\n    async def register_dynamic_tools(self, context):\n        \"\"\"Register tools based on context\"\"\"\n\n        if context.peer_user_id == \"admin\":\n            # Register admin tools\n            self.register_tool(self.admin_tool, scope=\"admin\")\n\n        if \"math\" in str(context.messages):\n            # Register math tools\n            self.register_tool(self.advanced_calc)\n\n        return context\n\n    def admin_tool(self, action: str) -&gt; str:\n        \"\"\"Admin-only tool\"\"\"\n        return f\"Admin action: {action}\"\n</code></pre>"},{"location":"agent/tools/#tool-middleware","title":"Tool Middleware","text":"<pre><code>class ToolMonitor(Skill):\n    @hook(\"before_toolcall\", priority=1)\n    async def validate_tool(self, context):\n        \"\"\"Validate before execution\"\"\"\n        tool_name = context[\"tool_call\"][\"function\"][\"name\"]\n\n        # Rate limiting\n        if self.is_rate_limited(tool_name):\n            raise RateLimitError(f\"Tool {tool_name} rate limited\")\n\n        # Parameter validation\n        args = json.loads(context[\"tool_call\"][\"function\"][\"arguments\"])\n        self.validate_args(tool_name, args)\n\n        return context\n\n    @hook(\"after_toolcall\", priority=90)\n    async def log_result(self, context):\n        \"\"\"Log tool execution\"\"\"\n        await self.log_tool_usage(\n            tool=context[\"tool_call\"][\"function\"][\"name\"],\n            result=context[\"tool_result\"],\n            duration=context.get(\"tool_duration\")\n        )\n        return context\n</code></pre>"},{"location":"agent/tools/#tool-pricing","title":"Tool Pricing","text":"<pre><code>from webagents.agents.tools.decorators import tool, pricing\n\nclass PaidToolsSkill(Skill):\n    @tool\n    @pricing(cost=0.10, currency=\"USD\")\n    def expensive_api_call(self, query: str) -&gt; str:\n        \"\"\"Call expensive external API\"\"\"\n        # Automatically tracks usage for billing\n        return self.call_paid_api(query)\n\n    @tool\n    @pricing(cost=0.01, per=\"request\")\n    def database_query(self, sql: str) -&gt; List[Dict]:\n        \"\"\"Execute database query\"\"\"\n        return self.execute_sql(sql)\n</code></pre>"},{"location":"agent/tools/#tool-patterns","title":"Tool Patterns","text":""},{"location":"agent/tools/#validation-pattern","title":"Validation Pattern","text":"<pre><code>@tool\ndef update_record(self, record_id: str, data: Dict) -&gt; Dict:\n    \"\"\"Update record with validation\"\"\"\n    # Validate inputs\n    if not self.validate_record_id(record_id):\n        return {\"error\": \"Invalid record ID\"}\n\n    if not self.validate_data(data):\n        return {\"error\": \"Invalid data format\"}\n\n    # Perform update\n    try:\n        result = self.db.update(record_id, data)\n        return {\"success\": True, \"record\": result}\n    except Exception as e:\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"agent/tools/#async-pattern","title":"Async Pattern","text":"<pre><code>@tool\nasync def fetch_data(self, urls: List[str]) -&gt; List[Dict]:\n    \"\"\"Fetch data from multiple URLs concurrently\"\"\"\n    import aiohttp\n\n    async with aiohttp.ClientSession() as session:\n        tasks = [self.fetch_url(session, url) for url in urls]\n        results = await asyncio.gather(*tasks)\n\n    return results\n</code></pre>"},{"location":"agent/tools/#caching-pattern","title":"Caching Pattern","text":"<pre><code>class CachedToolsSkill(Skill):\n    def __init__(self, config=None):\n        super().__init__(config)\n        self.cache = {}\n\n    @tool\n    def expensive_calculation(self, input: str) -&gt; str:\n        \"\"\"Cached expensive calculation\"\"\"\n        if input in self.cache:\n            return self.cache[input]\n\n        result = self.perform_calculation(input)\n        self.cache[input] = result\n        return result\n</code></pre>"},{"location":"agent/tools/#best-practices","title":"Best Practices","text":"<ol> <li>Clear Descriptions - Help LLM understand when to use tools</li> <li>Type Hints - Enable automatic schema generation</li> <li>Error Handling - Return errors as data, not exceptions</li> <li>Scope Control - Use appropriate access levels</li> <li>Performance - Consider caching and async execution </li> </ol>"},{"location":"blog/","title":"Blog","text":""},{"location":"developers/contributing/","title":"Contributing to WebAgents","text":"<p>Thank you for your interest in contributing to WebAgents! This guide will help you get started with contributing to the project.</p>"},{"location":"developers/contributing/#getting-started","title":"Getting Started","text":""},{"location":"developers/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li>Git</li> <li>A GitHub account</li> </ul>"},{"location":"developers/contributing/#development-environment-setup","title":"Development Environment Setup","text":"<ol> <li> <p>Fork the repository <pre><code># Fork the repository on GitHub, then clone your fork\ngit clone https://github.com/YOUR_USERNAME/webagents.git\ncd webagents\n</code></pre></p> </li> <li> <p>Set up the development environment <pre><code># Create a virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install development dependencies\npip install -e \".[dev]\"\n</code></pre></p> </li> <li> <p>Set up environment variables <pre><code># Create .env file\ncp .env.example .env\n\n# Add your API keys\nOPENAI_API_KEY=your-openai-api-key\nWEBAGENTS_API_KEY=your-robutler-api-key\n</code></pre></p> </li> <li> <p>Verify the setup <pre><code># Run tests to ensure everything is working\npytest\n\n# Run linting\nflake8 webagents/\nblack --check webagents/\n</code></pre></p> </li> </ol>"},{"location":"developers/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"developers/contributing/#1-create-a-branch","title":"1. Create a Branch","text":"<p>Always create a new branch for your work:</p> <pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/issue-description\n</code></pre>"},{"location":"developers/contributing/#2-make-your-changes","title":"2. Make Your Changes","text":"<ul> <li>Write clean, readable code</li> <li>Follow the existing code style</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> </ul>"},{"location":"developers/contributing/#3-test-your-changes","title":"3. Test Your Changes","text":"<pre><code># Run the full test suite\npytest\n\n# Run tests with coverage\npytest --cov=webagents\n\n# Run specific tests\npytest tests/test_agent.py\n\n# Run linting\nflake8 webagents/\nblack --check webagents/\n</code></pre>"},{"location":"developers/contributing/#4-commit-your-changes","title":"4. Commit Your Changes","text":"<p>We use conventional commits for clear commit messages:</p> <pre><code>git add .\ngit commit -m \"feat: add new agent configuration option\"\n# or\ngit commit -m \"fix: resolve payment processing error\"\n# or\ngit commit -m \"docs: update API documentation\"\n</code></pre> <p>Commit types: - <code>feat</code>: New features - <code>fix</code>: Bug fixes - <code>docs</code>: Documentation changes - <code>style</code>: Code style changes (formatting, etc.) - <code>refactor</code>: Code refactoring - <code>test</code>: Adding or updating tests - <code>chore</code>: Maintenance tasks</p>"},{"location":"developers/contributing/#5-push-and-create-a-pull-request","title":"5. Push and Create a Pull Request","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub with: - Clear title and description - Reference to any related issues - Screenshots or examples if applicable</p>"},{"location":"developers/contributing/#code-style-guidelines","title":"Code Style Guidelines","text":""},{"location":"developers/contributing/#python-code-style","title":"Python Code Style","text":"<p>We follow PEP 8 with some modifications:</p> <ul> <li>Line length: 88 characters (Black default)</li> <li>Use type hints for all public functions</li> <li>Use docstrings for all public classes and functions</li> <li>Prefer f-strings for string formatting</li> </ul> <p>Example:</p> <pre><code>from typing import Optional, List, Dict, Any\n\nclass ExampleClass:\n    \"\"\"Example class demonstrating code style.\n\n    This class shows the preferred code style for WebAgents\n    including type hints, docstrings, and formatting.\n    \"\"\"\n\n    def __init__(self, name: str, config: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"Initialize the example class.\n\n        Args:\n            name: The name of the instance\n            config: Optional configuration dictionary\n        \"\"\"\n        self.name = name\n        self.config = config or {}\n\n    def process_items(self, items: List[str]) -&gt; Dict[str, int]:\n        \"\"\"Process a list of items and return counts.\n\n        Args:\n            items: List of items to process\n\n        Returns:\n            Dictionary mapping items to their counts\n\n        Raises:\n            ValueError: If items list is empty\n        \"\"\"\n        if not items:\n            raise ValueError(\"Items list cannot be empty\")\n\n        return {item: items.count(item) for item in set(items)}\n</code></pre>"},{"location":"developers/contributing/#documentation-style","title":"Documentation Style","text":"<ul> <li>Use Google-style docstrings</li> <li>Include type information in docstrings</li> <li>Provide examples for complex functions</li> <li>Keep documentation up to date with code changes</li> </ul>"},{"location":"developers/contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"developers/contributing/#writing-tests","title":"Writing Tests","text":"<ul> <li>Write tests for all new functionality</li> <li>Use descriptive test names</li> <li>Follow the Arrange-Act-Assert pattern</li> <li>Use fixtures for common test data</li> </ul> <p>Example test:</p> <pre><code>import pytest\nfrom webagents.agent import BaseAgent\n\nclass TestBaseAgent:\n    \"\"\"Test cases for BaseAgent class.\"\"\"\n\n    def test_agent_creation_with_valid_config(self):\n        \"\"\"Test that agent can be created with valid configuration.\"\"\"\n        # Arrange\n        name = \"test-agent\"\n        instructions = \"You are a helpful assistant.\"\n\n        # Act\n        agent = BaseAgent(\n            name=name,\n            instructions=instructions,\n            credits_per_token=10\n        )\n\n        # Assert\n        assert agent.name == name\n        assert agent.instructions == instructions\n        assert agent.credits_per_token == 10\n\n    def test_agent_with_tools(self):\n        \"\"\"Test that agent can be created with tools.\"\"\"\n        from agents import function_tool\n\n        @function_tool\n        def test_tool() -&gt; str:\n            return \"test result\"\n\n        agent = BaseAgent(\n            name=\"tool-agent\",\n            instructions=\"You have tools.\",\n            tools=[test_tool]\n        )\n\n        assert len(agent.tools) == 1\n</code></pre>"},{"location":"developers/contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run specific test file\npytest tests/test_agent.py\n\n# Run with coverage\npytest --cov=webagents\n\n# Run tests matching a pattern\npytest -k \"test_agent\"\n\n# Run tests with verbose output\npytest -v\n</code></pre>"},{"location":"developers/contributing/#contributing-areas","title":"Contributing Areas","text":""},{"location":"developers/contributing/#areas-where-we-need-help","title":"Areas Where We Need Help","text":"<ol> <li>Agent Tools: New tools that extend agent capabilities</li> <li>Documentation: Improving guides and API documentation</li> <li>Testing: Adding test coverage for existing functionality</li> <li>Bug Fixes: Resolving reported issues</li> <li>Performance: Optimizing agent response times</li> <li>Examples: Creating example applications and use cases</li> </ol>"},{"location":"developers/contributing/#feature-requests","title":"Feature Requests","text":"<p>Before implementing new features:</p> <ol> <li>Check existing issues: See if the feature is already requested</li> <li>Create an issue: Discuss the feature with maintainers first</li> <li>Get approval: Wait for maintainer approval before starting work</li> <li>Follow guidelines: Use this contributing guide for implementation</li> </ol>"},{"location":"developers/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Issues: Check GitHub Issues for existing problems</li> <li>Discussions: Use GitHub Discussions for questions</li> <li>Discord: Join our community Discord for real-time help</li> </ul>"},{"location":"developers/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please note that this project is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms.</p> <p>Thank you for contributing to WebAgents! </p>"},{"location":"developers/development/","title":"Development Setup","text":"<p>This guide covers setting up a development environment for working on Robutler.</p>"},{"location":"developers/development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python: 3.8 or higher</li> <li>Git: Latest version</li> <li>OpenAI API Key: For agent functionality</li> </ul>"},{"location":"developers/development/#environment-setup","title":"Environment Setup","text":""},{"location":"developers/development/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code># Clone the repository\ngit clone https://github.com/robutlerai/robutler.git\ncd robutler-proxy\n\n# Or clone your fork\ngit clone https://github.com/YOUR_USERNAME/robutler.git\ncd robutler-proxy\n</code></pre>"},{"location":"developers/development/#2-python-environment","title":"2. Python Environment","text":""},{"location":"developers/development/#using-venv-recommended","title":"Using venv (Recommended)","text":"<pre><code># Create virtual environment\npython -m venv venv\n\n# Activate virtual environment\n# On Linux/Mac:\nsource venv/bin/activate\n# On Windows:\nvenv\\Scripts\\activate\n\n# Upgrade pip\npip install --upgrade pip\n</code></pre>"},{"location":"developers/development/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code># Install development dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"developers/development/#4-environment-variables","title":"4. Environment Variables","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code># Required for agent functionality\nOPENAI_API_KEY=your-openai-api-key\n\n# Optional Robutler API configuration\nWEBAGENTS_API_KEY=rok_your-robutler-api-key\nROBUTLER_API_URL=https://robutler.ai\n\n# Development settings\nROBUTLER_DEBUG=true\n</code></pre>"},{"location":"developers/development/#development-tools","title":"Development Tools","text":""},{"location":"developers/development/#code-formatting-and-linting","title":"Code Formatting and Linting","text":""},{"location":"developers/development/#black-code-formatting","title":"Black (Code Formatting)","text":"<pre><code># Format all Python files\nblack .\n\n# Check formatting without making changes\nblack --check .\n</code></pre>"},{"location":"developers/development/#isort-import-sorting","title":"isort (Import Sorting)","text":"<pre><code># Sort imports\nisort .\n\n# Check import sorting\nisort --check-only .\n</code></pre>"},{"location":"developers/development/#flake8-linting","title":"flake8 (Linting)","text":"<pre><code># Run linting\nflake8 robutler/\n</code></pre>"},{"location":"developers/development/#testing","title":"Testing","text":""},{"location":"developers/development/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=robutler\n\n# Run specific test file\npytest tests/test_agent.py\n\n# Run tests with verbose output\npytest -v\n</code></pre>"},{"location":"developers/development/#documentation","title":"Documentation","text":""},{"location":"developers/development/#building-documentation","title":"Building Documentation","text":"<pre><code># Serve documentation locally\ncd docs\nmkdocs serve\n\n# Build documentation\nmkdocs build\n</code></pre>"},{"location":"developers/development/#ide-configuration","title":"IDE Configuration","text":""},{"location":"developers/development/#vs-code","title":"VS Code","text":"<p>Recommended extensions: - Python - Black Formatter - isort - Flake8</p> <p>VS Code settings (<code>.vscode/settings.json</code>):</p> <pre><code>{\n  \"python.defaultInterpreterPath\": \"./venv/bin/python\",\n  \"python.formatting.provider\": \"black\",\n  \"python.linting.enabled\": true,\n  \"python.linting.flake8Enabled\": true,\n  \"python.testing.pytestEnabled\": true,\n  \"python.testing.pytestArgs\": [\"tests\"],\n  \"editor.formatOnSave\": true,\n  \"editor.codeActionsOnSave\": {\n    \"source.organizeImports\": true\n  }\n}\n</code></pre>"},{"location":"developers/development/#running-the-development-server","title":"Running the Development Server","text":""},{"location":"developers/development/#basic-agent-server","title":"Basic Agent Server","text":"<pre><code># Create a simple test agent\nfrom webagents.agent import RobutlerAgent\nfrom webagents.server import RobutlerServer\n\nagent = RobutlerAgent(\n    name=\"test-agent\",\n    instructions=\"You are a helpful test assistant.\",\n    credits_per_token=5\n)\n\napp = RobutlerServer(agents=[agent])\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n</code></pre>"},{"location":"developers/development/#common-development-tasks","title":"Common Development Tasks","text":""},{"location":"developers/development/#adding-a-new-tool","title":"Adding a New Tool","text":"<ol> <li>Create the tool function:</li> </ol> <pre><code>from agents import function_tool\nfrom webagents.server import pricing\n\n@function_tool\n@pricing(credits_per_call=1000)\ndef my_new_tool(input_text: str) -&gt; str:\n    \"\"\"Description of what the tool does.\"\"\"\n    # Implementation here\n    return f\"Processed: {input_text}\"\n</code></pre> <ol> <li>Add to agent:</li> </ol> <pre><code>agent = RobutlerAgent(\n    name=\"test-agent\",\n    instructions=\"You have access to custom tools.\",\n    tools=[my_new_tool],\n    credits_per_token=5\n)\n</code></pre> <ol> <li>Test the tool:</li> </ol> <pre><code># Test in development\nmessages = [{\"role\": \"user\", \"content\": \"Use the new tool\"}]\nresponse = await agent.run(messages=messages)\nprint(response)\n</code></pre>"},{"location":"developers/development/#adding-new-api-endpoints","title":"Adding New API Endpoints","text":"<pre><code>from webagents.server import RobutlerServer\n\napp = RobutlerServer()\n\n@app.agent(\"/custom-endpoint\")\n@app.pricing(credits_per_token=10)\nasync def custom_agent(request):\n    \"\"\"Custom agent endpoint.\"\"\"\n    messages = request.messages\n    # Process messages\n    return \"Custom response\"\n</code></pre>"},{"location":"developers/development/#testing-changes","title":"Testing Changes","text":"<pre><code># Run tests for specific modules\npytest tests/test_agent.py -v\n\n# Run integration tests\npytest tests/test_integration.py\n\n# Check code formatting\nblack --check .\nisort --check-only .\nflake8 robutler/\n</code></pre>"},{"location":"developers/development/#debugging","title":"Debugging","text":""},{"location":"developers/development/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Or set environment variable\nexport ROBUTLER_DEBUG=true\n</code></pre>"},{"location":"developers/development/#common-debug-tasks","title":"Common Debug Tasks","text":"<pre><code># Test agent endpoint\ncurl -X POST http://localhost:8000/test-agent/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"test-agent\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}'\n\n# Check available tools\ncurl http://localhost:8000/test-agent\n</code></pre> <p>This covers the essential development setup needed to contribute to Robutler. </p>"},{"location":"skills/custom/","title":"Creating Custom Skills","text":"<p>This guide shows how to build a minimal, production-ready skill that is consistent with the SDK, Quickstart, and platform conventions.</p>"},{"location":"skills/custom/#what-a-skill-provides","title":"What a Skill Provides","text":"<ul> <li><code>@tool</code> functions: executable capabilities</li> <li><code>@prompt</code> producers: guide LLM behavior</li> <li><code>@hook</code> handlers: react to lifecycle events (e.g., <code>on_message</code>)</li> <li><code>@handoff</code> rules: route to other agents when needed</li> <li>Optional <code>@http</code> endpoints: custom REST handlers mounted under the agent</li> <li>Declared dependencies: ensure other skills are present (e.g., memory)</li> </ul>"},{"location":"skills/custom/#minimal-skill","title":"Minimal Skill","text":"<pre><code>from webagents.agents.skills.base import Skill\nfrom webagents.agents.tools.decorators import tool\nfrom webagents.agents.skills.decorators import hook, handoff\n\nclass NotesSkill(Skill):\n    def __init__(self, config=None):\n        super().__init__(\n            config=config,\n            scope=\"all\",              # all | owner | admin\n            dependencies=[\"memory\"],  # requires memory for storage\n        )\n\n    @tool\n    def add_note(self, text: str) -&gt; dict:\n        \"\"\"Add a note to the user\u2019s short-term memory.\"\"\"\n        # In a real implementation, call memory skill here\n        return {\"status\": \"saved\", \"text\": text}\n\n    @hook(\"on_message\")\n    async def normalize_message(self, context):\n        # Lightweight preprocessing for downstream tools or model\n        return context\n\n    @handoff(\"notes-auditor\")\n    def route_to_auditor(self, text: str) -&gt; bool:\n        return \"audit\" in text.lower()\n</code></pre>"},{"location":"skills/custom/#adding-http-endpoints-optional","title":"Adding HTTP Endpoints (Optional)","text":"<pre><code>from webagents.agents.tools.decorators import http\n\n@http(\"/notes\", method=\"post\", scope=\"owner\")\nasync def create_note(payload: dict) -&gt; dict:\n    return {\"received\": payload, \"status\": \"ok\"}\n</code></pre> <ul> <li>Endpoints are mounted under your agent path when served</li> <li><code>scope</code> can restrict access to <code>owner</code> or <code>admin</code></li> </ul>"},{"location":"skills/custom/#use-your-skill-in-an-agent","title":"Use Your Skill in an Agent","text":"<pre><code>from webagents.agents.core.base_agent import BaseAgent\nfrom webagents.agents.skills.core.memory import ShortTermMemorySkill\n\nagent = BaseAgent(\n    name=\"notes\",\n    instructions=\"You help users capture and recall short notes.\",\n    model=\"openai/gpt-4o-mini\",\n    skills={\n        \"memory\": ShortTermMemorySkill(),\n        \"notes\": NotesSkill(),\n    }\n)\n</code></pre>"},{"location":"skills/custom/#serve-your-agent","title":"Serve Your Agent","text":"<pre><code>from webagents.server.core.app import create_server\nimport uvicorn\n\nserver = create_server(agents=[agent])\nuvicorn.run(server.app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"skills/custom/#best-practices","title":"Best Practices","text":"<ul> <li>Keep one clear responsibility per skill</li> <li>Validate inputs in tools and HTTP handlers</li> <li>Use <code>scope</code> appropriately (<code>all</code>, <code>owner</code>, <code>admin</code>)</li> <li>Prefer async for I/O and external API calls</li> <li>Leverage dependencies for cross-skill collaboration</li> </ul>"},{"location":"skills/custom/#learn-more","title":"Learn More","text":"<ul> <li>Skills Framework: skills/overview.md</li> <li>Platform Skills: platform/auth.md, platform/discovery.md, platform/nli.md, platform/payments.md</li> <li>Agent Overview: agent/overview.md</li> <li>Quickstart: quickstart.md</li> </ul>"},{"location":"skills/overview/","title":"Skills Repository","text":"<p>Welcome to the Skills Repository - a comprehensive collection of pre-built capabilities that extend your agents' functionality.</p> <ul> <li> <p>\ud83d\udd27 Core Skills</p> <p>Essential building blocks that enable you to build and serve your agent to the internet with no dependencies. </p> <p>Browse Core Skills \u2192</p> </li> <li> <p>\ud83c\udfd7\ufe0f Platform Skills</p> <p>Platform-specific skills that make it simple to add real-time discovery, trust, and monetization features.</p> <p>Browse Platform Skills \u2192</p> </li> <li> <p>\ud83c\udf10 Ecosystem Skills</p> <p>A growing collection of third-party integrations and community-contributed skills including X.com, n8n, Zapier, and more.</p> <p>Browse Ecosystem Skills \u2192</p> </li> <li> <p>\u2728 Your Custom Skills</p> <p>Build and use your own skills tailored to your specific needs. Create custom capabilities for unique use cases.</p> <p>Build Custom Skills \u2192</p> </li> </ul>"},{"location":"skills/core/llm/","title":"LLM Skills","text":"<p>Provides large language model (LLM) capabilities to agents.</p> <p>Robutler supports multiple providers through dedicated skills (e.g., OpenAI, Anthropic) and via LiteLLM proxying. In most cases you can specify <code>model=\"openai/gpt-4o\"</code> and the correct provider skill is created for you.</p>"},{"location":"skills/core/llm/#features","title":"Features","text":"<ul> <li>Text generation, completion, and chat using supported LLM backends</li> <li>Integration with agent tool and skill system</li> </ul>"},{"location":"skills/core/llm/#example-add-llm-skill-to-an-agent","title":"Example: Add LLM Skill to an Agent","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.core.llm.openai.skill import OpenAISkill\n\nagent = BaseAgent(\n    name=\"llm-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"llm\": OpenAISkill({\"model\": \"gpt-4o-mini\"})\n    }\n)\n</code></pre>"},{"location":"skills/core/llm/#example-use-llm-tool-in-a-skill","title":"Example: Use LLM Tool in a Skill","text":"<pre><code>from webagents.agents.skills import Skill, tool\n\nclass SummarizeSkill(Skill):\n    def __init__(self):\n        super().__init__()\n        self.llm = self.agent.skills[\"llm\"]\n\n    @tool\n    async def summarize(self, text: str) -&gt; str:\n        \"\"\"Summarize a block of text using the LLM\"\"\"\n        return await self.llm.generate(prompt=f\"Summarize: {text}\")\n</code></pre> <p>Implementation: provider-specific skills, e.g., <code>robutler/agents/skills/core/llm/openai/skill.py</code>.</p>"},{"location":"skills/core/mcp/","title":"MCP Skill","text":"<p>Integrates with the Multi-Channel Platform (MCP) for dynamic tool and agent registration.</p> <p>MCP is optional and complements the native skills system. Use it when you need to bridge into MCP-compatible ecosystems while keeping the agent model and hooks unchanged.</p>"},{"location":"skills/core/mcp/#features","title":"Features","text":"<ul> <li>Register tools and discover agents</li> <li>Enable cross-platform orchestration</li> </ul>"},{"location":"skills/core/mcp/#example-add-mcp-skill-to-an-agent","title":"Example: Add MCP Skill to an Agent","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.core.mcp import MCPSkill\n\nagent = BaseAgent(\n    name=\"mcp-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"mcp\": MCPSkill({})\n    }\n)\n</code></pre>"},{"location":"skills/core/mcp/#example-register-a-dynamic-tool","title":"Example: Register a Dynamic Tool","text":"<pre><code>from webagents.agents.skills import Skill, tool\n\nclass DynamicToolSkill(Skill):\n    def __init__(self):\n        super().__init__()\n        self.mcp = self.agent.skills[\"mcp\"]\n\n    @tool\n    async def register_tool(self, name: str, description: str) -&gt; str:\n        \"\"\"Register a new tool with MCP\"\"\"\n        await self.mcp.register_dynamic_tool(name, description)\n        return f\"Registered tool: {name}\"\n</code></pre> <p>Implementation: See <code>robutler/agents/skills/core/mcp/skill.py</code>. </p>"},{"location":"skills/core/memory/","title":"Memory Skills","text":"<p>Adds memory and context retention to agents.</p> <p>Robutler offers multiple memory options (short-term, vector) as individual skills so you can choose the right persistence strategy. Memory skills integrate with the unified context to store/retrieve data safely in async environments.</p>"},{"location":"skills/core/memory/#features","title":"Features","text":"<ul> <li>Store, retrieve, and manage conversational or task memory</li> <li>Integrates with agent context and skills</li> </ul>"},{"location":"skills/core/memory/#example-add-memory-skill-to-an-agent","title":"Example: Add Memory Skill to an Agent","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.core.memory.short_term.skill import ShortTermMemorySkill\n\nagent = BaseAgent(\n    name=\"memory-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"memory\": ShortTermMemorySkill({\"max_messages\": 50})\n    }\n)\n</code></pre>"},{"location":"skills/core/memory/#example-use-memory-in-a-skill","title":"Example: Use Memory in a Skill","text":"<pre><code>from webagents.agents.skills import Skill, tool\n\nclass RememberSkill(Skill):\n    def __init__(self):\n        super().__init__()\n        self.memory = self.agent.skills[\"memory\"]\n\n    @tool\n    async def remember(self, key: str, value: str) -&gt; str:\n        \"\"\"Store a value in memory\"\"\"\n        await self.memory.set(key, value)\n        return f\"Remembered {key} = {value}\"\n\n    @tool\n    async def recall(self, key: str) -&gt; str:\n        \"\"\"Retrieve a value from memory\"\"\"\n        value = await self.memory.get(key)\n        return value or \"Not found\"\n</code></pre> <p>Implementation: e.g., <code>robutler/agents/skills/core/memory/short_term/skill.py</code>.</p>"},{"location":"skills/ecosystem/","title":"Ecosystem Skills","text":"<p>A growing collection of third-party integrations and community-contributed skills that extend your agents' capabilities with popular services and platforms.</p>"},{"location":"skills/ecosystem/#workflow-automation","title":"Workflow Automation","text":"<ul> <li> <p>\ud83d\ude80 n8n Skill</p> <p>Connect to n8n instances (self-hosted or cloud) to execute workflows, monitor status, and automate tasks.</p> <p>Features: Workflow execution, status monitoring, secure API key storage</p> <p>Learn more \u2192</p> </li> <li> <p>\u26a1 Zapier Skill</p> <p>Integrate with Zapier to trigger Zaps and automate workflows across 7,000+ supported applications.</p> <p>Features: Zap triggering, task monitoring, 7,000+ app integrations</p> <p>Learn more \u2192</p> </li> <li> <p>\ud83e\udd16 CrewAI Skill</p> <p>Orchestrate multi-agent crews for collaborative AI workflows and complex task execution.</p> <p>Features: Agent crews, task delegation, process management, execution tracking</p> <p>Learn more \u2192</p> </li> </ul>"},{"location":"skills/ecosystem/#social-media-communication","title":"Social Media &amp; Communication","text":"<ul> <li> <p>\ud83d\udc26 X.com (Twitter) Skill</p> <p>Ultra-minimal X.com integration with OAuth 1.0a authentication, user subscriptions, and real-time notifications.</p> <p>Features: Tweet posting, user subscriptions, webhook monitoring, per-user rate limits</p> <p>Learn more \u2192</p> </li> </ul>"},{"location":"skills/ecosystem/#cloud-services","title":"Cloud Services","text":"<ul> <li> <p>\ud83d\udd0d Google Skill</p> <p>Integrate with Google services including Search, Gmail, Calendar, and Drive.</p> <p>Features: Google services integration, OAuth authentication</p> <p>Learn more \u2192</p> </li> </ul>"},{"location":"skills/ecosystem/#ai-machine-learning","title":"AI &amp; Machine Learning","text":"<ul> <li> <p>\ud83e\udd16 Replicate Skill</p> <p>Execute any machine learning model via Replicate's API - from text generation to image creation and video processing.</p> <p>Features: ML model execution, real-time monitoring, model discovery, prediction management</p> <p>Learn more \u2192</p> </li> <li> <p>\ud83e\udd16 CrewAI Skill</p> <p>Orchestrate multi-agent workflows using the CrewAI framework.</p> <p>Features: Multi-agent coordination, task delegation</p> <p>Learn more \u2192</p> </li> </ul>"},{"location":"skills/ecosystem/#data-storage","title":"Data &amp; Storage","text":"<ul> <li> <p>\ud83d\uddc3\ufe0f Supabase/PostgreSQL Skill</p> <p>Connect to Supabase and PostgreSQL databases for data operations and real-time functionality.</p> <p>Features: SQL queries, CRUD operations, secure credential storage, dual database support</p> <p>Learn more \u2192</p> </li> <li> <p>\ud83d\udcc4 MongoDB Skill</p> <p>Connect to MongoDB Atlas, local, or self-hosted deployments for document database operations.</p> <p>Features: Document CRUD, aggregation pipelines, flexible deployment support, simple management</p> <p>Learn more \u2192</p> </li> <li> <p>\ud83d\udcc1 Filesystem Skill</p> <p>Read, write, and manage files and directories on the local filesystem.</p> <p>Features: File operations, directory management, metadata access</p> <p>Learn more \u2192</p> </li> </ul>"},{"location":"skills/ecosystem/#integration-patterns","title":"Integration Patterns","text":""},{"location":"skills/ecosystem/#quick-setup-example","title":"Quick Setup Example","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.ecosystem.x_com import XComSkill\nfrom webagents.agents.skills.ecosystem.n8n import N8nSkill\nfrom webagents.agents.skills.ecosystem.zapier import ZapierSkill\nfrom webagents.agents.skills.ecosystem.replicate import ReplicateSkill\nfrom webagents.agents.skills.ecosystem.crewai import CrewAISkill\nfrom webagents.agents.skills.ecosystem.database import SupabaseSkill\nfrom webagents.agents.skills.ecosystem.mongodb import MongoDBSkill\n\n# All dependencies are automatically resolved\nagent = BaseAgent(\n    name=\"automation-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"n8n\": N8nSkill(),        # Auto-resolves: auth, kv\n        \"zapier\": ZapierSkill(),  # Auto-resolves: auth, kv\n        \"replicate\": ReplicateSkill(),  # Auto-resolves: auth, kv\n        \"crewai\": CrewAISkill(),  # Auto-resolves: auth, kv\n        \"database\": SupabaseSkill(),  # Auto-resolves: auth, kv\n        \"mongodb\": MongoDBSkill(), # Auto-resolves: auth, kv\n        \"x_com\": XComSkill()      # Auto-resolves: auth, kv, notifications\n    }\n)\n</code></pre>"},{"location":"skills/ecosystem/#robutler-agentic-use-cases","title":"Robutler Agentic Use Cases","text":"<p>Automated Content Generation and Publishing - AI agents autonomously generate, edit, and publish content - Agents collaborate to ensure brand alignment and audience preferences - CrewAI orchestrates content teams with specialized roles</p> <p>Dynamic Supply Chain Management - Agents monitor inventory and predict demand fluctuations - Autonomous coordination with suppliers in real-time - Database skills track inventory and transaction history</p> <p>Personalized Customer Support - AI agents handle inquiries, returns, and refunds autonomously - X.com monitoring for customer service opportunities - Zapier/n8n workflows for seamless customer journey automation</p> <p>Agent-to-Agent Economic Transactions - Agents discover intents and coordinate economic exchanges - Database tracking of agent transactions and performance - Workflow automation for payment processing and compliance</p>"},{"location":"skills/ecosystem/#best-practices","title":"Best Practices","text":"<p>All ecosystem skills follow WebAgents best practices:</p> <ul> <li>\ud83d\udc64 User context management via auth skill</li> <li>\ud83d\udcbe Simple credential storage via KV skill  </li> <li>\u2705 API key validation during setup</li> <li>\ud83d\udcdd Clear error handling with helpful user messages</li> <li>\ud83e\uddea Comprehensive testing with automated test suites</li> </ul>"},{"location":"skills/ecosystem/#contributing","title":"Contributing","text":"<p>Help grow the ecosystem by contributing new skills:</p> <ol> <li>Follow the skill pattern established by existing ecosystem skills</li> <li>Include comprehensive tests covering all functionality</li> <li>Provide clear documentation with examples and troubleshooting</li> <li>Implement simple credential management using auth/KV skills</li> <li>Add error handling with user-friendly messages</li> </ol>"},{"location":"skills/ecosystem/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Each skill has comprehensive docs with examples</li> <li>Test Suites: Review test files for usage patterns</li> <li>Community: Join discussions about skill development</li> <li>Issues: Report bugs or request features in the main repository</li> </ul> <p>The ecosystem grows with every contribution. Build the skill you need and share it with the community!</p>"},{"location":"skills/ecosystem/crewai/","title":"CrewAI Skill","text":"<p>Alpha Software Notice</p> <p>This skill is in alpha stage and under active development. APIs, features, and functionality may change without notice. Use with caution in production environments and expect potential breaking changes in future releases.</p> <p>Simplified CrewAI integration for multi-agent orchestration. Execute pre-configured crews of specialized AI agents with collaborative workflows.</p>"},{"location":"skills/ecosystem/crewai/#features","title":"Features","text":"<ul> <li>Pre-configured Crews: Set up crews during skill initialization</li> <li>Single Tool Execution: One simple tool to run configured crews</li> <li>Multi-Agent Orchestration: Specialized AI agents with defined roles</li> <li>Process Management: Sequential and hierarchical workflow execution</li> <li>Input-driven Execution: Provide inputs to drive crew workflows</li> <li>No Dependencies: Simplified architecture with no external skill dependencies</li> </ul>"},{"location":"skills/ecosystem/crewai/#quick-setup","title":"Quick Setup","text":""},{"location":"skills/ecosystem/crewai/#prerequisites","title":"Prerequisites","text":"<p>Install CrewAI:</p> <pre><code>pip install crewai\n</code></pre>"},{"location":"skills/ecosystem/crewai/#option-1-using-crewai-crew-object-recommended","title":"Option 1: Using CrewAI Crew Object (Recommended)","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.ecosystem.crewai import CrewAISkill\nfrom crewai import Agent, Task, Crew, Process\n\n# Create CrewAI agents and tasks using native API\nresearcher = Agent(\n    role='Senior Researcher',\n    goal='Uncover groundbreaking technologies in AI for year 2024',\n    backstory='Driven by curiosity, you explore and share the latest innovations.',\n    verbose=True\n)\n\nresearch_task = Task(\n    description='Identify the next big trend in {topic} with pros and cons.',\n    expected_output='A 3-paragraph report on emerging {topic} technologies.',\n    agent=researcher\n)\n\n# Form the crew\ncrew = Crew(\n    agents=[researcher],\n    tasks=[research_task],\n    process=Process.sequential,\n    verbose=True\n)\n\n# Create agent with CrewAI crew object\nagent = BaseAgent(\n    name=\"research-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"crewai\": CrewAISkill(crew)  # Pass Crew object directly\n    }\n)\n</code></pre>"},{"location":"skills/ecosystem/crewai/#option-2-using-configuration-dictionary","title":"Option 2: Using Configuration Dictionary","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.ecosystem.crewai import CrewAISkill\n\n# Define your crew configuration\ncrew_config = {\n    'agents': [\n        {\n            'role': 'Senior Researcher',\n            'goal': 'Uncover groundbreaking technologies in AI for year 2024',\n            'backstory': 'Driven by curiosity, you explore and share the latest innovations.',\n            'verbose': True\n        }\n    ],\n    'tasks': [\n        {\n            'description': 'Identify the next big trend in {topic} with pros and cons.',\n            'expected_output': 'A 3-paragraph report on emerging {topic} technologies.',\n            'agent_index': 0\n        }\n    ],\n    'process': 'sequential',\n    'verbose': True\n}\n\n# Create agent with configured CrewAI skill\nagent = BaseAgent(\n    name=\"research-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"crewai\": CrewAISkill(crew_config)\n    }\n)\n</code></pre>"},{"location":"skills/ecosystem/crewai/#usage","title":"Usage","text":""},{"location":"skills/ecosystem/crewai/#core-tool","title":"Core Tool","text":"<p>Tool: <code>crewai_run(inputs)</code></p> <p>Execute the configured CrewAI crew with the provided inputs.</p> <pre><code># Example usage via LLM\nmessages = [{\n    'role': 'user', \n    'content': 'Research AI Agents and their applications'\n}]\nresponse = await agent.run(messages=messages)\n</code></pre> <p>The LLM will automatically call <code>crewai_run({'topic': 'AI Agents'})</code> based on the crew configuration and user request.</p>"},{"location":"skills/ecosystem/crewai/#use-case-example-research-team","title":"Use Case Example: Research Team","text":"<p>This example shows a complete research crew that analyzes topics and provides insights:</p>"},{"location":"skills/ecosystem/crewai/#using-crewai-objects-recommended","title":"Using CrewAI Objects (Recommended)","text":"<pre><code>from crewai import Agent, Task, Crew, Process\n\n# Create specialized agents\nresearcher = Agent(\n    role='Senior Researcher',\n    goal='Conduct comprehensive research on specified topics',\n    backstory='You are an experienced researcher with access to latest information and analytical skills.'\n)\n\nanalyst = Agent(\n    role='Data Analyst',\n    goal='Analyze research data and extract key insights',\n    backstory='You specialize in data analysis and pattern recognition.'\n)\n\n# Create sequential tasks\nresearch_task = Task(\n    description='Research the current state of {topic} technology and market trends',\n    expected_output='Detailed research report with key findings',\n    agent=researcher\n)\n\nanalysis_task = Task(\n    description='Analyze the research data and identify the top 3 opportunities in {topic}',\n    expected_output='Analysis report with ranked opportunities and recommendations',\n    agent=analyst\n)\n\n# Form the crew\nresearch_crew = Crew(\n    agents=[researcher, analyst],\n    tasks=[research_task, analysis_task],\n    process=Process.sequential\n)\n\n# Initialize skill with crew\nagent = BaseAgent(\n    name=\"research-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"crewai\": CrewAISkill(research_crew)\n    }\n)\n</code></pre>"},{"location":"skills/ecosystem/crewai/#using-configuration-dictionary","title":"Using Configuration Dictionary","text":"<pre><code>research_crew_config = {\n    'agents': [\n        {\n            'role': 'Senior Researcher',\n            'goal': 'Conduct comprehensive research on specified topics',\n            'backstory': 'You are an experienced researcher with access to latest information and analytical skills.'\n        },\n        {\n            'role': 'Data Analyst', \n            'goal': 'Analyze research data and extract key insights',\n            'backstory': 'You specialize in data analysis and pattern recognition.'\n        }\n    ],\n    'tasks': [\n        {\n            'description': 'Research the current state of {topic} technology and market trends',\n            'expected_output': 'Detailed research report with key findings',\n            'agent_index': 0\n        },\n        {\n            'description': 'Analyze the research data and identify the top 3 opportunities in {topic}',\n            'expected_output': 'Analysis report with ranked opportunities and recommendations',\n            'agent_index': 1\n        }\n    ],\n    'process': 'sequential'\n}\n\n# Initialize skill with configuration\nagent = BaseAgent(\n    name=\"research-agent\", \n    model=\"openai/gpt-4o\",\n    skills={\n        \"crewai\": CrewAISkill(research_crew_config)\n    }\n)\n</code></pre>"},{"location":"skills/ecosystem/crewai/#example-interaction","title":"Example Interaction","text":"<pre><code># User asks for research\nmessages = [{\n    'role': 'user',\n    'content': 'Research the latest developments in quantum computing and identify business opportunities'\n}]\n\n# The LLM automatically:\n# 1. Recognizes this as a research request\n# 2. Calls crewai_run({'topic': 'quantum computing'})\n# 3. The researcher agent researches quantum computing trends\n# 4. The analyst agent identifies top 3 business opportunities\n# 5. Returns comprehensive analysis\n\nresponse = await agent.run(messages=messages)\nprint(response)\n</code></pre>"},{"location":"skills/ecosystem/crewai/#configuration-reference","title":"Configuration Reference","text":"<pre><code>crew_config = {\n    'agents': [\n        {\n            'role': 'Agent Role/Title',           # Required\n            'goal': 'Agent primary objective',   # Required\n            'backstory': 'Agent background',     # Required\n            'verbose': True,                     # Optional\n            'allow_delegation': False            # Optional\n        }\n    ],\n    'tasks': [\n        {\n            'description': 'Task with {input_variable} placeholders',  # Required\n            'expected_output': 'Expected deliverable format',          # Optional\n            'agent_index': 0                                           # Optional, defaults to 0\n        }\n    ],\n    'process': 'sequential',  # 'sequential' or 'hierarchical'\n    'verbose': True           # Optional, defaults to True\n}\n</code></pre>"},{"location":"skills/ecosystem/crewai/#troubleshooting","title":"Troubleshooting","text":"<p>\"No CrewAI crew configured\" - Initialize the skill with a valid crew configuration or Crew object</p> <p>\"Inputs are required to run the crew\" - Ensure your tasks use input variables like <code>{topic}</code> that the LLM can populate</p> <p>\"CrewAI execution failed\" - Check CrewAI installation: <code>pip install crewai</code></p>"},{"location":"skills/ecosystem/database/","title":"Supabase/PostgreSQL Skill","text":"<p>Alpha Software Notice</p> <p>This skill is in alpha stage and under active development. APIs, features, and functionality may change without notice. Use with caution in production environments and expect potential breaking changes in future releases.</p> <p>Minimalistic database integration for Supabase and PostgreSQL operations. Execute queries, manage data, and perform CRUD operations with secure credential storage.</p>"},{"location":"skills/ecosystem/database/#features","title":"Features","text":"<ul> <li>Dual Database Support: Works with both Supabase and PostgreSQL</li> <li>SQL Query Execution: Run parameterized SQL queries safely</li> <li>CRUD Operations: Create, Read, Update, Delete operations on tables</li> <li>Secure Credentials: Connection string storage</li> <li>Per-User Isolation: Each user has their own isolated database configuration</li> </ul>"},{"location":"skills/ecosystem/database/#quick-setup","title":"Quick Setup","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.ecosystem.database import SupabaseSkill\n\nagent = BaseAgent(\n    name=\"database-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"database\": SupabaseSkill()  # Auto-resolves: auth, kv\n    }\n)\n</code></pre> <p>Install dependencies: <pre><code>pip install supabase psycopg2-binary\n</code></pre></p>"},{"location":"skills/ecosystem/database/#core-tools","title":"Core Tools","text":""},{"location":"skills/ecosystem/database/#supabase_setupconfig","title":"<code>supabase_setup(config)</code>","text":"<p>Configure database connection with Supabase or PostgreSQL credentials.</p>"},{"location":"skills/ecosystem/database/#supabase_querysql-params","title":"<code>supabase_query(sql, params)</code>","text":"<p>Execute raw SQL queries with parameterization (PostgreSQL mode only).</p>"},{"location":"skills/ecosystem/database/#supabase_table_opsoperation-table-data-filters","title":"<code>supabase_table_ops(operation, table, data, filters)</code>","text":"<p>Perform CRUD operations: select, insert, update, delete.</p>"},{"location":"skills/ecosystem/database/#supabase_status","title":"<code>supabase_status()</code>","text":"<p>Check database configuration and connection health.</p>"},{"location":"skills/ecosystem/database/#usage-example","title":"Usage Example","text":"<pre><code># Setup database, create user, and query data\nmessages = [{\n    'role': 'user',\n    'content': 'Set up Supabase with URL https://myproject.supabase.co and my API key, then create a new user Alice Smith'\n}]\nresponse = await agent.run(messages=messages)\n</code></pre>"},{"location":"skills/ecosystem/database/#configuration","title":"Configuration","text":"<p>Supabase: <code>supabase_url</code>, <code>supabase_key</code> PostgreSQL: Connection string or individual parameters (<code>host</code>, <code>port</code>, <code>database</code>, <code>user</code>, <code>password</code>)</p>"},{"location":"skills/ecosystem/database/#troubleshooting","title":"Troubleshooting","text":"<p>\"Database not configured\" - Run <code>supabase_setup()</code> with your credentials \"Connection failed\" - Verify database server is accessible and credentials are correct \"Permission denied\" - Check database user privileges and Row Level Security policies</p>"},{"location":"skills/ecosystem/filesystem/","title":"Filesystem Skill","text":"<p>Alpha Software Notice</p> <p>This skill is in alpha stage and under active development. APIs, features, and functionality may change without notice. Use with caution in production environments and expect potential breaking changes in future releases.</p> <p>Status: Under construction \u2013 implementation is in progress. The API below is illustrative and may change.</p>"},{"location":"skills/ecosystem/filesystem/#features","title":"Features","text":"<ul> <li>Read/write files</li> <li>List directories</li> <li>File metadata access</li> </ul>"},{"location":"skills/ecosystem/filesystem/#planned-usage-add-filesystem-skill-to-an-agent","title":"Planned usage: Add Filesystem Skill to an Agent","text":"<pre><code>from webagents.agents import BaseAgent\n# from webagents.agents.skills.ecosystem.filesystem import FilesystemSkill  # coming soon\n\nagent = BaseAgent(\n    name=\"fs-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        # \"filesystem\": FilesystemSkill({})  # coming soon\n    }\n)\n</code></pre>"},{"location":"skills/ecosystem/filesystem/#planned-usage-use-filesystem-tool-in-a-skill","title":"Planned usage: Use Filesystem Tool in a Skill","text":"<pre><code>from webagents.agents.skills import Skill, tool  # example pattern\n\nclass FileOpsSkill(Skill):  # illustrative\n    @tool\n    async def read_file(self, path: str) -&gt; str:\n        \"\"\"Read a file from the filesystem\"\"\"\n        # return await self.agent.skills[\"filesystem\"].read_file(path)\n        raise NotImplementedError(\"filesystem skill integration coming soon\")\n</code></pre> <p>Implementation: Under construction in <code>webagents/agents/skills/ecosystem/filesystem/</code>.</p>"},{"location":"skills/ecosystem/google/","title":"Google Skills","text":"<p>Alpha Software Notice</p> <p>This skill is in alpha stage and under active development. APIs, features, and functionality may change without notice. Use with caution in production environments and expect potential breaking changes in future releases.</p> <p>Integrate with Google APIs including Calendar, with Gmail coming soon.</p>"},{"location":"skills/ecosystem/google/#available-google-skills","title":"Available Google Skills","text":""},{"location":"skills/ecosystem/google/#google-calendar-skill","title":"Google Calendar Skill","text":"<p>Comprehensive Google Calendar integration with OAuth 2.0 authentication and event management.</p>"},{"location":"skills/ecosystem/google/#features","title":"Features","text":"<ul> <li>OAuth 2.0 Authentication: Secure user authorization flow</li> <li>Calendar Event Listing: View upcoming events from primary calendar</li> <li>Token Management: Automatic token refresh and secure storage</li> <li>Multi-User Support: Per-user authentication and data isolation</li> <li>HTTP Callback Handler: Seamless OAuth redirect handling</li> </ul>"},{"location":"skills/ecosystem/google/#quick-setup","title":"Quick Setup","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.ecosystem.google.calendar import GoogleCalendarSkill\n\nagent = BaseAgent(\n    name=\"calendar-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"google_calendar\": GoogleCalendarSkill()\n    }\n)\n</code></pre>"},{"location":"skills/ecosystem/google/#prerequisites","title":"Prerequisites","text":"<p>Environment Variables: <pre><code>export GOOGLE_CLIENT_ID=\"your-google-oauth-client-id\"\nexport GOOGLE_CLIENT_SECRET=\"your-google-oauth-client-secret\"\nexport AGENTS_BASE_URL=\"http://localhost:2224\"  # Your agent server base URL\n</code></pre></p> <p>Google Cloud Console Setup: 1. Create a project in Google Cloud Console 2. Enable the Google Calendar API 3. Create OAuth 2.0 credentials (Web application) 4. Add your redirect URI: <code>{AGENTS_BASE_URL}/agents/{agent-name}/oauth/google/calendar/callback</code></p>"},{"location":"skills/ecosystem/google/#core-tools","title":"Core Tools","text":"<p>1. Calendar Event Listing</p> <p>Tool: <code>list_events(max_results=10)</code></p> <p>Lists upcoming events from the user's primary Google Calendar.</p> <pre><code># Example usage via LLM\nmessages = [{\n    'role': 'user', \n    'content': 'Show me my upcoming calendar events for this week'\n}]\nresponse = await agent.run(messages=messages)\n</code></pre> <p>Features: - Automatic OAuth authorization flow if not authenticated - Token refresh handling for expired tokens - Configurable number of results (default: 10) - Chronological ordering of events - Support for both datetime and all-day events</p>"},{"location":"skills/ecosystem/google/#authentication-flow","title":"Authentication Flow","text":"<p>1. First-Time Authorization: - When <code>list_events()</code> is called without authentication, it returns an authorization URL - User opens the URL and grants calendar access permissions - Google redirects to the callback endpoint with an authorization code - The skill exchanges the code for access and refresh tokens - Tokens are securely stored for future use</p> <p>2. Automatic Token Refresh: - Expired access tokens are automatically refreshed using the refresh token - No user intervention required for token maintenance - Seamless re-authentication for API calls</p>"},{"location":"skills/ecosystem/google/#usage-examples","title":"Usage Examples","text":"<p>Calendar Event Summary: <pre><code># The LLM will automatically handle authentication and API calls\nmessages = [{\n    'role': 'user',\n    'content': 'What meetings do I have today and tomorrow?'\n}]\nresponse = await agent.run(messages=messages)\n</code></pre></p> <p>Event Planning: <pre><code>messages = [{\n    'role': 'user',\n    'content': 'Check my calendar for the next 5 events and suggest optimal times for a 1-hour meeting this week'\n}]\nresponse = await agent.run(messages=messages)\n</code></pre></p> <p>Schedule Analysis: <pre><code>messages = [{\n    'role': 'user',\n    'content': 'Analyze my upcoming calendar events and identify any scheduling conflicts or busy periods'\n}]\nresponse = await agent.run(messages=messages)\n</code></pre></p>"},{"location":"skills/ecosystem/google/#configuration","title":"Configuration","text":"<p>OAuth Scopes: - <code>https://www.googleapis.com/auth/calendar.readonly</code> - <code>https://www.googleapis.com/auth/calendar.events.readonly</code></p> <p>Token Storage: - Uses KV skill for secure token persistence - Fallback to in-memory storage if KV skill unavailable - Per-user token isolation with user ID-based keys</p> <p>Callback Endpoint: - Path: <code>/oauth/google/calendar/callback</code> - Method: GET - Handles OAuth authorization code exchange - Returns user-friendly HTML confirmation page</p>"},{"location":"skills/ecosystem/google/#security-features","title":"Security Features","text":"<ul> <li>Secure Token Storage: Tokens stored via KV skill with proper namespacing</li> <li>User Isolation: Each user's tokens stored separately by user ID</li> <li>Scope Limitation: Read-only calendar access only</li> <li>Token Refresh: Automatic handling of expired tokens</li> <li>Error Handling: Comprehensive error messages for troubleshooting</li> </ul>"},{"location":"skills/ecosystem/google/#troubleshooting","title":"Troubleshooting","text":"<p>Common Issues:</p> <p>\"Missing user identity\" - Ensure the agent has access to user context - Verify authentication middleware is properly configured</p> <p>\"Not authorized\" - User needs to complete OAuth flow by opening the provided authorization URL - Check that Google OAuth credentials are properly configured</p> <p>\"Permission error (403)\" - Verify Google Calendar API is enabled in Google Cloud Console - Check OAuth scopes are correctly configured - Ensure user has granted necessary permissions</p> <p>\"Token expired or invalid\" - The skill will automatically attempt token refresh - If refresh fails, user needs to re-authorize</p>"},{"location":"skills/ecosystem/google/#api-reference","title":"API Reference","text":"<p>GoogleCalendarSkill Methods: - <code>list_events(max_results: int = 10) -&gt; str</code>: List upcoming calendar events - OAuth callback handler for authorization code exchange - Automatic token refresh and storage management</p>"},{"location":"skills/ecosystem/google/#coming-soon","title":"Coming Soon","text":""},{"location":"skills/ecosystem/google/#gmail-skill","title":"Gmail Skill \ud83d\udea7","text":"<p>Gmail integration is currently in development and will include:</p> <ul> <li>Email Management: Read, send, and organize emails</li> <li>OAuth 2.0 Authentication: Secure Gmail access</li> <li>Search and Filter: Advanced email search capabilities  </li> <li>Attachment Handling: Download and process email attachments</li> <li>Label Management: Organize emails with Gmail labels</li> <li>Draft Management: Create and manage email drafts</li> </ul> <p>Stay tuned for the Gmail skill release in upcoming versions!</p>"},{"location":"skills/ecosystem/google/#dependencies","title":"Dependencies","text":"<p>Google Calendar Skill: - <code>httpx</code>: HTTP client for Google API requests - <code>json</code>: Token serialization and storage - KV skill (optional): Secure token persistence</p> <p>Required Environment: - Google Cloud Console project with Calendar API enabled - OAuth 2.0 web application credentials - Properly configured redirect URIs</p>"},{"location":"skills/ecosystem/google/#best-practices","title":"Best Practices","text":""},{"location":"skills/ecosystem/google/#security","title":"Security","text":"<ul> <li>Always use environment variables for OAuth credentials</li> <li>Never commit client secrets to version control</li> <li>Use HTTPS in production for OAuth callbacks</li> <li>Implement proper user session management</li> </ul>"},{"location":"skills/ecosystem/google/#performance","title":"Performance","text":"<ul> <li>Cache calendar data when appropriate</li> <li>Use reasonable <code>max_results</code> limits for API calls</li> <li>Implement error handling and retry logic</li> <li>Monitor API quota usage</li> </ul>"},{"location":"skills/ecosystem/google/#user-experience","title":"User Experience","text":"<ul> <li>Provide clear authorization instructions</li> <li>Handle authentication errors gracefully</li> <li>Offer helpful error messages for common issues</li> <li>Support multiple calendar time zones</li> </ul>"},{"location":"skills/ecosystem/google/#advanced-features","title":"Advanced Features","text":""},{"location":"skills/ecosystem/google/#multi-calendar-support","title":"Multi-Calendar Support","text":"<p>The current implementation focuses on the primary calendar, with multi-calendar support planned for future releases.</p>"},{"location":"skills/ecosystem/google/#event-creation","title":"Event Creation","text":"<p>Write capabilities (event creation, modification) are planned for future versions with appropriate OAuth scopes.</p>"},{"location":"skills/ecosystem/google/#webhook-support","title":"Webhook Support","text":"<p>Real-time calendar change notifications via Google Calendar webhooks are under consideration for future releases. </p>"},{"location":"skills/ecosystem/mongodb/","title":"MongoDB Skill","text":"<p>Alpha Software Notice</p> <p>This skill is in alpha stage and under active development. APIs, features, and functionality may change without notice. Use with caution in production environments and expect potential breaking changes in future releases.</p> <p>Minimalistic MongoDB integration for document database operations. Connect to MongoDB Atlas, local instances, or self-hosted deployments with secure credential storage.</p>"},{"location":"skills/ecosystem/mongodb/#features","title":"Features","text":"<ul> <li>Flexible Deployment Support: Works with MongoDB Atlas, local, and self-hosted instances</li> <li>Document Operations: Complete CRUD operations for MongoDB collections</li> <li>Aggregation Pipelines: Execute complex data processing and analytics</li> <li>Credential Management: Connection string storage</li> <li>Per-User Isolation: Each user has their own isolated MongoDB configuration</li> </ul>"},{"location":"skills/ecosystem/mongodb/#quick-setup","title":"Quick Setup","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.ecosystem.mongodb import MongoDBSkill\n\nagent = BaseAgent(\n    name=\"mongodb-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"mongodb\": MongoDBSkill()  # Auto-resolves: auth, kv\n    }\n)\n</code></pre> <p>Install dependencies: <pre><code>pip install pymongo\n</code></pre></p>"},{"location":"skills/ecosystem/mongodb/#core-tools","title":"Core Tools","text":""},{"location":"skills/ecosystem/mongodb/#mongodb_setupconfig","title":"<code>mongodb_setup(config)</code>","text":"<p>Configure MongoDB connection with Atlas, local, or custom deployment credentials.</p>"},{"location":"skills/ecosystem/mongodb/#mongodb_querydatabase-collection-operation-query-data","title":"<code>mongodb_query(database, collection, operation, query, data)</code>","text":"<p>Execute CRUD operations: find, insert_one, update_one, delete_one, etc.</p>"},{"location":"skills/ecosystem/mongodb/#mongodb_aggregatedatabase-collection-pipeline","title":"<code>mongodb_aggregate(database, collection, pipeline)</code>","text":"<p>Execute MongoDB aggregation pipelines for data processing and analytics.</p>"},{"location":"skills/ecosystem/mongodb/#mongodb_status","title":"<code>mongodb_status()</code>","text":"<p>Check MongoDB configuration and connection health.</p>"},{"location":"skills/ecosystem/mongodb/#usage-example","title":"Usage Example","text":"<pre><code># Setup MongoDB Atlas and perform operations\nmessages = [{\n    'role': 'user',\n    'content': 'Set up MongoDB Atlas with connection string mongodb+srv://user:pass@cluster.net/db, then find all active agents'\n}]\nresponse = await agent.run(messages=messages)\n</code></pre>"},{"location":"skills/ecosystem/mongodb/#configuration","title":"Configuration","text":"<p>MongoDB Atlas: <code>mongodb+srv://user:pass@cluster.mongodb.net/database</code> Local MongoDB: <code>mongodb://localhost:27017/database</code> Self-hosted: <code>mongodb://user:pass@hostname:27017/database</code></p>"},{"location":"skills/ecosystem/mongodb/#troubleshooting","title":"Troubleshooting","text":"<p>\"MongoDB not configured\" - Run <code>mongodb_setup()</code> with your connection credentials \"Connection failed\" - Verify MongoDB server is accessible and credentials are correct \"Authentication failed\" - Check username/password and database privileges</p>"},{"location":"skills/ecosystem/n8n/","title":"n8n Skill","text":"<p>Alpha Software Notice</p> <p>This skill is in alpha stage and under active development. APIs, features, and functionality may change without notice. Use with caution in production environments and expect potential breaking changes in future releases.</p> <p>Minimalistic n8n integration for workflow automation. Execute workflows, monitor status, and manage automation tasks with secure credential storage.</p>"},{"location":"skills/ecosystem/n8n/#features","title":"Features","text":"<ul> <li>Secure API key storage via auth and KV skills</li> <li>Execute workflows with custom input data</li> <li>List workflows from your n8n instance</li> <li>Monitor execution status in real-time</li> <li>Multi-instance support (localhost, self-hosted, n8n Cloud)</li> </ul>"},{"location":"skills/ecosystem/n8n/#quick-setup","title":"Quick Setup","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.ecosystem.n8n import N8nSkill\n\nagent = BaseAgent(\n    name=\"n8n-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"n8n\": N8nSkill()  # Auto-resolves: auth, kv\n    }\n)\n</code></pre>"},{"location":"skills/ecosystem/n8n/#core-tools","title":"Core Tools","text":""},{"location":"skills/ecosystem/n8n/#n8n_setupapi_key-base_url","title":"<code>n8n_setup(api_key, base_url)</code>","text":"<p>Set up n8n API credentials with automatic validation.</p>"},{"location":"skills/ecosystem/n8n/#n8n_executeworkflow_id-data","title":"<code>n8n_execute(workflow_id, data)</code>","text":"<p>Execute an n8n workflow with optional input data.</p>"},{"location":"skills/ecosystem/n8n/#n8n_list_workflows","title":"<code>n8n_list_workflows()</code>","text":"<p>List all available workflows in your n8n instance.</p>"},{"location":"skills/ecosystem/n8n/#n8n_statusexecution_id","title":"<code>n8n_status(execution_id)</code>","text":"<p>Check the status of a workflow execution.</p>"},{"location":"skills/ecosystem/n8n/#usage-example","title":"Usage Example","text":"<pre><code># Setup, list workflows, execute, and check status\nmessages = [\n    {\"role\": \"user\", \"content\": \"Set up n8n with API key your_api_key, list workflows, then execute workflow 123 with customer data\"}\n]\nresponse = await agent.run(messages=messages)\n</code></pre>"},{"location":"skills/ecosystem/n8n/#getting-your-n8n-api-key","title":"Getting Your n8n API Key","text":"<p>n8n Cloud: Settings &gt; n8n API &gt; Create an API key Self-hosted: Settings &gt; n8n API &gt; Create an API key Local: Start n8n (<code>npx n8n start</code>) &gt; Settings &gt; n8n API &gt; Create key</p>"},{"location":"skills/ecosystem/n8n/#troubleshooting","title":"Troubleshooting","text":"<p>Connection Issues - Verify n8n instance is running and base URL is correct Authentication Problems - Check API key is active and has required permissions Workflow Execution Issues - Confirm workflow exists and is properly configured</p>"},{"location":"skills/ecosystem/openai/","title":"OpenAI Workflows Skill","text":"<p>Execute OpenAI hosted agents and workflows seamlessly within your WebAgents, with real-time streaming and automatic cost tracking.</p>"},{"location":"skills/ecosystem/openai/#overview","title":"Overview","text":"<p>The OpenAI Agent Builder skill allows you to integrate OpenAI's hosted workflows as handoff handlers, enabling you to leverage OpenAI's agent building capabilities while maintaining full integration with the WebAgents platform.</p> <p>Key Features:</p> <ul> <li>\ud83c\udf0a Real-time streaming - Word-by-word response streaming</li> <li>\ud83d\udcb0 Automatic cost tracking - Token usage logged for accurate billing</li> <li>\ud83d\udd04 Session support - Multi-turn conversations with memory</li> <li>\ud83d\udd0c Seamless handoffs - Integrates as a standard handoff handler</li> <li>\ud83d\udcca Tracing enabled - Built-in debugging and monitoring</li> <li>\ud83e\udde0 Thinking support - Detects and wraps reasoning model thinking in <code>&lt;think&gt;</code> tags</li> </ul>"},{"location":"skills/ecosystem/openai/#installation","title":"Installation","text":"<p>The OpenAI Workflows skill is included in the ecosystem skills package:</p> <pre><code>from webagents.agents.skills.ecosystem.openai import OpenAIAgentBuilderSkill\n</code></pre>"},{"location":"skills/ecosystem/openai/#configuration","title":"Configuration","text":""},{"location":"skills/ecosystem/openai/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>workflow_id</code>: Your OpenAI workflow ID (e.g., <code>wf_68e56f477fe48190ad3056eff9ad5e0200d2d26229af6c70</code>)</li> <li><code>OPENAI_API_KEY</code>: Set in your environment (<code>.env</code> file)</li> </ul>"},{"location":"skills/ecosystem/openai/#optional-parameters","title":"Optional Parameters","text":"<ul> <li><code>api_base</code>: OpenAI API base URL (default: <code>https://api.openai.com/v1</code>)</li> <li><code>version</code>: Workflow version (default: <code>None</code> = latest)</li> </ul> <p>Best Practice: Omit Version</p> <p>Don't specify a version unless required. When omitted, the workflow uses its default version, which:</p> <ul> <li>\u2705 Automatically uses the latest stable version</li> <li>\u2705 Benefits from workflow improvements</li> <li>\u2705 Reduces maintenance burden</li> </ul> <p>Only specify version if you need a specific workflow structure or the default doesn't work.</p>"},{"location":"skills/ecosystem/openai/#basic-usage","title":"Basic Usage","text":""},{"location":"skills/ecosystem/openai/#with-baseagent","title":"With BaseAgent","text":"<pre><code>from webagents.agents.core.base_agent import BaseAgent\nfrom webagents.agents.skills.ecosystem.openai import OpenAIAgentBuilderSkill\n\nagent = BaseAgent(\n    name=\"workflow-agent\",\n    instructions=\"You are powered by OpenAI workflows\",\n    skills={\n        \"openai_workflow\": OpenAIAgentBuilderSkill({\n            'workflow_id': 'wf_68e56f477fe48190ad3056eff9ad5e0200d2d26229af6c70'\n        })\n    }\n)\n\n# Run streaming\nasync for chunk in agent.run_streaming([\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n]):\n    print(chunk)\n</code></pre>"},{"location":"skills/ecosystem/openai/#environment-setup","title":"Environment Setup","text":"<p>Create a <code>.env</code> file:</p> <pre><code>OPENAI_API_KEY=sk-proj-your-key-here\n</code></pre> <p>The skill automatically loads this key at initialization.</p>"},{"location":"skills/ecosystem/openai/#how-it-works","title":"How It Works","text":""},{"location":"skills/ecosystem/openai/#message-flow","title":"Message Flow","text":"<ol> <li>Input: Standard OpenAI chat format messages</li> <li>Filter: Only user messages sent to workflow (system/assistant filtered out)</li> <li>Convert: Transform to workflow input format</li> <li>Stream: SSE events from OpenAI workflows API</li> <li>Normalize: Convert to OpenAI completion chunks</li> <li>Yield: Real-time to client</li> </ol>"},{"location":"skills/ecosystem/openai/#message-filtering","title":"Message Filtering","text":"<p>OpenAI workflows don't handle <code>system</code> or <code>assistant</code> roles. The skill automatically filters:</p> <pre><code># Input\n[\n  {\"role\": \"system\", \"content\": \"You are helpful\"},\n  {\"role\": \"user\", \"content\": \"Hello!\"},\n  {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n  {\"role\": \"user\", \"content\": \"How are you?\"}\n]\n\n# Sent to workflow (user messages only)\n[\n  {\"role\": \"user\", \"content\": \"Hello!\"},\n  {\"role\": \"user\", \"content\": \"How are you?\"}\n]\n</code></pre>"},{"location":"skills/ecosystem/openai/#streaming-deltas","title":"Streaming Deltas","text":"<p>The skill extracts word-by-word deltas from <code>workflow.node.agent.response</code> events:</p> <pre><code>workflow.started \u2192 workflow.node.agent.response (delta: \"Hello\")\n                \u2192 workflow.node.agent.response (delta: \" there\")\n                \u2192 workflow.node.agent.response (delta: \"!\")\n                \u2192 workflow.finished\n</code></pre> <p>Each delta is immediately yielded as a streaming chunk for real-time display.</p>"},{"location":"skills/ecosystem/openai/#usage-tracking","title":"Usage Tracking","text":"<p>Token usage is automatically tracked and logged to <code>context.usage</code>:</p> <pre><code>{\n    'type': 'llm',\n    'timestamp': 1759984808.392,\n    'model': 'gpt-5-nano-2025-08-07',\n    'prompt_tokens': 17,\n    'completion_tokens': 208,\n    'total_tokens': 225,\n    'streaming': True,\n    'source': 'openai_workflow'\n}\n</code></pre> <p>This integrates with the Payment Skill for automatic cost calculation and billing.</p>"},{"location":"skills/ecosystem/openai/#thinking-content-detection","title":"Thinking Content Detection","text":"<p>The skill automatically detects and wraps thinking/reasoning content in <code>&lt;think&gt;</code> tags for proper UI rendering.</p>"},{"location":"skills/ecosystem/openai/#type-based-detection","title":"Type-Based Detection","text":"<p>Detection is based purely on the <code>type</code> field in OpenAI's SSE responses - no model name checking required:</p> <ol> <li>Type field monitoring - Checks <code>response_data.get('type')</code> for keywords</li> <li>Automatic wrapping - Opens <code>&lt;think&gt;</code> tag when <code>reasoning</code>, <code>thinking</code>, or <code>summary</code> detected</li> <li>Smart closure - Closes <code>&lt;/think&gt;</code> tag when type changes to regular content</li> <li>Guaranteed closure - Ensures tags are closed at workflow finish</li> </ol> <p>Why this works: OpenAI workflows explicitly mark content types in their SSE responses, making detection reliable regardless of which model is used.</p>"},{"location":"skills/ecosystem/openai/#openai-workflow-format","title":"OpenAI Workflow Format","text":"<p>OpenAI workflows use specific type markers in their SSE responses:</p> <pre><code>{\n  \"delta\": \"Let me think about this...\",\n  \"type\": \"response.reasoning_summary_text.delta\",  // Thinking content\n  ...\n}\n</code></pre> <pre><code>{\n  \"delta\": \"Based on my analysis...\",\n  \"type\": \"response.text.delta\",  // Regular output\n  ...\n}\n</code></pre>"},{"location":"skills/ecosystem/openai/#example-output","title":"Example Output","text":"<p>For any workflow that generates thinking content (e.g., gpt5-nano, o1, o3):</p> <pre><code>&lt;think&gt;\n**Analyzing the problem**\n\nI need to consider:\n1. The core requirements\n2. Potential edge cases\n3. Performance implications\n\nLet me work through this step by step...\n&lt;/think&gt;\n\nBased on my analysis, I recommend approach B because...\n</code></pre>"},{"location":"skills/ecosystem/openai/#supported-type-markers","title":"Supported Type Markers","text":"<p>The skill wraps content when the delta <code>type</code> field contains: - <code>\"reasoning\"</code> - Reasoning/chain-of-thought content - <code>\"thinking\"</code> - Internal thought process - <code>\"summary\"</code> - Reasoning summaries</p> <p>Common OpenAI workflow types: - <code>response.reasoning_summary_text.delta</code> \u2192 Wrapped in <code>&lt;think&gt;</code> - <code>response.text.delta</code> \u2192 Regular output (not wrapped)</p>"},{"location":"skills/ecosystem/openai/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"skills/ecosystem/openai/#pin-to-specific-version","title":"Pin to Specific Version","text":"<pre><code>OpenAIAgentBuilderSkill({\n    'workflow_id': 'wf_68e56f477fe48190ad3056eff9ad5e0200d2d26229af6c70',\n    'version': '3'  # Pin to version 3\n})\n</code></pre>"},{"location":"skills/ecosystem/openai/#custom-api-base","title":"Custom API Base","text":"<pre><code>OpenAIAgentBuilderSkill({\n    'workflow_id': 'wf_68e56f477fe48190ad3056eff9ad5e0200d2d26229af6c70',\n    'api_base': 'https://custom-api.example.com/v1'\n})\n</code></pre>"},{"location":"skills/ecosystem/openai/#testing","title":"Testing","text":"<p>Test the workflow directly with curl:</p> <pre><code>curl 'https://api.openai.com/v1/workflows/wf_YOUR_WORKFLOW_ID/run' \\\n  -H 'authorization: Bearer YOUR_OPENAI_API_KEY' \\\n  -H 'content-type: application/json' \\\n  --data-raw '{\n    \"input_data\": {\n      \"input\": [{\n        \"role\": \"user\",\n        \"content\": [{\"type\": \"input_text\", \"text\": \"hi\"}]\n      }]\n    },\n    \"state_values\": [],\n    \"session\": true,\n    \"tracing\": {\"enabled\": true},\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"skills/ecosystem/openai/#handoff-integration","title":"Handoff Integration","text":"<p>The skill registers itself as a streaming handoff handler:</p> <pre><code>agent.register_handoff(\n    Handoff(\n        target=f\"openai_workflow_{workflow_id}\",\n        description=f\"OpenAI Workflow handler\",\n        metadata={\n            'function': self.run_workflow_stream,\n            'priority': 10,\n            'is_generator': True  # Streaming enabled\n        }\n    )\n)\n</code></pre> <p>This allows the agent to use OpenAI workflows as its primary completion handler.</p>"},{"location":"skills/ecosystem/openai/#architecture","title":"Architecture","text":"<pre><code>graph LR\n    A[User Message] --&gt; B[BaseAgent]\n    B --&gt; C[OpenAI Workflows Skill]\n    C --&gt; D[OpenAI Workflows API]\n    D --&gt; E[SSE Stream]\n    E --&gt; F[Normalize Format]\n    F --&gt; G[Stream to Client]</code></pre>"},{"location":"skills/ecosystem/openai/#error-handling","title":"Error Handling","text":"<ul> <li>HTTP Errors: Captured and returned as error messages</li> <li>Malformed SSE: Logged and skipped</li> <li>Connection Timeouts: 120s default timeout</li> <li>Workflow Failures: <code>workflow.failed</code> events converted to error responses</li> </ul>"},{"location":"skills/ecosystem/openai/#limitations","title":"Limitations","text":"<ol> <li>User Messages Only: System and assistant messages are filtered out</li> <li>No Tool Calling: Workflows don't support external tool integration</li> <li>Workflow-Specific Versions: Each workflow has its own versioning scheme</li> </ol>"},{"location":"skills/ecosystem/openai/#best-practices","title":"Best Practices","text":"<ol> <li>\u2705 Use <code>OPENAI_API_KEY</code> from environment, not config</li> <li>\u2705 Omit <code>version</code> unless you need a specific structure</li> <li>\u2705 Test workflows with curl before integration</li> <li>\u2705 Monitor usage logs to verify cost tracking</li> <li>\u2705 Enable session support for multi-turn conversations</li> </ol>"},{"location":"skills/ecosystem/openai/#api-reference","title":"API Reference","text":"<p>               Bases: <code>Skill</code></p> <p>Skill for running OpenAI hosted agents/workflows via streaming handoffs</p> Source code in <code>webagents/agents/skills/ecosystem/openai/skill.py</code> <pre><code>class OpenAIAgentBuilderSkill(Skill):\n    \"\"\"Skill for running OpenAI hosted agents/workflows via streaming handoffs\"\"\"\n\n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        \"\"\"\n        Initialize OpenAI Agent Builder Skill\n\n        Args:\n            config: Configuration dictionary with:\n                - workflow_id: OpenAI workflow ID (e.g., wf_68e56f477fe48190ad3056eff9ad5e0200d2d26229af6c70)\n                - api_key: OpenAI API key (defaults to OPENAI_API_KEY env var)\n                - api_base: OpenAI API base URL (defaults to https://api.openai.com/v1)\n                - version: Workflow version (optional, defaults to None = use workflow default)\n        \"\"\"\n        super().__init__(config or {})\n\n        self.workflow_id = self.config.get('workflow_id')\n        if not self.workflow_id:\n            raise ValueError(\"workflow_id is required in OpenAIAgentBuilderSkill config\")\n\n        self.api_key = self.config.get('api_key') or os.getenv('OPENAI_API_KEY')\n        if not self.api_key:\n            raise ValueError(\"OpenAI API key not found in config or OPENAI_API_KEY env var\")\n\n        self.api_base = self.config.get('api_base', 'https://api.openai.com/v1')\n        self.version = self.config.get('version')  # Optional: workflow version (None = use default)\n\n        self.logger = get_logger('openai_agent_builder')\n\n        # State for thinking detection\n        self._in_thinking_block = False\n\n    async def initialize(self, agent):\n        \"\"\"Register as streaming handoff handler\"\"\"\n        self.agent = agent\n\n        # Register as handoff (streaming for real-time workflow execution)\n        agent.register_handoff(\n            Handoff(\n                target=f\"openai_workflow_{self.workflow_id}\",\n                description=f\"OpenAI Workflow handler using {self.workflow_id}\",\n                scope=\"all\",\n                metadata={\n                    'function': self.run_workflow_stream,\n                    'priority': 10,\n                    'is_generator': True  # Streaming\n                }\n            ),\n            source=\"openai_agent_builder\"\n        )\n\n        self.logger.info(f\"\ud83d\udd27 OpenAI Agent Builder registered with workflow: {self.workflow_id}\")\n\n    def _log_workflow_usage(self, usage_data: Dict[str, Any], model: Optional[str]) -&gt; None:\n        \"\"\"Log workflow usage to context for cost tracking\n\n        Args:\n            usage_data: Usage data from workflow response\n            model: Model identifier (optional)\n        \"\"\"\n        try:\n            context = get_context()\n            if not context or not hasattr(context, 'usage'):\n                return\n\n            # Extract token counts from usage data\n            # OpenAI workflows may use different field names\n            prompt_tokens = usage_data.get('prompt_tokens', 0) or usage_data.get('input_tokens', 0)\n            completion_tokens = usage_data.get('completion_tokens', 0) or usage_data.get('output_tokens', 0)\n            total_tokens = usage_data.get('total_tokens', 0) or (prompt_tokens + completion_tokens)\n\n            if total_tokens &gt; 0:\n                usage_record = {\n                    'type': 'llm',\n                    'timestamp': time.time(),\n                    'model': model or f'openai-workflow-{self.workflow_id}',\n                    'prompt_tokens': int(prompt_tokens),\n                    'completion_tokens': int(completion_tokens),\n                    'total_tokens': int(total_tokens),\n                    'streaming': True,\n                    'source': 'openai_workflow'\n                }\n                context.usage.append(usage_record)\n                self.logger.info(f\"\ud83d\udcb0 Workflow usage logged: {total_tokens} tokens (prompt={prompt_tokens}, completion={completion_tokens}) for model={model}\")\n            else:\n                self.logger.debug(f\"\u26a0\ufe0f Workflow usage data present but no tokens: {usage_data}\")\n        except Exception as e:\n            self.logger.warning(f\"Failed to log workflow usage: {e}\")\n\n    def _wrap_thinking_content(self, delta_text: str, response_data: Dict[str, Any]) -&gt; str:\n        \"\"\"Detect and wrap thinking content in &lt;think&gt; tags\n\n        Args:\n            delta_text: The delta content from workflow response\n            response_data: Full response data for context\n\n        Returns:\n            Delta text, potentially wrapped in thinking tags\n        \"\"\"\n        # Check the 'type' field in response_data for thinking markers\n        # OpenAI workflows use: \"response.reasoning_summary_text.delta\" for thinking\n        delta_type = response_data.get('type', '')\n\n        # Check if this is reasoning/thinking content\n        is_reasoning = 'reasoning' in delta_type.lower()\n        is_thinking = 'thinking' in delta_type.lower()\n        is_summary = 'summary' in delta_type.lower()\n\n        # Reasoning or thinking content should be wrapped\n        if is_reasoning or is_thinking or is_summary:\n            if not self._in_thinking_block:\n                self._in_thinking_block = True\n                self.logger.debug(f\"\ud83e\udde0 Starting thinking block (type={delta_type})\")\n                return f\"&lt;think&gt;{delta_text}\"\n            return delta_text\n\n        # If we were in a thinking block and now we're not, close it\n        if self._in_thinking_block and delta_type and not (is_reasoning or is_thinking or is_summary):\n            self._in_thinking_block = False\n            self.logger.debug(f\"\ud83e\udde0 Ending thinking block (type={delta_type})\")\n            return f\"&lt;/think&gt;{delta_text}\"\n\n        # Regular content - pass through\n        return delta_text\n\n    def _convert_messages_to_workflow_input(self, messages: List[Dict[str, Any]]) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Convert OpenAI chat messages to OpenAI workflow input format\n\n        Args:\n            messages: OpenAI format messages [{\"role\": \"user\", \"content\": \"...\"}]\n\n        Returns:\n            Workflow input format [{\"role\": \"user\", \"content\": [{\"type\": \"input_text\", \"text\": \"...\"}]}]\n        \"\"\"\n        workflow_input = []\n\n        for msg in messages:\n            role = msg.get('role', 'user')\n            content = msg.get('content', '')\n\n            # Convert string content to workflow format\n            if isinstance(content, str):\n                workflow_msg = {\n                    \"role\": role,\n                    \"content\": [{\"type\": \"input_text\", \"text\": content}]\n                }\n            elif isinstance(content, list):\n                # Already in structured format\n                workflow_msg = {\n                    \"role\": role,\n                    \"content\": content\n                }\n            else:\n                # Fallback\n                workflow_msg = {\n                    \"role\": role,\n                    \"content\": [{\"type\": \"input_text\", \"text\": str(content)}]\n                }\n\n            workflow_input.append(workflow_msg)\n\n        return workflow_input\n\n    async def run_workflow_stream(\n        self,\n        messages: List[Dict[str, Any]],\n        tools: Optional[List[Dict[str, Any]]] = None,\n        **kwargs\n    ) -&gt; AsyncGenerator[Dict[str, Any], None]:\n        \"\"\"\n        Run OpenAI workflow and stream normalized responses\n\n        Args:\n            messages: OpenAI format chat messages\n            tools: Optional tools (not used by workflows currently)\n            **kwargs: Additional parameters\n\n        Yields:\n            OpenAI chat completion streaming chunks\n        \"\"\"\n        # Reset usage logging flag and thinking state for this request\n        self._usage_logged = False\n        self._in_thinking_block = False\n\n        workflow_url = f\"{self.api_base}/workflows/{self.workflow_id}/run\"\n\n        # Filter to only user messages (workflows don't handle system/assistant roles)\n        user_messages = [msg for msg in messages if msg.get('role') == 'user']\n\n        if not user_messages:\n            # No user messages, use empty input\n            workflow_input = []\n        else:\n            # Convert only user messages to workflow input format\n            workflow_input = self._convert_messages_to_workflow_input(user_messages)\n\n        # Build request payload matching OpenAI workflows v6 format\n        payload = {\n            \"input_data\": {\n                \"input\": workflow_input\n            },\n            \"state_values\": [],\n            \"session\": True,  # Enable session for multi-turn conversations\n            \"tracing\": {\n                \"enabled\": True  # Enable tracing for debugging\n            },\n            \"stream\": True\n        }\n\n        # Include version if explicitly specified\n        if self.version is not None:\n            payload[\"version\"] = str(self.version)\n\n        self.logger.debug(f\"\ud83d\udd04 Calling OpenAI workflow: {workflow_url}\")\n\n        headers = {\n            \"authorization\": f\"Bearer {self.api_key}\",\n            \"content-type\": \"application/json\"\n        }\n\n        # Initialize chunk ID counter\n        chunk_id = 0\n        accumulated_content = \"\"\n\n        try:\n            async with httpx.AsyncClient(timeout=120.0) as client:\n                async with client.stream('POST', workflow_url, json=payload, headers=headers) as response:\n                    response.raise_for_status()\n\n                    # Parse SSE stream\n                    async for line in response.aiter_lines():\n                        if not line or line.startswith(':'):\n                            continue\n\n                        # Parse SSE format: \"event: type\" and \"data: json\"\n                        if line.startswith('event: '):\n                            current_event = line[7:].strip()\n                            continue\n\n                        if line.startswith('data: '):\n                            data_str = line[6:].strip()\n\n                            try:\n                                data = json.loads(data_str)\n                                event_type = data.get('type', current_event if 'current_event' in locals() else '')\n\n                                # Handle workflow.node.agent.response - streaming content deltas\n                                if event_type == 'workflow.node.agent.response':\n                                    response_data = data.get('data', {})\n                                    delta_text = response_data.get('delta')\n\n                                    # Check for usage data in the response\n                                    response_obj = response_data.get('response', {})\n                                    if response_obj and isinstance(response_obj, dict):\n                                        usage_data = response_obj.get('usage')\n                                        model = response_obj.get('model')\n\n                                        if usage_data and isinstance(usage_data, dict):\n                                            # Log usage once (check if we haven't logged it yet)\n                                            if not self._usage_logged:\n                                                self._usage_logged = True\n                                                self._log_workflow_usage(usage_data, model)\n\n                                    # Yield streaming delta if present and non-empty\n                                    if delta_text and isinstance(delta_text, str):\n                                        chunk_id += 1\n\n                                        # Wrap thinking content if this is a reasoning model\n                                        wrapped_delta = self._wrap_thinking_content(delta_text, response_data)\n                                        accumulated_content += wrapped_delta\n\n                                        # Build delta object\n                                        delta_obj = {'content': wrapped_delta}\n                                        if chunk_id == 1:\n                                            delta_obj['role'] = 'assistant'\n\n                                        yield {\n                                            'id': f'chatcmpl-wf-{self.workflow_id}',\n                                            'object': 'chat.completion.chunk',\n                                            'created': data.get('workflow_run', {}).get('created_at', 0),\n                                            'model': f'openai-workflow-{self.workflow_id}',\n                                            'choices': [{\n                                                'index': 0,\n                                                'delta': delta_obj,\n                                                'finish_reason': None\n                                            }]\n                                        }\n                                        continue  # Skip other processing for this event\n\n                                # Handle workflow.finished event\n                                if event_type == 'workflow.finished':\n                                    self.logger.debug(f\"\ud83d\udce5 Workflow finished. Total content: {len(accumulated_content)} chars\")\n\n                                    # Check for usage data as fallback (if not already logged)\n                                    if not self._usage_logged:\n                                        workflow_result = data.get('result', {})\n                                        if workflow_result and isinstance(workflow_result, dict):\n                                            usage_data = workflow_result.get('usage')\n                                            model = workflow_result.get('model')\n\n                                            if usage_data and isinstance(usage_data, dict):\n                                                self._usage_logged = True\n                                                self._log_workflow_usage(usage_data, model)\n\n                                    # Close thinking block if still open\n                                    if self._in_thinking_block:\n                                        self.logger.debug(\"\ud83e\udde0 Closing thinking block at workflow finish\")\n                                        yield {\n                                            'id': f'chatcmpl-wf-{self.workflow_id}',\n                                            'object': 'chat.completion.chunk',\n                                            'created': data.get('workflow_run', {}).get('created_at', 0),\n                                            'model': f'openai-workflow-{self.workflow_id}',\n                                            'choices': [{\n                                                'index': 0,\n                                                'delta': {'content': '&lt;/think&gt;'},\n                                                'finish_reason': None\n                                            }]\n                                        }\n                                        self._in_thinking_block = False\n\n                                    # Yield finish chunk (content already streamed via deltas)\n                                    yield {\n                                        'id': f'chatcmpl-wf-{self.workflow_id}',\n                                        'object': 'chat.completion.chunk',\n                                        'created': data.get('workflow_run', {}).get('created_at', 0),\n                                        'model': f'openai-workflow-{self.workflow_id}',\n                                        'choices': [{\n                                            'index': 0,\n                                            'delta': {},\n                                            'finish_reason': 'stop'\n                                        }]\n                                    }\n\n                                # Handle workflow.failed event\n                                elif event_type == 'workflow.failed':\n                                    error_msg = data.get('workflow_run', {}).get('error', 'Unknown error')\n                                    self.logger.error(f\"\u274c Workflow failed: {json.dumps(error_msg, indent=2)}\")\n                                    # Yield error message\n                                    yield {\n                                        'id': f'chatcmpl-wf-{self.workflow_id}',\n                                        'object': 'chat.completion.chunk',\n                                        'created': data.get('workflow_run', {}).get('created_at', 0),\n                                        'model': f'openai-workflow-{self.workflow_id}',\n                                        'choices': [{\n                                            'index': 0,\n                                            'delta': {\n                                                'role': 'assistant',\n                                                'content': f\"Workflow error: {error_msg}\"\n                                            },\n                                            'finish_reason': 'stop'\n                                        }]\n                                    }\n\n                                # Log other events for debugging\n                                elif event_type in ['workflow.started', 'workflow.node.started', 'workflow.node.finished']:\n                                    self.logger.debug(f\"\ud83d\udd04 Workflow event: {event_type}\")\n\n                            except json.JSONDecodeError as e:\n                                self.logger.warning(f\"Failed to parse SSE data: {e}\")\n                                continue\n\n        except httpx.HTTPStatusError as e:\n            # Don't try to read response.text on streaming responses\n            error_msg = f\"HTTP {e.response.status_code}\"\n            try:\n                # Try to read error body if not streaming\n                if hasattr(e.response, '_content') and e.response._content is not None:\n                    error_msg = f\"{error_msg} - {e.response.text[:200]}\"\n            except Exception:\n                pass\n\n            self.logger.error(f\"OpenAI workflow API error: {error_msg}\")\n\n            # Yield error message\n            yield {\n                'id': f'chatcmpl-wf-{self.workflow_id}',\n                'object': 'chat.completion.chunk',\n                'created': 0,\n                'model': f'openai-workflow-{self.workflow_id}',\n                'choices': [{\n                    'index': 0,\n                    'delta': {\n                        'role': 'assistant',\n                        'content': f\"Error running workflow: {error_msg}\"\n                    },\n                    'finish_reason': 'stop'\n                }]\n            }\n\n        except Exception as e:\n            self.logger.error(f\"Error running OpenAI workflow: {e}\", exc_info=True)\n            # Yield error message\n            yield {\n                'id': f'chatcmpl-wf-{self.workflow_id}',\n                'object': 'chat.completion.chunk',\n                'created': 0,\n                'model': f'openai-workflow-{self.workflow_id}',\n                'choices': [{\n                    'index': 0,\n                    'delta': {\n                        'role': 'assistant',\n                        'content': f\"Error running workflow: {str(e)}\"\n                    },\n                    'finish_reason': 'stop'\n                }]\n            }\n</code></pre>"},{"location":"skills/ecosystem/openai/#webagents.agents.skills.ecosystem.openai.skill.OpenAIAgentBuilderSkill.__init__","title":"__init__","text":"<pre><code>__init__(config: Optional[Dict[str, Any]] = None)\n</code></pre> <p>Initialize OpenAI Agent Builder Skill</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Configuration dictionary with: - workflow_id: OpenAI workflow ID (e.g., wf_68e56f477fe48190ad3056eff9ad5e0200d2d26229af6c70) - api_key: OpenAI API key (defaults to OPENAI_API_KEY env var) - api_base: OpenAI API base URL (defaults to https://api.openai.com/v1) - version: Workflow version (optional, defaults to None = use workflow default)</p> <code>None</code> Source code in <code>webagents/agents/skills/ecosystem/openai/skill.py</code> <pre><code>def __init__(self, config: Optional[Dict[str, Any]] = None):\n    \"\"\"\n    Initialize OpenAI Agent Builder Skill\n\n    Args:\n        config: Configuration dictionary with:\n            - workflow_id: OpenAI workflow ID (e.g., wf_68e56f477fe48190ad3056eff9ad5e0200d2d26229af6c70)\n            - api_key: OpenAI API key (defaults to OPENAI_API_KEY env var)\n            - api_base: OpenAI API base URL (defaults to https://api.openai.com/v1)\n            - version: Workflow version (optional, defaults to None = use workflow default)\n    \"\"\"\n    super().__init__(config or {})\n\n    self.workflow_id = self.config.get('workflow_id')\n    if not self.workflow_id:\n        raise ValueError(\"workflow_id is required in OpenAIAgentBuilderSkill config\")\n\n    self.api_key = self.config.get('api_key') or os.getenv('OPENAI_API_KEY')\n    if not self.api_key:\n        raise ValueError(\"OpenAI API key not found in config or OPENAI_API_KEY env var\")\n\n    self.api_base = self.config.get('api_base', 'https://api.openai.com/v1')\n    self.version = self.config.get('version')  # Optional: workflow version (None = use default)\n\n    self.logger = get_logger('openai_agent_builder')\n\n    # State for thinking detection\n    self._in_thinking_block = False\n</code></pre>"},{"location":"skills/ecosystem/openai/#webagents.agents.skills.ecosystem.openai.skill.OpenAIAgentBuilderSkill.initialize","title":"initialize  <code>async</code>","text":"<pre><code>initialize(agent)\n</code></pre> <p>Register as streaming handoff handler</p> Source code in <code>webagents/agents/skills/ecosystem/openai/skill.py</code> <pre><code>async def initialize(self, agent):\n    \"\"\"Register as streaming handoff handler\"\"\"\n    self.agent = agent\n\n    # Register as handoff (streaming for real-time workflow execution)\n    agent.register_handoff(\n        Handoff(\n            target=f\"openai_workflow_{self.workflow_id}\",\n            description=f\"OpenAI Workflow handler using {self.workflow_id}\",\n            scope=\"all\",\n            metadata={\n                'function': self.run_workflow_stream,\n                'priority': 10,\n                'is_generator': True  # Streaming\n            }\n        ),\n        source=\"openai_agent_builder\"\n    )\n\n    self.logger.info(f\"\ud83d\udd27 OpenAI Agent Builder registered with workflow: {self.workflow_id}\")\n</code></pre>"},{"location":"skills/ecosystem/openai/#webagents.agents.skills.ecosystem.openai.skill.OpenAIAgentBuilderSkill.run_workflow_stream","title":"run_workflow_stream  <code>async</code>","text":"<pre><code>run_workflow_stream(messages: List[Dict[str, Any]], tools: Optional[List[Dict[str, Any]]] = None, **kwargs) -&gt; AsyncGenerator[Dict[str, Any], None]\n</code></pre> <p>Run OpenAI workflow and stream normalized responses</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, Any]]</code> <p>OpenAI format chat messages</p> required <code>tools</code> <code>Optional[List[Dict[str, Any]]]</code> <p>Optional tools (not used by workflows currently)</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Yields:</p> Type Description <code>AsyncGenerator[Dict[str, Any], None]</code> <p>OpenAI chat completion streaming chunks</p> Source code in <code>webagents/agents/skills/ecosystem/openai/skill.py</code> <pre><code>async def run_workflow_stream(\n    self,\n    messages: List[Dict[str, Any]],\n    tools: Optional[List[Dict[str, Any]]] = None,\n    **kwargs\n) -&gt; AsyncGenerator[Dict[str, Any], None]:\n    \"\"\"\n    Run OpenAI workflow and stream normalized responses\n\n    Args:\n        messages: OpenAI format chat messages\n        tools: Optional tools (not used by workflows currently)\n        **kwargs: Additional parameters\n\n    Yields:\n        OpenAI chat completion streaming chunks\n    \"\"\"\n    # Reset usage logging flag and thinking state for this request\n    self._usage_logged = False\n    self._in_thinking_block = False\n\n    workflow_url = f\"{self.api_base}/workflows/{self.workflow_id}/run\"\n\n    # Filter to only user messages (workflows don't handle system/assistant roles)\n    user_messages = [msg for msg in messages if msg.get('role') == 'user']\n\n    if not user_messages:\n        # No user messages, use empty input\n        workflow_input = []\n    else:\n        # Convert only user messages to workflow input format\n        workflow_input = self._convert_messages_to_workflow_input(user_messages)\n\n    # Build request payload matching OpenAI workflows v6 format\n    payload = {\n        \"input_data\": {\n            \"input\": workflow_input\n        },\n        \"state_values\": [],\n        \"session\": True,  # Enable session for multi-turn conversations\n        \"tracing\": {\n            \"enabled\": True  # Enable tracing for debugging\n        },\n        \"stream\": True\n    }\n\n    # Include version if explicitly specified\n    if self.version is not None:\n        payload[\"version\"] = str(self.version)\n\n    self.logger.debug(f\"\ud83d\udd04 Calling OpenAI workflow: {workflow_url}\")\n\n    headers = {\n        \"authorization\": f\"Bearer {self.api_key}\",\n        \"content-type\": \"application/json\"\n    }\n\n    # Initialize chunk ID counter\n    chunk_id = 0\n    accumulated_content = \"\"\n\n    try:\n        async with httpx.AsyncClient(timeout=120.0) as client:\n            async with client.stream('POST', workflow_url, json=payload, headers=headers) as response:\n                response.raise_for_status()\n\n                # Parse SSE stream\n                async for line in response.aiter_lines():\n                    if not line or line.startswith(':'):\n                        continue\n\n                    # Parse SSE format: \"event: type\" and \"data: json\"\n                    if line.startswith('event: '):\n                        current_event = line[7:].strip()\n                        continue\n\n                    if line.startswith('data: '):\n                        data_str = line[6:].strip()\n\n                        try:\n                            data = json.loads(data_str)\n                            event_type = data.get('type', current_event if 'current_event' in locals() else '')\n\n                            # Handle workflow.node.agent.response - streaming content deltas\n                            if event_type == 'workflow.node.agent.response':\n                                response_data = data.get('data', {})\n                                delta_text = response_data.get('delta')\n\n                                # Check for usage data in the response\n                                response_obj = response_data.get('response', {})\n                                if response_obj and isinstance(response_obj, dict):\n                                    usage_data = response_obj.get('usage')\n                                    model = response_obj.get('model')\n\n                                    if usage_data and isinstance(usage_data, dict):\n                                        # Log usage once (check if we haven't logged it yet)\n                                        if not self._usage_logged:\n                                            self._usage_logged = True\n                                            self._log_workflow_usage(usage_data, model)\n\n                                # Yield streaming delta if present and non-empty\n                                if delta_text and isinstance(delta_text, str):\n                                    chunk_id += 1\n\n                                    # Wrap thinking content if this is a reasoning model\n                                    wrapped_delta = self._wrap_thinking_content(delta_text, response_data)\n                                    accumulated_content += wrapped_delta\n\n                                    # Build delta object\n                                    delta_obj = {'content': wrapped_delta}\n                                    if chunk_id == 1:\n                                        delta_obj['role'] = 'assistant'\n\n                                    yield {\n                                        'id': f'chatcmpl-wf-{self.workflow_id}',\n                                        'object': 'chat.completion.chunk',\n                                        'created': data.get('workflow_run', {}).get('created_at', 0),\n                                        'model': f'openai-workflow-{self.workflow_id}',\n                                        'choices': [{\n                                            'index': 0,\n                                            'delta': delta_obj,\n                                            'finish_reason': None\n                                        }]\n                                    }\n                                    continue  # Skip other processing for this event\n\n                            # Handle workflow.finished event\n                            if event_type == 'workflow.finished':\n                                self.logger.debug(f\"\ud83d\udce5 Workflow finished. Total content: {len(accumulated_content)} chars\")\n\n                                # Check for usage data as fallback (if not already logged)\n                                if not self._usage_logged:\n                                    workflow_result = data.get('result', {})\n                                    if workflow_result and isinstance(workflow_result, dict):\n                                        usage_data = workflow_result.get('usage')\n                                        model = workflow_result.get('model')\n\n                                        if usage_data and isinstance(usage_data, dict):\n                                            self._usage_logged = True\n                                            self._log_workflow_usage(usage_data, model)\n\n                                # Close thinking block if still open\n                                if self._in_thinking_block:\n                                    self.logger.debug(\"\ud83e\udde0 Closing thinking block at workflow finish\")\n                                    yield {\n                                        'id': f'chatcmpl-wf-{self.workflow_id}',\n                                        'object': 'chat.completion.chunk',\n                                        'created': data.get('workflow_run', {}).get('created_at', 0),\n                                        'model': f'openai-workflow-{self.workflow_id}',\n                                        'choices': [{\n                                            'index': 0,\n                                            'delta': {'content': '&lt;/think&gt;'},\n                                            'finish_reason': None\n                                        }]\n                                    }\n                                    self._in_thinking_block = False\n\n                                # Yield finish chunk (content already streamed via deltas)\n                                yield {\n                                    'id': f'chatcmpl-wf-{self.workflow_id}',\n                                    'object': 'chat.completion.chunk',\n                                    'created': data.get('workflow_run', {}).get('created_at', 0),\n                                    'model': f'openai-workflow-{self.workflow_id}',\n                                    'choices': [{\n                                        'index': 0,\n                                        'delta': {},\n                                        'finish_reason': 'stop'\n                                    }]\n                                }\n\n                            # Handle workflow.failed event\n                            elif event_type == 'workflow.failed':\n                                error_msg = data.get('workflow_run', {}).get('error', 'Unknown error')\n                                self.logger.error(f\"\u274c Workflow failed: {json.dumps(error_msg, indent=2)}\")\n                                # Yield error message\n                                yield {\n                                    'id': f'chatcmpl-wf-{self.workflow_id}',\n                                    'object': 'chat.completion.chunk',\n                                    'created': data.get('workflow_run', {}).get('created_at', 0),\n                                    'model': f'openai-workflow-{self.workflow_id}',\n                                    'choices': [{\n                                        'index': 0,\n                                        'delta': {\n                                            'role': 'assistant',\n                                            'content': f\"Workflow error: {error_msg}\"\n                                        },\n                                        'finish_reason': 'stop'\n                                    }]\n                                }\n\n                            # Log other events for debugging\n                            elif event_type in ['workflow.started', 'workflow.node.started', 'workflow.node.finished']:\n                                self.logger.debug(f\"\ud83d\udd04 Workflow event: {event_type}\")\n\n                        except json.JSONDecodeError as e:\n                            self.logger.warning(f\"Failed to parse SSE data: {e}\")\n                            continue\n\n    except httpx.HTTPStatusError as e:\n        # Don't try to read response.text on streaming responses\n        error_msg = f\"HTTP {e.response.status_code}\"\n        try:\n            # Try to read error body if not streaming\n            if hasattr(e.response, '_content') and e.response._content is not None:\n                error_msg = f\"{error_msg} - {e.response.text[:200]}\"\n        except Exception:\n            pass\n\n        self.logger.error(f\"OpenAI workflow API error: {error_msg}\")\n\n        # Yield error message\n        yield {\n            'id': f'chatcmpl-wf-{self.workflow_id}',\n            'object': 'chat.completion.chunk',\n            'created': 0,\n            'model': f'openai-workflow-{self.workflow_id}',\n            'choices': [{\n                'index': 0,\n                'delta': {\n                    'role': 'assistant',\n                    'content': f\"Error running workflow: {error_msg}\"\n                },\n                'finish_reason': 'stop'\n            }]\n        }\n\n    except Exception as e:\n        self.logger.error(f\"Error running OpenAI workflow: {e}\", exc_info=True)\n        # Yield error message\n        yield {\n            'id': f'chatcmpl-wf-{self.workflow_id}',\n            'object': 'chat.completion.chunk',\n            'created': 0,\n            'model': f'openai-workflow-{self.workflow_id}',\n            'choices': [{\n                'index': 0,\n                'delta': {\n                    'role': 'assistant',\n                    'content': f\"Error running workflow: {str(e)}\"\n                },\n                'finish_reason': 'stop'\n            }]\n        }\n</code></pre>"},{"location":"skills/ecosystem/openai/#related-documentation","title":"Related Documentation","text":"<ul> <li>Handoffs System</li> <li>Payment Skill</li> <li>Agent Skills</li> </ul>"},{"location":"skills/ecosystem/replicate/","title":"Replicate Skill","text":"<p>Alpha Software Notice</p> <p>This skill is in alpha stage and under active development. APIs, features, and functionality may change without notice. Use with caution in production environments and expect potential breaking changes in future releases.</p> <p>Advanced machine learning model execution via Replicate's API. Run any public model, from text generation to image creation, with secure credential management and real-time monitoring.</p>"},{"location":"skills/ecosystem/replicate/#features","title":"Features","text":"<ul> <li>Run ML models - Execute text, image, audio, and video models</li> <li>Model discovery - Browse and get detailed information about available models</li> <li>Real-time monitoring - Track prediction progress and get results</li> <li>Secure API token storage via auth and KV skills</li> <li>Cancel predictions to save compute costs</li> <li>Per-user isolation with authentication</li> </ul>"},{"location":"skills/ecosystem/replicate/#quick-setup","title":"Quick Setup","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.ecosystem.replicate import ReplicateSkill\n\nagent = BaseAgent(\n    name=\"replicate-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"replicate\": ReplicateSkill()  # Auto-resolves: auth, kv\n    }\n)\n</code></pre>"},{"location":"skills/ecosystem/replicate/#core-tools","title":"Core Tools","text":""},{"location":"skills/ecosystem/replicate/#replicate_setupapi_token","title":"<code>replicate_setup(api_token)</code>","text":"<p>Set up Replicate API credentials with automatic validation.</p>"},{"location":"skills/ecosystem/replicate/#replicate_list_modelsowner","title":"<code>replicate_list_models(owner)</code>","text":"<p>List available models, optionally filtered by owner (e.g., \"stability-ai\").</p>"},{"location":"skills/ecosystem/replicate/#replicate_get_model_infomodel","title":"<code>replicate_get_model_info(model)</code>","text":"<p>Get detailed information about a specific model including parameters.</p>"},{"location":"skills/ecosystem/replicate/#replicate_run_predictionmodel-input_data","title":"<code>replicate_run_prediction(model, input_data)</code>","text":"<p>Run a prediction with a model and input parameters.</p>"},{"location":"skills/ecosystem/replicate/#replicate_get_predictionprediction_id","title":"<code>replicate_get_prediction(prediction_id)</code>","text":"<p>Get prediction status and results.</p>"},{"location":"skills/ecosystem/replicate/#replicate_cancel_predictionprediction_id","title":"<code>replicate_cancel_prediction(prediction_id)</code>","text":"<p>Cancel a running prediction to save costs.</p>"},{"location":"skills/ecosystem/replicate/#usage-examples","title":"Usage Examples","text":""},{"location":"skills/ecosystem/replicate/#text-generation","title":"Text Generation","text":"<pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"Generate a haiku about AI using LLaMA-2\"}\n]\nresponse = await agent.run(messages=messages)\n</code></pre>"},{"location":"skills/ecosystem/replicate/#image-generation","title":"Image Generation","text":"<pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"Create an image of a dragon flying over mountains using Stable Diffusion\"}\n]\nresponse = await agent.run(messages=messages)\n</code></pre>"},{"location":"skills/ecosystem/replicate/#model-discovery","title":"Model Discovery","text":"<pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"Show me what video generation models are available from Stability AI\"}\n]\nresponse = await agent.run(messages=messages)\n</code></pre>"},{"location":"skills/ecosystem/replicate/#audio-processing","title":"Audio Processing","text":"<pre><code>messages = [\n    {\"role\": \"user\", \"content\": \"Transcribe this audio file using Whisper: https://example.com/audio.mp3\"}\n]\nresponse = await agent.run(messages=messages)\n</code></pre>"},{"location":"skills/ecosystem/replicate/#getting-your-replicate-api-token","title":"Getting Your Replicate API Token","text":"<ol> <li>Sign up at replicate.com</li> <li>Go to Account Settings \u2192 API tokens</li> <li>Create token and copy it (starts with <code>r8_</code>)</li> </ol>"},{"location":"skills/ecosystem/replicate/#model-categories","title":"Model Categories","text":"<p>Text Models: LLaMA, GPT alternatives, chat models, code generation Image Models: Stable Diffusion, DALL-E alternatives, image editing, upscaling Audio Models: Whisper transcription, text-to-speech, music generation Video Models: Text-to-video, video editing, style transfer Multimodal: Vision-language models, image captioning</p>"},{"location":"skills/ecosystem/replicate/#pricing","title":"Pricing","text":"<ul> <li>Free tier: Limited compute credits per month</li> <li>Pay-per-use: Charged based on compute time and model complexity</li> <li>Model-specific pricing: Check individual model pages for costs</li> </ul>"},{"location":"skills/ecosystem/replicate/#troubleshooting","title":"Troubleshooting","text":"<p>Authentication Issues - Verify API token starts with <code>r8_</code> and is correctly copied Model Not Found - Use model discovery tools to find correct model names Invalid Input - Get model info to see required parameters and types Long Predictions - Some models take minutes; use status monitoring Failed Predictions - Check error messages and verify input data format</p>"},{"location":"skills/ecosystem/replicate/#architecture","title":"Architecture","text":"<p>The skill integrates with Replicate's REST API v1, using secure credential storage via KV skill and per-user authentication via Auth skill. Supports both synchronous and asynchronous model execution patterns.</p>"},{"location":"skills/ecosystem/replicate/#integration-tips","title":"Integration Tips","text":"<ol> <li>Monitor costs - Check model pricing before expensive predictions</li> <li>Use appropriate models - Choose right model size for your task  </li> <li>Cancel unused predictions - Save costs by canceling long runs</li> <li>Cache results - Store outputs to avoid re-running identical predictions</li> <li>Test first - Try models with simple inputs before complex tasks</li> </ol>"},{"location":"skills/ecosystem/x_com/","title":"X.com (Twitter) Skill","text":"<p>Alpha Software Notice</p> <p>This skill is in alpha stage and under active development. APIs, features, and functionality may change without notice. Use with caution in production environments and expect potential breaking changes in future releases.</p> <p>Ultra-minimal X.com integration for multitenant applications with OAuth 1.0a authentication, user subscriptions, and real-time notifications.</p>"},{"location":"skills/ecosystem/x_com/#features","title":"Features","text":"<ul> <li>OAuth 1.0a User Context - Per-user rate limits (900 vs 450 requests/15min)</li> <li>Post tweets with automatic authentication</li> <li>Subscribe to users and monitor their posts</li> <li>Smart notifications via notification skill integration  </li> <li>LLM-powered relevance checking for subscribed posts</li> <li>Real-time webhooks for instant post monitoring</li> </ul>"},{"location":"skills/ecosystem/x_com/#quick-setup","title":"Quick Setup","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.ecosystem.x_com import XComSkill\n\nagent = BaseAgent(\n    name=\"x-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"x_com\": XComSkill()  # Auto-resolves: auth, kv, notifications\n    }\n)\n</code></pre> <p>Environment setup: <pre><code>export X_API_KEY=\"your_api_key\"\nexport X_API_SECRET=\"your_api_secret\"\nexport AGENTS_BASE_URL=\"https://your-agent-domain.com\"\n</code></pre></p>"},{"location":"skills/ecosystem/x_com/#core-tools","title":"Core Tools","text":""},{"location":"skills/ecosystem/x_com/#x_subscribeusername-instructions","title":"<code>x_subscribe(username, instructions)</code>","text":"<p>Subscribe to an X.com user with automatic authentication and notification setup.</p>"},{"location":"skills/ecosystem/x_com/#x_posttext","title":"<code>x_post(text)</code>","text":"<p>Post a tweet with automatic authentication handling.</p>"},{"location":"skills/ecosystem/x_com/#x_manageaction-username","title":"<code>x_manage(action, username)</code>","text":"<p>Manage X.com subscriptions (list or unsubscribe).</p>"},{"location":"skills/ecosystem/x_com/#usage-example","title":"Usage Example","text":"<pre><code># Subscribe, post, and manage in one conversation\nmessages = [\n    {\"role\": \"user\", \"content\": \"Subscribe to @openai for AI research updates, then post 'Hello from my WebAgent! \ud83e\udd16'\"}\n]\nresponse = await agent.run(messages=messages)\n</code></pre>"},{"location":"skills/ecosystem/x_com/#authentication-flow","title":"Authentication Flow","text":"<p>OAuth 1.0a handled automatically: First use provides authorization URL \u2192 User grants permission \u2192 Credentials stored securely \u2192 Future tools work seamlessly</p>"},{"location":"skills/ecosystem/x_com/#troubleshooting","title":"Troubleshooting","text":"<p>\"Authentication required\" - Ensure auth skill is configured and user is authenticated \"API credentials not configured\" - Set <code>X_API_KEY</code> and <code>X_API_SECRET</code> environment variables \"User not found\" - Check username spelling and verify account exists</p>"},{"location":"skills/ecosystem/zapier/","title":"Zapier Skill","text":"<p>Alpha Software Notice</p> <p>This skill is in alpha stage and under active development. APIs, features, and functionality may change without notice. Use with caution in production environments and expect potential breaking changes in future releases.</p> <p>Minimalistic Zapier integration for workflow automation. Trigger Zaps, monitor executions, and automate tasks across 7,000+ supported applications.</p>"},{"location":"skills/ecosystem/zapier/#features","title":"Features","text":"<ul> <li>Secure API key storage via auth and KV skills</li> <li>Trigger Zaps with custom input data</li> <li>List Zaps from your Zapier account</li> <li>Monitor task status in real-time</li> <li>7,000+ app integrations via Zapier ecosystem</li> </ul>"},{"location":"skills/ecosystem/zapier/#quick-setup","title":"Quick Setup","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.ecosystem.zapier import ZapierSkill\n\nagent = BaseAgent(\n    name=\"zapier-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"zapier\": ZapierSkill()  # Auto-resolves: auth, kv\n    }\n)\n</code></pre>"},{"location":"skills/ecosystem/zapier/#core-tools","title":"Core Tools","text":""},{"location":"skills/ecosystem/zapier/#zapier_setupapi_key","title":"<code>zapier_setup(api_key)</code>","text":"<p>Set up Zapier API credentials with automatic validation.</p>"},{"location":"skills/ecosystem/zapier/#zapier_triggerzap_id-data","title":"<code>zapier_trigger(zap_id, data)</code>","text":"<p>Trigger a Zapier Zap with optional input data.</p>"},{"location":"skills/ecosystem/zapier/#zapier_list_zaps","title":"<code>zapier_list_zaps()</code>","text":"<p>List all available Zaps in your Zapier account.</p>"},{"location":"skills/ecosystem/zapier/#zapier_statustask_id","title":"<code>zapier_status(task_id)</code>","text":"<p>Check the status of a Zap execution.</p>"},{"location":"skills/ecosystem/zapier/#usage-example","title":"Usage Example","text":"<pre><code># Setup, list Zaps, trigger, and check status\nmessages = [\n    {\"role\": \"user\", \"content\": \"Set up Zapier with API key AK_your_key, list my Zaps, then trigger lead processing Zap for John Smith\"}\n]\nresponse = await agent.run(messages=messages)\n</code></pre>"},{"location":"skills/ecosystem/zapier/#getting-your-zapier-api-key","title":"Getting Your Zapier API Key","text":"<ol> <li>Log into zapier.com</li> <li>Go to Account Settings &gt; Developer &gt; Manage API Keys</li> <li>Create API Key and copy it</li> </ol>"},{"location":"skills/ecosystem/zapier/#troubleshooting","title":"Troubleshooting","text":"<p>Authentication Issues - Verify API key is correct and has required permissions Zap Execution Problems - Ensure Zap is enabled and properly configured Rate Limiting - Respect 1 request/second limit and monitor task usage</p>"},{"location":"skills/platform/auth/","title":"Auth Skill","text":"<p>Authentication and authorization for agents using the Robutler Platform. Establishes a unified <code>AuthContext</code> and a secure, interoperable mechanism for agent\u2011to\u2011agent authorization via RS256 owner assertions (JWT).</p>"},{"location":"skills/platform/auth/#features","title":"Features","text":"<ul> <li>API key authentication with the Robutler Platform</li> <li>Role\u2011based access control: admin, owner, user</li> <li><code>on_connection</code> authentication hook</li> <li>Agent owner scope detection (from agent metadata)</li> <li>Harmonized <code>AuthContext</code> with minimal, stable fields</li> <li>Optional agent\u2011to\u2011agent assertions via <code>X-Owner-Assertion</code> (short\u2011lived RS256 JWT, verified via JWKS)</li> </ul>"},{"location":"skills/platform/auth/#configuration","title":"Configuration","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.robutler.auth import AuthSkill\n\nagent = BaseAgent(\n    name=\"secure-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"auth\": AuthSkill({\n            \"api_key\": \"your_platform_api_key\",         # Optional: defaults to agent.api_key\n            \"platform_api_url\": \"https://robutler.ai\",  # Optional: $ROBUTLER_INTERNAL_API_URL or $ROBUTLER_API_URL or http://localhost:3000\n            \"require_auth\": True                          # Optional: defaults to True\n        })\n    }\n)\n</code></pre> <p>Note: <code>platform_api_url</code> resolves in this order: <code>$ROBUTLER_INTERNAL_API_URL</code> \u2192 <code>$ROBUTLER_API_URL</code> \u2192 <code>http://localhost:3000</code>.</p>"},{"location":"skills/platform/auth/#scopes","title":"Scopes","text":"<ul> <li>admin: Platform administrators</li> <li>owner: Agent owner (API key belongs to the agent owner)</li> <li>user: Regular authenticated users</li> </ul> <p>If not explicitly set, the default scope is <code>user</code>.</p>"},{"location":"skills/platform/auth/#identity-and-context","title":"Identity and Context","text":"<p>The auth skill validates the API key during <code>on_connection</code> and exposes an <code>AuthContext</code> on the request context:</p> <pre><code>from webagents.server.context.context_vars import get_context\n\ncontext = get_context()\nauth = context.auth  # instance of AuthContext\n\n# Harmonized fields\nuser_id = auth.user_id               # caller identity; overridden by JWT `sub` when verified\nagent_id = auth.agent_id             # agent id from verified assertion (if provided)\nscope = auth.scope.value             # \"admin\" | \"owner\" | \"user\"\nauthenticated = auth.authenticated   # bool\nassertion = auth.assertion           # dict of decoded claims (if provided)\n</code></pre> <p>Deprecated identity fields (e.g., <code>origin_user_id</code>, <code>peer_user_id</code>, <code>agent_owner_user_id</code>) have been removed in favor of the harmonized fields above.</p>"},{"location":"skills/platform/auth/#authentication-flow","title":"Authentication Flow","text":"<ol> <li>Extract API key from <code>Authorization</code> (Bearer) or <code>X-API-Key</code></li> <li>Validate API key with the Robutler Platform</li> <li>Determine scope based on the validated user and agent ownership</li> <li>Optionally verify <code>X-Owner-Assertion</code> (RS256, JWKS) and merge acting identity into <code>AuthContext</code></li> <li>Populate <code>context.auth</code> with an <code>AuthContext</code> instance</li> </ol>"},{"location":"skills/platform/auth/#agenttoagent-assertions-owner-assertions","title":"Agent\u2011to\u2011Agent Assertions (Owner Assertions)","text":"<ul> <li>Primary purpose: secure, interoperable agent\u2011to\u2011agent authentication and authorization across services.</li> <li>Also enables owner\u2011only actions (e.g., ControlSkill) without exposing agent API keys to clients.</li> <li>Transport: send <code>X-Owner-Assertion: &lt;jwt&gt;</code> alongside your <code>Authorization</code> header.</li> </ul>"},{"location":"skills/platform/auth/#claims","title":"Claims","text":"<ul> <li><code>aud = robutler-agent:&lt;agentId&gt;</code> \u2014 audience bound to the target agent</li> <li><code>agent_id = &lt;agentId&gt;</code> \u2014 agent identity binding</li> <li><code>sub = &lt;userId&gt;</code> \u2014 acting end\u2011user identity</li> <li><code>owner_user_id = &lt;ownerId&gt;</code> \u2014 agent owner (advisory)</li> <li><code>jti</code> \u2014 unique token id for optional replay tracking</li> <li><code>iat</code> / <code>nbf</code> / <code>exp</code> \u2014 very short TTL (2\u20135 minutes)</li> </ul>"},{"location":"skills/platform/auth/#verification-by-authskill","title":"Verification by AuthSkill","text":"<ul> <li>Signature verification via JWKS (RS256)</li> <li>Enforce audience and agent binding: <code>aud == robutler-agent:&lt;agentId&gt;</code> and <code>agent_id == agent.id</code></li> <li>On success, update context:</li> <li><code>context.auth.user_id = sub</code> (acting identity)</li> <li><code>context.auth.agent_id = agent_id</code></li> <li><code>context.auth.assertion = &lt;decoded claims&gt;</code></li> <li>OWNER scope is derived by comparing the API\u2011key owner to the agent\u2019s <code>owner_user_id</code>; the assertion does not grant owner scope by itself.</li> </ul>"},{"location":"skills/platform/auth/#jwks-and-configuration","title":"JWKS and configuration","text":"<ul> <li>The skill discovers the JWKS at <code>OWNER_ASSERTION_JWKS_URL</code> if set; otherwise at <code>{platform_api_url}/api/auth/jwks</code>.</li> <li>Only RS256 is supported. HS256 and shared\u2011secret fallbacks are not supported.</li> </ul>"},{"location":"skills/platform/auth/#highlevel-flow","title":"High\u2011level flow","text":"<pre><code>sequenceDiagram\n  participant U as User\n  participant C as Chat Server\n  participant A as Agent Service\n  participant Auth as AuthSkill\n  participant Ctrl as ControlSkill\n\n  U-&gt;&gt;C: Edit agent description\n  C-&gt;&gt;A: Request + headers\\nAuthorization, X-Owner-Assertion, X-Payment-Token\n  A-&gt;&gt;Auth: on_connection()\n  Auth--&gt;&gt;Auth: Verify API key + (optional) verify assertion\n  Auth--&gt;&gt;A: Set context.auth; derive scope (OWNER if API\u2011key owner == agent.owner_user_id)\n  A-&gt;&gt;Ctrl: manage_agent(update_description)\n  Ctrl--&gt;&gt;A: Allowed (owner scope)\n  A-&gt;&gt;U: \u2705 Description updated</code></pre>"},{"location":"skills/platform/auth/#defaults-and-edge-cases","title":"Defaults and edge cases","text":"<ul> <li>If the skill is enabled and authentication succeeds, <code>auth.scope</code> defaults to <code>user</code> unless elevated to <code>owner</code> or <code>admin</code>.</li> <li>If the skill is disabled (<code>require_auth=False</code>) or not configured, downstream tools should treat the request as unauthenticated and avoid owner/admin\u2011scoped operations.</li> </ul>"},{"location":"skills/platform/auth/#example-protecting-an-owneronly-tool","title":"Example: protecting an owner\u2011only tool","text":"<pre><code>from webagents.agents.skills.robutler.auth.skill import AuthScope\n\ndef update_agent_settings(context, patch):\n    if context.auth.scope != AuthScope.OWNER:\n        raise PermissionError(\"Owner scope required\")\n    # proceed with update\n</code></pre> <p>Implementation: see <code>robutler/agents/skills/robutler/auth/skill.py</code>.</p>"},{"location":"skills/platform/discovery/","title":"Discovery Skill","text":"<p>Agent discovery skill for Robutler platform. Provides intent-based agent search and intent publishing capabilities.</p> <p>Discovery is designed to support dynamic agent resolution without listing the entire catalog on every request. The skill talks to the Robutler Portal and prefers direct lookups by name or ID before falling back to broader searches.</p>"},{"location":"skills/platform/discovery/#key-features","title":"Key Features","text":"<ul> <li>Intent-based agent search via Portal API</li> <li>Semantic similarity matching for agent discovery</li> <li>Intent registration and publishing (requires server handshake)</li> <li>Agent capability filtering and ranking</li> <li>Multiple search modes (semantic, exact, fuzzy)</li> </ul>"},{"location":"skills/platform/discovery/#configuration","title":"Configuration","text":"<ul> <li><code>robutler_api_key</code> (config, agent, or env)</li> <li><code>cache_ttl</code>, <code>max_agents</code>, <code>enable_discovery</code>, <code>search_mode</code></li> <li><code>portal_base_url</code> (optional; defaults from server env)</li> </ul>"},{"location":"skills/platform/discovery/#example-add-discovery-skill-to-an-agent","title":"Example: Add Discovery Skill to an Agent","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.robutler.discovery import DiscoverySkill\n\nagent = BaseAgent(\n    name=\"discovery-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"discovery\": DiscoverySkill({\n            \"cache_ttl\": 300,\n            \"max_agents\": 10\n        })\n    }\n)\n</code></pre>"},{"location":"skills/platform/discovery/#example-use-discovery-tool-in-a-skill","title":"Example: Use Discovery Tool in a Skill","text":"<pre><code>from webagents.agents.skills import Skill, tool\n\nclass FindExpertSkill(Skill):\n    def __init__(self):\n        super().__init__()\n        self.discovery = self.agent.skills[\"discovery\"]\n\n    @tool\n    async def find_expert(self, topic: str) -&gt; str:\n        \"\"\"Find an expert agent for a given topic\"\"\"\n        results = await self.discovery.search_agents(query=topic)\n        if results and results.get('agents'):\n            return f\"Top expert: {results['agents'][0]['name']}\"\n        return \"No expert found.\"\n</code></pre> <p>Implementation: <code>robutler/agents/skills/robutler/discovery/skill.py</code>.</p>"},{"location":"skills/platform/files/","title":"File Storage Skill","text":"<p>Comprehensive file management with harmonized API for storing, retrieving, and managing files.</p>"},{"location":"skills/platform/files/#overview","title":"Overview","text":"<p>The <code>RobutlerFilesSkill</code> provides file management capabilities using the harmonized content API. It allows agents to download files from URLs, store files from base64 data, and list accessible files with proper scope-based access control.</p>"},{"location":"skills/platform/files/#features","title":"Features","text":"<ul> <li>URL-based File Storage: Download and store files directly from URLs</li> <li>Base64 File Upload: Store files from base64 encoded data</li> <li>File Listing: List files with scope-based filtering (public/private)</li> <li>Agent Name Prefixing: Automatically prefixes uploaded files with agent name</li> <li>Visibility Control: Support for public, private, and shared file visibility</li> <li>Owner-Scoped Uploads: File upload operations restricted to agent owners</li> <li>Harmonized API: Uses the new <code>/api/content/agent</code> endpoints for efficient operations</li> </ul>"},{"location":"skills/platform/files/#usage","title":"Usage","text":""},{"location":"skills/platform/files/#basic-setup","title":"Basic Setup","text":"<pre><code>from webagents.agents.core.base_agent import BaseAgent\nfrom webagents.agents.skills.robutler.storage.files.skill import RobutlerFilesSkill\n\nagent = BaseAgent(\n    name=\"file-agent\",\n    model=\"openai/gpt-4o-mini\",\n    skills={\n        \"files\": RobutlerFilesSkill()\n    }\n)\n</code></pre>"},{"location":"skills/platform/files/#file-operations","title":"File Operations","text":"<pre><code># Download and store a file from URL\nresponse = await agent.run(messages=[\n    {\"role\": \"user\", \"content\": \"Download and store the image from https://example.com/image.jpg\"}\n])\n\n# List all accessible files\nresponse = await agent.run(messages=[\n    {\"role\": \"user\", \"content\": \"Show me all my files\"}\n])\n</code></pre>"},{"location":"skills/platform/files/#tool-reference","title":"Tool Reference","text":""},{"location":"skills/platform/files/#store_file_from_url","title":"<code>store_file_from_url</code>","text":"<p>Download and store a file from a URL.</p> <p>Parameters:</p> <ul> <li><code>url</code> (str, required): URL to download file from</li> <li><code>filename</code> (str, optional): Custom filename (auto-detected if not provided)</li> <li><code>description</code> (str, optional): Description of the file</li> <li><code>tags</code> (List[str], optional): List of tags for the file</li> <li><code>visibility</code> (str, optional): File visibility - \"public\", \"private\", or \"shared\" (default: \"private\")</li> </ul> <p>Returns:</p> <p>JSON string with storage result including: - <code>success</code>: Boolean indicating success/failure - <code>id</code>: File ID in the system - <code>filename</code>: Stored filename (with agent prefix) - <code>url</code>: Public URL for accessing the file - <code>size</code>: File size in bytes - <code>content_type</code>: MIME type of the file - <code>visibility</code>: File visibility setting - <code>source_url</code>: Original URL the file was downloaded from</p> <p>Scope: <code>owner</code> - Only the agent owner can upload files</p>"},{"location":"skills/platform/files/#store_file_from_base64","title":"<code>store_file_from_base64</code>","text":"<p>Store a file from base64 encoded data.</p> <p>Parameters:</p> <ul> <li><code>filename</code> (str, required): Name of the file</li> <li><code>base64_data</code> (str, required): Base64 encoded file content</li> <li><code>content_type</code> (str, optional): MIME type of the file (default: \"application/octet-stream\")</li> <li><code>description</code> (str, optional): Description of the file</li> <li><code>tags</code> (List[str], optional): List of tags for the file</li> <li><code>visibility</code> (str, optional): File visibility - \"public\", \"private\", or \"shared\" (default: \"private\")</li> </ul> <p>Returns:</p> <p>JSON string with storage result including: - <code>success</code>: Boolean indicating success/failure - <code>id</code>: File ID in the system - <code>filename</code>: Stored filename (with agent prefix) - <code>url</code>: Public URL for accessing the file - <code>size</code>: File size in bytes - <code>content_type</code>: MIME type of the file - <code>visibility</code>: File visibility setting</p> <p>Scope: <code>owner</code> - Only the agent owner can upload files</p>"},{"location":"skills/platform/files/#list_files","title":"<code>list_files</code>","text":"<p>List files accessible by the current agent with scope-based filtering.</p> <p>Parameters:</p> <ul> <li><code>scope</code> (str, optional): Scope filter - \"public\", \"private\", or None (all files for owner)</li> </ul> <p>Returns:</p> <p>JSON string with file list including: - <code>success</code>: Boolean indicating success/failure - <code>agent_name</code>: Name of the agent - <code>total_files</code>: Number of files returned - <code>files</code>: Array of file objects with details</p> <p>Scope: Available to all users, but results filtered based on ownership: - Agent owner: Can see all files (public + private) or filter by scope - Non-owner: Only sees public files regardless of scope parameter</p> <p>Pricing: 0.005 credits per call</p>"},{"location":"skills/platform/files/#file-visibility-levels","title":"File Visibility Levels","text":"<ul> <li>private: Only visible to the agent owner</li> <li>public: Visible to anyone who can access the agent</li> <li>shared: Visible to authorized users (implementation-dependent)</li> </ul>"},{"location":"skills/platform/files/#configuration","title":"Configuration","text":""},{"location":"skills/platform/files/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>ROBUTLER_API_URL</code>: Portal API base URL (default: \"http://localhost:3000\")</li> <li><code>ROBUTLER_CHAT_URL</code>: Chat server base URL for public content (default: \"http://localhost:3001\")</li> <li><code>WEBAGENTS_API_KEY</code>: Default API key if not provided in config</li> </ul>"},{"location":"skills/platform/files/#skill-configuration","title":"Skill Configuration","text":"<pre><code>config = {\n    \"portal_url\": \"https://robutler.ai\",\n    \"chat_base_url\": \"https://chat.robutler.ai\",\n    \"api_key\": \"your-api-key\"\n}\n\nfiles_skill = RobutlerFilesSkill(config)\n</code></pre>"},{"location":"skills/platform/files/#example-integration","title":"Example Integration","text":"<pre><code>from webagents.agents.skills.base import Skill\nfrom webagents.agents.tools.decorators import tool\n\nclass ImageProcessingSkill(Skill):\n    @tool\n    async def process_image_from_url(self, image_url: str) -&gt; str:\n        # Download and store the image\n        store_result = await self.discover_and_call(\n            \"files\", \n            \"store_file_from_url\",\n            image_url,\n            description=\"Image for processing\"\n        )\n\n        # Parse the result\n        import json\n        result = json.loads(store_result)\n\n        if result[\"success\"]:\n            # Process the stored image\n            file_url = result[\"url\"]\n            return f\"\u2705 Image stored and ready for processing: {file_url}\"\n        else:\n            return f\"\u274c Failed to store image: {result['error']}\"\n\n    @tool\n    async def list_my_images(self) -&gt; str:\n        # List only public image files\n        files_result = await self.discover_and_call(\"files\", \"list_files\", \"public\")\n\n        import json\n        result = json.loads(files_result)\n\n        if result[\"success\"]:\n            image_files = [f for f in result[\"files\"] \n                          if f[\"content_type\"].startswith(\"image/\")]\n            return f\"Found {len(image_files)} image files\"\n        else:\n            return f\"\u274c Failed to list files: {result['error']}\"\n</code></pre>"},{"location":"skills/platform/files/#security","title":"Security","text":"<ul> <li>Owner-Only Uploads: File upload operations are restricted to agent owners</li> <li>Scope-Based Access: File listing respects ownership and visibility settings</li> <li>Agent Isolation: Files are associated with specific agents</li> <li>API Key Authentication: Uses secure agent API keys for all operations</li> <li>Automatic Prefixing: Agent names are automatically prefixed to prevent conflicts</li> </ul>"},{"location":"skills/platform/files/#error-handling","title":"Error Handling","text":"<p>The skill provides comprehensive error handling:</p> <ul> <li>Download Failures: Returns detailed error messages for URL download issues</li> <li>Upload Failures: Handles API upload errors with descriptive messages</li> <li>Authentication Errors: Manages missing or invalid API keys</li> <li>Network Issues: Provides meaningful error responses for connectivity problems</li> <li>Invalid Data: Handles malformed base64 data and other input validation</li> </ul>"},{"location":"skills/platform/files/#advanced-features","title":"Advanced Features","text":""},{"location":"skills/platform/files/#agent-name-prefixing","title":"Agent Name Prefixing","text":"<p>All uploaded files are automatically prefixed with the agent name to prevent conflicts: - Original: <code>image.jpg</code> - Stored as: <code>my-agent_image.jpg</code></p>"},{"location":"skills/platform/files/#url-rewriting","title":"URL Rewriting","text":"<p>Public content URLs are automatically rewritten to point to the chat server for optimal delivery: - Portal URL: <code>http://localhost:3000/api/content/public/...</code> - Rewritten: <code>http://localhost:3001/api/content/public/...</code></p>"},{"location":"skills/platform/files/#ownership-detection","title":"Ownership Detection","text":"<p>The skill automatically detects whether the current user is the actual owner of the agent for proper access control, not just checking admin privileges.</p>"},{"location":"skills/platform/files/#dependencies","title":"Dependencies","text":"<ul> <li>Agent API Key: Requires valid agent API key for portal authentication</li> <li>Portal Connectivity: Requires network access to Robutler portal API endpoints</li> <li>RobutlerClient: Uses the official Robutler client for API operations</li> <li>Agent Context: Requires proper agent initialization and context</li> </ul>"},{"location":"skills/platform/kv/","title":"KV Storage Skill","text":"<p>Simple per-agent key-value storage for persistent data and configuration.</p>"},{"location":"skills/platform/kv/#overview","title":"Overview","text":"<p>The <code>KVSkill</code> provides owner-scoped key-value storage capabilities, allowing agents to persistently store and retrieve simple string data via the Robutler portal <code>/api/kv</code> endpoint.</p>"},{"location":"skills/platform/kv/#features","title":"Features","text":"<ul> <li>Owner-Only Access: All operations are restricted to the agent owner using <code>scope=\"owner\"</code></li> <li>Per-Agent Storage: Each agent has its own isolated key-value namespace</li> <li>Namespace Support: Optional namespacing for organizing keys</li> <li>Simple String Storage: Store and retrieve string values by key</li> <li>Automatic Authentication: Uses agent API keys for secure access</li> </ul>"},{"location":"skills/platform/kv/#usage","title":"Usage","text":""},{"location":"skills/platform/kv/#basic-setup","title":"Basic Setup","text":"<pre><code>from webagents.agents.core.base_agent import BaseAgent\nfrom webagents.agents.skills.robutler.kv.skill import KVSkill\n\nagent = BaseAgent(\n    name=\"kv-agent\",\n    model=\"openai/gpt-4o-mini\",\n    skills={\n        \"kv\": KVSkill()\n    }\n)\n</code></pre>"},{"location":"skills/platform/kv/#storing-and-retrieving-data","title":"Storing and Retrieving Data","text":"<p>The skill provides simple key-value operations:</p> <pre><code># Store configuration\nresponse = await agent.run(messages=[\n    {\"role\": \"user\", \"content\": \"Store my API key as 'openai_key' with value 'sk-...'\"}\n])\n\n# Retrieve configuration\nresponse = await agent.run(messages=[\n    {\"role\": \"user\", \"content\": \"Get my stored API key from 'openai_key'\"}\n])\n</code></pre>"},{"location":"skills/platform/kv/#tool-reference","title":"Tool Reference","text":""},{"location":"skills/platform/kv/#kv_set","title":"<code>kv_set</code>","text":"<p>Set a key to a string value.</p> <p>Parameters:</p> <ul> <li><code>key</code> (str, required): The key to store the value under</li> <li><code>value</code> (str, required): The string value to store</li> <li><code>namespace</code> (str, optional): Optional namespace for organizing keys</li> </ul> <p>Returns:</p> <ul> <li>Success: <code>\"\u2705 Saved\"</code></li> <li>Error: <code>\"\u274c KV set failed: {error}\"</code></li> </ul> <p>Scope: <code>owner</code> - Only the agent owner can set values</p>"},{"location":"skills/platform/kv/#kv_get","title":"<code>kv_get</code>","text":"<p>Get a string value by key.</p> <p>Parameters:</p> <ul> <li><code>key</code> (str, required): The key to retrieve the value for</li> <li><code>namespace</code> (str, optional): Optional namespace to search in</li> </ul> <p>Returns:</p> <ul> <li>Success: The stored string value</li> <li>Not found or error: Empty string <code>\"\"</code></li> </ul> <p>Scope: <code>owner</code> - Only the agent owner can retrieve values</p>"},{"location":"skills/platform/kv/#kv_delete","title":"<code>kv_delete</code>","text":"<p>Delete a key and its value.</p> <p>Parameters:</p> <ul> <li><code>key</code> (str, required): The key to delete</li> <li><code>namespace</code> (str, optional): Optional namespace the key is in</li> </ul> <p>Returns:</p> <ul> <li>Success: <code>\"\ud83d\uddd1\ufe0f Deleted\"</code></li> <li>Error: Empty string <code>\"\"</code></li> </ul> <p>Scope: <code>owner</code> - Only the agent owner can delete keys</p>"},{"location":"skills/platform/kv/#configuration","title":"Configuration","text":"<p>The skill requires no additional configuration beyond adding it to your agent. It automatically:</p> <ul> <li>Resolves the agent and user context from the current request</li> <li>Uses the agent's API key for authentication</li> <li>Connects to the appropriate Robutler portal API endpoint</li> </ul>"},{"location":"skills/platform/kv/#use-cases","title":"Use Cases","text":"<p>Perfect for storing:</p> <ul> <li>API Keys and Tokens: Securely store third-party API credentials</li> <li>Configuration Settings: Agent-specific configuration values</li> <li>State Information: Simple state data between agent interactions</li> <li>User Preferences: Store user-specific settings and preferences</li> </ul>"},{"location":"skills/platform/kv/#example-integration","title":"Example Integration","text":"<pre><code>from webagents.agents.skills.base import Skill\nfrom webagents.agents.tools.decorators import tool\n\nclass WeatherSkill(Skill):\n    @tool\n    async def get_weather(self, location: str) -&gt; str:\n        # Retrieve stored API key\n        api_key = await self.discover_and_call(\"kv\", \"get\", \"weather_api_key\")\n\n        if not api_key:\n            return \"\u274c Weather API key not configured\"\n\n        # Use API key to fetch weather data\n        # ... weather API logic here\n\n        return f\"Weather in {location}: Sunny, 72\u00b0F\"\n\n    @tool\n    async def configure_weather_api(self, api_key: str) -&gt; str:\n        # Store API key for future use\n        result = await self.discover_and_call(\"kv\", \"set\", \"weather_api_key\", api_key)\n        return f\"Weather API configured: {result}\"\n</code></pre>"},{"location":"skills/platform/kv/#security","title":"Security","text":"<ul> <li>Owner-Only Access: All KV operations are scoped to <code>owner</code> only</li> <li>Agent Isolation: Each agent has its own isolated key-value store</li> <li>API Key Authentication: Uses secure agent API keys for portal communication</li> <li>Context Resolution: Automatically resolves agent and user context for proper isolation</li> </ul>"},{"location":"skills/platform/kv/#error-handling","title":"Error Handling","text":"<p>The skill handles common error scenarios:</p> <ul> <li>Missing Context: Returns error messages if agent/user context cannot be resolved</li> <li>API Authentication Failures: Handles missing or invalid API keys</li> <li>Network Issues: Returns empty strings or error messages for connection problems</li> <li>Portal API Errors: Surfaces API error responses for debugging</li> </ul>"},{"location":"skills/platform/kv/#limitations","title":"Limitations","text":"<ul> <li>String Values Only: Only supports string values (use JSON encoding for complex data)</li> <li>Owner Scope: Only the agent owner can access the key-value store</li> <li>No Bulk Operations: Operations are performed one key at a time</li> <li>Simple Querying: No advanced querying or pattern matching capabilities</li> </ul>"},{"location":"skills/platform/kv/#dependencies","title":"Dependencies","text":"<ul> <li>Agent API Key: Requires valid agent API key for portal authentication</li> <li>Agent Context: Requires agent to be properly initialized with context</li> <li>Portal Connectivity: Requires network access to Robutler portal API endpoints</li> </ul>"},{"location":"skills/platform/nli/","title":"NLI Skill (Natural Language Interface)","text":"<p>Natural Language Interface skill for agent-to-agent communication.</p> <p>NLI lets agents collaborate over HTTP using natural language. It adds resilient request/response primitives and optional budgeting controls (authorization caps) so one agent can safely call another.</p>"},{"location":"skills/platform/nli/#features","title":"Features","text":"<ul> <li>HTTP-based communication with other Robutler agents</li> <li>Authorization limits and cost tracking</li> <li>Communication history and success rate tracking</li> <li>Automatic timeout and retry handling</li> <li>Agent endpoint discovery and management</li> </ul>"},{"location":"skills/platform/nli/#configuration","title":"Configuration","text":"<ul> <li><code>timeout</code>, <code>max_retries</code></li> <li><code>default_authorization</code>, <code>max_authorization</code> (optional budgeting)</li> <li><code>portal_base_url</code> (optional for resolving agents)</li> </ul>"},{"location":"skills/platform/nli/#example-add-nli-skill-to-an-agent","title":"Example: Add NLI Skill to an Agent","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.robutler.nli import NLISkill\n\nagent = BaseAgent(\n    name=\"nli-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"nli\": NLISkill({\n            \"timeout\": 20.0,\n            \"max_retries\": 3\n        })\n    }\n)\n</code></pre>"},{"location":"skills/platform/nli/#example-use-nli-tool-in-a-skill","title":"Example: Use NLI Tool in a Skill","text":"<pre><code>from webagents.agents.skills import Skill, tool\n\nclass CollaborateSkill(Skill):\n    def __init__(self):\n        super().__init__()\n        self.nli = self.agent.skills[\"nli\"]\n\n    @tool\n    async def ask_agent(self, agent_url: str, message: str) -&gt; str:\n        \"\"\"Send a message to another agent and get the response\"\"\"\n        return await self.nli.nli_tool(agent_url=agent_url, message=message)\n</code></pre> <p>Implementation: <code>robutler/agents/skills/robutler/nli/skill.py</code>.</p>"},{"location":"skills/platform/notifications/","title":"Notifications Skill","text":"<p>Send push notifications to agent owners through the Robutler platform.</p>"},{"location":"skills/platform/notifications/#overview","title":"Overview","text":"<p>The <code>NotificationsSkill</code> provides owner-scoped push notification capabilities, allowing agents to send notifications directly to their owners through the Robutler portal notification system.</p>"},{"location":"skills/platform/notifications/#features","title":"Features","text":"<ul> <li>Owner-Only Access: Notifications are restricted to the agent owner using <code>scope=\"owner\"</code></li> <li>Push Notification Delivery: Integrates with the Robutler portal notification API</li> <li>Customizable Notifications: Support for different notification types, priorities, and settings</li> <li>Automatic Authentication: Uses agent API keys for secure notification delivery</li> </ul>"},{"location":"skills/platform/notifications/#usage","title":"Usage","text":""},{"location":"skills/platform/notifications/#basic-setup","title":"Basic Setup","text":"<pre><code>from webagents.agents.core.base_agent import BaseAgent\nfrom webagents.agents.skills.robutler.notifications.skill import NotificationsSkill\n\nagent = BaseAgent(\n    name=\"notification-agent\",\n    model=\"openai/gpt-4o-mini\",\n    skills={\n        \"notifications\": NotificationsSkill()\n    }\n)\n</code></pre>"},{"location":"skills/platform/notifications/#sending-notifications","title":"Sending Notifications","text":"<p>The skill provides a single tool for sending notifications:</p> <pre><code># The agent can use this tool to send notifications\nresponse = await agent.run(messages=[\n    {\"role\": \"user\", \"content\": \"Send me a notification that the task is complete\"}\n])\n</code></pre>"},{"location":"skills/platform/notifications/#tool-reference","title":"Tool Reference","text":""},{"location":"skills/platform/notifications/#send_notification","title":"<code>send_notification</code>","text":"<p>Send a push notification to the agent owner.</p> <p>Parameters:</p> <ul> <li><code>title</code> (str, required): Notification title</li> <li><code>body</code> (str, required): Notification body text</li> <li><code>tag</code> (str, optional): Notification tag for grouping</li> <li><code>type</code> (str, optional): Notification type (<code>chat_message</code>, <code>agent_update</code>, <code>system_announcement</code>, <code>marketing</code>). Default: <code>agent_update</code></li> <li><code>priority</code> (str, optional): Priority level (<code>low</code>, <code>normal</code>, <code>high</code>, <code>urgent</code>). Default: <code>normal</code></li> <li><code>requireInteraction</code> (bool, optional): Whether notification requires user interaction. Default: <code>false</code></li> <li><code>silent</code> (bool, optional): Whether notification should be silent. Default: <code>false</code></li> <li><code>ttl</code> (int, optional): Time-to-live in seconds. Default: <code>86400</code> (24 hours)</li> </ul> <p>Returns:</p> <ul> <li>Success: <code>\"\u2705 Notification queued: {message}\"</code></li> <li>Error: <code>\"\u274c Failed to send notification: {error}\"</code></li> </ul> <p>Scope: <code>owner</code> - Only the agent owner can trigger notifications</p>"},{"location":"skills/platform/notifications/#configuration","title":"Configuration","text":"<p>The skill requires no additional configuration beyond adding it to your agent. It automatically:</p> <ul> <li>Resolves the agent owner's user ID</li> <li>Uses the agent's API key for authentication</li> <li>Connects to the appropriate Robutler portal API endpoint</li> </ul>"},{"location":"skills/platform/notifications/#security","title":"Security","text":"<ul> <li>Owner-Only Access: All notification tools are scoped to <code>owner</code> only</li> <li>API Key Authentication: Uses secure agent API keys for portal communication</li> <li>User ID Resolution: Automatically identifies the correct recipient based on agent ownership</li> </ul>"},{"location":"skills/platform/notifications/#example-integration","title":"Example Integration","text":"<pre><code>from webagents.agents.skills.base import Skill\nfrom webagents.agents.tools.decorators import tool\n\nclass TaskSkill(Skill):\n    @tool\n    async def complete_task(self, task_name: str) -&gt; str:\n        # Perform task logic here\n        task_result = f\"Completed: {task_name}\"\n\n        # Send notification when task completes\n        await self.discover_and_call(\n            \"notifications\", \n            f\"Task Complete: {task_name}\", \n            f\"Your task '{task_name}' has been completed successfully.\"\n        )\n\n        return task_result\n</code></pre>"},{"location":"skills/platform/notifications/#error-handling","title":"Error Handling","text":"<p>The skill handles common error scenarios:</p> <ul> <li>Missing Owner ID: Returns error message if agent owner cannot be resolved</li> <li>API Authentication Failures: Handles missing or invalid API keys</li> <li>Network Issues: Provides meaningful error messages for connection problems</li> <li>Portal API Errors: Surfaces API error responses for debugging</li> </ul>"},{"location":"skills/platform/notifications/#dependencies","title":"Dependencies","text":"<ul> <li>Agent API Key: Requires valid agent API key for portal authentication</li> <li>Owner Context: Requires agent to have identifiable owner for targeting notifications</li> <li>Portal Connectivity: Requires network access to Robutler portal API endpoints</li> </ul>"},{"location":"skills/platform/payments/","title":"Payment Skill","text":"<p>Payment processing and billing skill for the Robutler platform. This skill enforces billing policies up-front and finalizes charges when a request completes.</p>"},{"location":"skills/platform/payments/#key-features","title":"Key Features","text":"<ul> <li>Payment token validation during <code>on_connection</code> (returns 402 if required and missing)</li> <li>LLM cost calculation using LiteLLM <code>cost_per_token</code></li> <li>Tool pricing via optional <code>@pricing</code> decorator (results logged to <code>context.usage</code> by the agent)</li> <li>Final charging based on <code>context.usage</code> at <code>finalize_connection</code></li> <li>Optional async/sync <code>amount_calculator</code> to customize total charge</li> <li>Transaction creation via Portal API</li> <li>Depends on <code>AuthSkill</code> for user identity propagation</li> </ul>"},{"location":"skills/platform/payments/#configuration","title":"Configuration","text":"<ul> <li><code>enable_billing</code> (default: true)</li> <li><code>agent_pricing_percent</code> (percent, e.g., <code>20</code> for 20%)</li> <li><code>minimum_balance</code> (USD required to proceed; 0 allows free trials without up-front token)</li> <li><code>robutler_api_url</code>, <code>robutler_api_key</code> (server-to-portal calls)</li> <li><code>amount_calculator</code> (optional): async or sync callable <code>(llm_cost_usd, tool_cost_usd, agent_pricing_percent_percent) -&gt; float</code></li> <li>Default: <code>(llm + tool) * (1 + agent_pricing_percent_percent/100)</code></li> </ul>"},{"location":"skills/platform/payments/#example-add-payment-skill-to-an-agent","title":"Example: Add Payment Skill to an Agent","text":"<pre><code>from webagents.agents import BaseAgent\nfrom webagents.agents.skills.robutler.auth.skill import AuthSkill\nfrom webagents.agents.skills.robutler.payments import PaymentSkill\n\nagent = BaseAgent(\n    name=\"paid-agent\",\n    model=\"openai/gpt-4o\",\n    skills={\n        \"auth\": AuthSkill(),  # Required dependency\n        \"payments\": PaymentSkill({\n            \"enable_billing\": True,\n            \"agent_pricing_percent\": 20,   # percent\n            \"minimum_balance\": 1.0 # USD\n        })\n    }\n)\n</code></pre>"},{"location":"skills/platform/payments/#tool-pricing-with-pricing-decorator-optional","title":"Tool Pricing with @pricing Decorator (optional)","text":"<p>The PaymentSkill provides a <code>@pricing</code> decorator to annotate tools with pricing metadata. Tools can also return explicit usage objects and will be accounted from <code>context.usage</code> during finalize.</p> <pre><code>from webagents.agents.tools.decorators import tool\nfrom webagents.agents.skills.robutler.payments import pricing, PricingInfo\n\n@tool\n@pricing(credits_per_call=0.05, reason=\"Database query\")\nasync def query_database(sql: str) -&gt; dict:\n    \"\"\"Query database - costs 0.05 credits per call\"\"\"\n    return {\"results\": [...]}\n\n@tool  \n@pricing()  # Dynamic pricing\nasync def analyze_data(data: str) -&gt; tuple:\n    \"\"\"Analyze data with variable pricing based on complexity\"\"\"\n    complexity = len(data)\n    result = f\"Analysis of {complexity} characters\"\n\n    # Simple complexity-based pricing: 0.001 credits per character\n    credits = max(0.01, complexity * 0.001)  # Minimum 0.01 credits\n\n    pricing_info = PricingInfo(\n        credits=credits,\n        reason=f\"Data analysis of {complexity} chars\",\n        metadata={\"character_count\": complexity, \"rate_per_char\": 0.001}\n    )\n    return result, pricing_info\n</code></pre>"},{"location":"skills/platform/payments/#pricing-options","title":"Pricing Options","text":"<ol> <li>Fixed Pricing: <code>@pricing(credits_per_call=0.05)</code> (0.05 credits per call)</li> <li>Dynamic Pricing: Return <code>(result, PricingInfo(credits=0.15, ...))</code></li> <li>Conditional Pricing: Override base pricing in function logic</li> </ol>"},{"location":"skills/platform/payments/#cost-calculation","title":"Cost Calculation","text":"<ul> <li>LLM Costs: Calculated in <code>finalize_connection</code> using LiteLLM <code>cost_per_token(model, prompt_tokens, completion_tokens)</code></li> <li>Tool Costs: Read from tool usage records in <code>context.usage</code> (e.g., a record with <code>{\"pricing\": {\"credits\": ...}}</code>), which are appended automatically by the agent when a priced tool returns <code>(result, usage_payload)</code></li> <li>Total: If <code>amount_calculator</code> is provided, its return value is used; otherwise <code>(llm + tool) * (1 + agent_pricing_percent_percent/100)</code></li> </ul>"},{"location":"skills/platform/payments/#example-validate-a-payment-token","title":"Example: Validate a Payment Token","text":"<pre><code>from webagents.agents.skills import Skill, tool\n\nclass PaymentOpsSkill(Skill):\n    def __init__(self):\n        super().__init__()\n        self.payment = self.agent.skills[\"payment\"]\n\n    @tool\n    async def validate_token(self, token: str) -&gt; str:\n        \"\"\"Validate a payment token\"\"\"\n        result = await self.payment.validate_payment_token(token)\n        return str(result)\n</code></pre>"},{"location":"skills/platform/payments/#hook-integration","title":"Hook Integration","text":"<p>The PaymentSkill uses BaseAgent hooks for lifecycle, but cost aggregation is done at finalize:</p> <ul> <li><code>on_connection</code>: Validate payment token and check balance. If <code>enable_billing</code> and no token is provided while <code>minimum_balance &gt; 0</code>, a 402 error is raised and processing stops. <code>finalize_connection</code> will still run for cleanup but will be a no-op.</li> <li><code>on_message</code>: No-op (costs are computed at finalize)</li> <li><code>after_toolcall</code>: No-op (tool costs come from usage records)</li> <li><code>finalize_connection</code>: Aggregate from <code>context.usage</code>, compute final amount, and charge the token. If there are costs but no token, a 402 error is raised.</li> </ul>"},{"location":"skills/platform/payments/#context-namespacing","title":"Context Namespacing","text":"<p>The PaymentSkill stores data in the <code>payments</code> namespace of the request context:</p> <pre><code>from webagents.server.context.context_vars import get_context\n\ncontext = get_context()\npayments_data = getattr(context, 'payments', None)\npayment_token = getattr(payments_data, 'payment_token', None) if payments_data else None\n</code></pre>"},{"location":"skills/platform/payments/#usage-tracking","title":"Usage Tracking","text":"<p>All usage is centralized on <code>context.usage</code> by the agent:</p> <ul> <li>LLM usage records are appended after each completion (including streaming final usage chunk).</li> <li>Tool usage is appended when a priced tool returns <code>(result, usage_payload)</code>; the agent unwraps the result and stores <code>usage_payload</code> as a <code>{type: 'tool', pricing: {...}}</code> record.</li> </ul> <p>At <code>finalize_connection</code>, the Payment Skill sums LLM and tool costs from <code>context.usage</code> and performs the charge.</p>"},{"location":"skills/platform/payments/#advanced-amount_calculator","title":"Advanced: amount_calculator","text":"<p>You can provide an async or sync <code>amount_calculator</code> to fully control the final charge amount:</p> <pre><code>async def my_amount_calculator(llm_cost_usd: float, tool_cost_usd: float, agent_pricing_percent_percent: float) -&gt; float:\n    base = llm_cost_usd + tool_cost_usd\n    # Custom logic here (e.g., tiered discounts)\n    return base * (1 + agent_pricing_percent_percent/100)\n\npayment = PaymentSkill({\n    \"enable_billing\": True,\n    \"agent_pricing_percent\": 15,  # percent\n    \"amount_calculator\": my_amount_calculator,\n})\n</code></pre> <p>If omitted, the default formula is used: <code>(llm + tool) * (1 + agent_pricing_percent/100)</code>.</p>"},{"location":"skills/platform/payments/#dependencies","title":"Dependencies","text":"<ul> <li>AuthSkill: Required for user identity headers (<code>X-Origin-User-ID</code>, <code>X-Peer-User-ID</code>, <code>X-Agent-Owner-User-ID</code>). The Payment Skill reads them from the auth namespace on the context.</li> </ul> <p>Implementation: <code>robutler/agents/skills/robutler/payments/skill.py</code>.</p>"},{"location":"skills/platform/payments/#error-semantics-402","title":"Error semantics (402)","text":"<ul> <li>Missing token while <code>enable_billing</code> and <code>minimum_balance &gt; 0</code> \u279c 402 Payment Required</li> <li>Invalid or expired token \u279c 402 Payment Token Invalid</li> <li>Insufficient balance \u279c 402 Insufficient Balance</li> </ul> <p>Finalize hooks still run for cleanup but perform no charge if no token/usage is present.</p>"}]}